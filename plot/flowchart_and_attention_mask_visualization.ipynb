{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FlexAttention 代码流程图和注意力掩码可视化\n",
    "# FlexAttention Code Flowchart and Attention Mask Visualization\n",
    "\n",
    "本笔记本提供了 FlexAttention 实现的完整可视化，包括：\n",
    "This notebook provides comprehensive visualizations for the FlexAttention implementation, including:\n",
    "\n",
    "1. **代码流程图** - 展示整个处理流程 / **Code Flowchart** - Shows the entire processing pipeline\n",
    "2. **注意力掩码形状** - 精确展示不同场景下的掩码矩阵 / **Attention Mask Shapes** - Precisely shows mask matrices in different scenarios\n",
    "3. **可交互参数** - 方便修改和实验 / **Interactive Parameters** - Easy to modify for experimentation\n",
    "\n",
    "---\n",
    "\n",
    "## 设置说明 / Setup Instructions\n",
    "\n",
    "确保安装了必要的包：Make sure you have the required packages installed:\n",
    "\n",
    "```bash\n",
    "pip install matplotlib numpy jupyter\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库 / Import required libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.patches import FancyBboxPatch, FancyArrowPatch\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 设置中文字体支持 / Set Chinese font support\n",
    "plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial Unicode MS', 'SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 设置图表样式 / Set plot style\n",
    "plt.style.use('default')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第一部分：代码流程图 / Part 1: Code Flowchart\n",
    "\n",
    "### 可修改参数 / Modifiable Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 流程图参数配置 / Flowchart Configuration Parameters\n",
    "# ============================================================================\n",
    "\n",
    "# 可以修改这些参数来调整流程图的外观 / Modify these to adjust flowchart appearance\n",
    "FLOWCHART_CONFIG = {\n",
    "    'figure_size': (14, 16),          # 图表大小 / Figure size\n",
    "    'box_width': 3.5,                 # 框的宽度 / Box width\n",
    "    'box_height': 0.6,                # 框的高度 / Box height\n",
    "    'vertical_spacing': 1.2,          # 垂直间距 / Vertical spacing\n",
    "    'arrow_width': 2,                 # 箭头宽度 / Arrow width\n",
    "    'font_size_title': 11,            # 标题字体大小 / Title font size\n",
    "    'font_size_desc': 9,              # 描述字体大小 / Description font size\n",
    "    \n",
    "    # 颜色方案 / Color scheme\n",
    "    'color_input': '#E3F2FD',         # 输入阶段 / Input phase\n",
    "    'color_processing': '#FFF3E0',    # 处理阶段 / Processing phase  \n",
    "    'color_attention': '#F3E5F5',     # 注意力阶段 / Attention phase\n",
    "    'color_generation': '#E8F5E9',    # 生成阶段 / Generation phase\n",
    "    'color_output': '#FCE4EC',        # 输出阶段 / Output phase\n",
    "    'edge_color': '#424242',          # 边框颜色 / Edge color\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_flowchart(config=FLOWCHART_CONFIG):\n",
    "    \"\"\"\n",
    "    绘制 FlexAttention 代码流程图\n",
    "    Draw FlexAttention code flowchart\n",
    "    \n",
    "    Parameters:\n",
    "        config: 配置字典 / Configuration dictionary\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=config['figure_size'])\n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_ylim(0, 20)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # 中心 x 坐标 / Center x coordinate\n",
    "    center_x = 5\n",
    "    box_w = config['box_width']\n",
    "    box_h = config['box_height']\n",
    "    spacing = config['vertical_spacing']\n",
    "    \n",
    "    # 流程步骤定义 / Flow step definitions\n",
    "    # Format: (y_position, title_en, title_zh, description, color)\n",
    "    steps = [\n",
    "        # 1. Input Phase\n",
    "        (18.5, '1. Input Preparation', '1. 输入准备', \n",
    "         'Load question + paraphrases', config['color_input']),\n",
    "        \n",
    "        (17.3, 'Generate 5 Paraphrases', '生成 5 个改写', \n",
    "         'dataset.construct_prompts()', config['color_input']),\n",
    "        \n",
    "        # 2. Concatenation Phase\n",
    "        (16.1, '2. Concatenation with Position Tracking', '2. 拼接并追踪位置',\n",
    "         'concatenate_paraphrases_with_positions()', config['color_processing']),\n",
    "        \n",
    "        (14.9, 'Track Segment Positions', '记录分段位置',\n",
    "         'segment_positions = [(0,48), (48,95), ...]', config['color_processing']),\n",
    "        \n",
    "        (13.7, 'Tokenization', '分词处理',\n",
    "         'tokenizer(concatenated_text)', config['color_processing']),\n",
    "        \n",
    "        # 3. Attention Mask Creation\n",
    "        (12.5, '3. Create FlexAttention Mask', '3. 创建 FlexAttention 掩码',\n",
    "         'create_flex_attention_mask()', config['color_attention']),\n",
    "        \n",
    "        (11.3, 'Define Mask Function', '定义掩码函数',\n",
    "         'mask_mod(b, h, q_idx, kv_idx) -> bool', config['color_attention']),\n",
    "        \n",
    "        # 4. Model Patching\n",
    "        (10.1, '4. Patch Model with FlexAttention', '4. 为模型打补丁',\n",
    "         'FlexAttentionWrapper.patch_model()', config['color_attention']),\n",
    "        \n",
    "        # 5. Encoding Phase\n",
    "        (8.9, '5. Encoding Phase', '5. 编码阶段',\n",
    "         'Process original tokens with segment isolation', config['color_generation']),\n",
    "        \n",
    "        (7.7, 'Segment-Isolated Attention', '分段隔离注意力',\n",
    "         'Each paraphrase attends only to itself', config['color_generation']),\n",
    "        \n",
    "        # 6. Generation Loop\n",
    "        (6.5, '6. Generation Loop (Auto-regressive)', '6. 生成循环（自回归）',\n",
    "         'for step in range(max_new_tokens):', config['color_generation']),\n",
    "        \n",
    "        (5.3, 'Forward Pass', '前向传播',\n",
    "         'logits = model(input_ids)', config['color_generation']),\n",
    "        \n",
    "        (4.1, 'Fusion Attention', '融合注意力',\n",
    "         'Generated tokens attend to ALL segments', config['color_generation']),\n",
    "        \n",
    "        (2.9, 'Token Selection', '选择令牌',\n",
    "         'next_token = argmax(logits)', config['color_generation']),\n",
    "        \n",
    "        (1.7, 'Update Input', '更新输入',\n",
    "         'input_ids = concat(input_ids, next_token)', config['color_generation']),\n",
    "        \n",
    "        # 7. Output\n",
    "        (0.5, '7. Decode & Return', '7. 解码并返回',\n",
    "         'tokenizer.decode(generated)', config['color_output']),\n",
    "    ]\n",
    "    \n",
    "    # 绘制所有步骤 / Draw all steps\n",
    "    for y, title_en, title_zh, desc, color in steps:\n",
    "        # 绘制框 / Draw box\n",
    "        box = FancyBboxPatch(\n",
    "            (center_x - box_w/2, y - box_h/2),\n",
    "            box_w, box_h,\n",
    "            boxstyle=\"round,pad=0.05\",\n",
    "            facecolor=color,\n",
    "            edgecolor=config['edge_color'],\n",
    "            linewidth=1.5\n",
    "        )\n",
    "        ax.add_patch(box)\n",
    "        \n",
    "        # 添加文字 / Add text\n",
    "        ax.text(center_x, y + 0.15, title_en, \n",
    "                ha='center', va='center', fontsize=config['font_size_title'], \n",
    "                fontweight='bold')\n",
    "        ax.text(center_x, y - 0.15, desc, \n",
    "                ha='center', va='center', fontsize=config['font_size_desc'],\n",
    "                style='italic', color='#555')\n",
    "    \n",
    "    # 绘制箭头 / Draw arrows\n",
    "    for i in range(len(steps) - 1):\n",
    "        y_from = steps[i][0] - box_h/2\n",
    "        y_to = steps[i+1][0] + box_h/2\n",
    "        arrow = FancyArrowPatch(\n",
    "            (center_x, y_from),\n",
    "            (center_x, y_to),\n",
    "            arrowstyle='->,head_width=0.4,head_length=0.4',\n",
    "            color=config['edge_color'],\n",
    "            linewidth=config['arrow_width'],\n",
    "            zorder=1\n",
    "        )\n",
    "        ax.add_patch(arrow)\n",
    "    \n",
    "    # 添加标题 / Add title\n",
    "    ax.text(center_x, 19.5, \n",
    "            'FlexAttention Code Flowchart / FlexAttention 代码流程图',\n",
    "            ha='center', va='center', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 添加图例 / Add legend\n",
    "    legend_elements = [\n",
    "        mpatches.Patch(color=config['color_input'], label='Input / 输入'),\n",
    "        mpatches.Patch(color=config['color_processing'], label='Processing / 处理'),\n",
    "        mpatches.Patch(color=config['color_attention'], label='Attention Mask / 注意力掩码'),\n",
    "        mpatches.Patch(color=config['color_generation'], label='Generation / 生成'),\n",
    "        mpatches.Patch(color=config['color_output'], label='Output / 输出')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='upper right', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# 绘制流程图 / Draw flowchart\n",
    "fig = draw_flowchart()\n",
    "plt.show()\n",
    "\n",
    "# 保存图片 / Save figure\n",
    "# fig.savefig('flowchart.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第二部分：注意力掩码可视化 / Part 2: Attention Mask Visualization\n",
    "\n",
    "### 可修改参数 / Modifiable Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 注意力掩码参数配置 / Attention Mask Configuration Parameters\n",
    "# ============================================================================\n",
    "\n",
    "# 场景 1: 小型示例（完整展示）/ Scenario 1: Small example (full display)\n",
    "MASK_CONFIG_SMALL = {\n",
    "    'num_paraphrases': 3,              # 改写数量 / Number of paraphrases\n",
    "    'tokens_per_paraphrase': 15,       # 每个改写的令牌数 / Tokens per paraphrase\n",
    "    'separator_tokens': 2,             # 分隔符令牌数 / Separator tokens\n",
    "    'num_generated_tokens': 5,         # 生成的令牌数 / Generated tokens\n",
    "    'display_mode': 'full',            # 显示模式: 'full' 或 'sampled' / Display mode\n",
    "}\n",
    "\n",
    "# 场景 2: 中型示例（智能采样）/ Scenario 2: Medium example (smart sampling)\n",
    "MASK_CONFIG_MEDIUM = {\n",
    "    'num_paraphrases': 5,              # 改写数量 / Number of paraphrases\n",
    "    'tokens_per_paraphrase': 25,       # 每个改写的令牌数 / Tokens per paraphrase\n",
    "    'separator_tokens': 3,             # 分隔符令牌数 / Separator tokens\n",
    "    'num_generated_tokens': 8,         # 生成的令牌数 / Generated tokens\n",
    "    'display_mode': 'sampled',         # 显示模式 / Display mode\n",
    "    'max_display_positions': 30,       # 最大显示位置数 / Max display positions\n",
    "}\n",
    "\n",
    "# 场景 3: 大型示例（真实场景）/ Scenario 3: Large example (realistic scenario)\n",
    "MASK_CONFIG_LARGE = {\n",
    "    'num_paraphrases': 5,              # 改写数量 / Number of paraphrases  \n",
    "    'tokens_per_paraphrase': 50,       # 每个改写的令牌数 / Tokens per paraphrase\n",
    "    'separator_tokens': 4,             # 分隔符令牌数 / Separator tokens\n",
    "    'num_generated_tokens': 15,        # 生成的令牌数 / Generated tokens\n",
    "    'display_mode': 'sampled',         # 显示模式 / Display mode\n",
    "    'max_display_positions': 35,       # 最大显示位置数 / Max display positions\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_segment_positions(config):\n",
    "    \"\"\"\n",
    "    根据配置创建分段位置\n",
    "    Create segment positions based on configuration\n",
    "    \n",
    "    Returns:\n",
    "        segment_positions: List of (start, end) tuples\n",
    "        original_length: Total length of original content\n",
    "        total_length: Total length including generated tokens\n",
    "    \"\"\"\n",
    "    segment_positions = []\n",
    "    current_pos = 0\n",
    "    \n",
    "    for i in range(config['num_paraphrases']):\n",
    "        if i > 0:\n",
    "            # Add separator\n",
    "            current_pos += config['separator_tokens']\n",
    "        \n",
    "        start = current_pos\n",
    "        end = current_pos + config['tokens_per_paraphrase']\n",
    "        segment_positions.append((start, end))\n",
    "        current_pos = end\n",
    "    \n",
    "    original_length = current_pos\n",
    "    total_length = original_length + config['num_generated_tokens']\n",
    "    \n",
    "    return segment_positions, original_length, total_length\n",
    "\n",
    "\n",
    "def create_attention_mask_function(segment_positions, original_length):\n",
    "    \"\"\"\n",
    "    创建注意力掩码函数（与 flex_attention_generate.py 中的实现一致）\n",
    "    Create attention mask function (consistent with implementation in flex_attention_generate.py)\n",
    "    \n",
    "    Args:\n",
    "        segment_positions: List of (start, end) tuples\n",
    "        original_length: Length of original content\n",
    "        \n",
    "    Returns:\n",
    "        mask_func: Function (b, h, q_idx, kv_idx) -> bool\n",
    "    \"\"\"\n",
    "    def mask_func(b, h, q_idx, kv_idx):\n",
    "        # 因果约束 / Causal constraint\n",
    "        if q_idx < kv_idx:\n",
    "            return False\n",
    "        \n",
    "        # 生成的令牌可以关注所有之前的内容 / Generated tokens can attend to all previous\n",
    "        if q_idx >= original_length:\n",
    "            return True\n",
    "        \n",
    "        # 原始令牌只在同一分段内关注 / Original tokens only attend within same segment\n",
    "        q_segment = None\n",
    "        kv_segment = None\n",
    "        \n",
    "        for seg_id, (start, end) in enumerate(segment_positions):\n",
    "            if start <= q_idx < end:\n",
    "                q_segment = seg_id\n",
    "            if start <= kv_idx < end:\n",
    "                kv_segment = seg_id\n",
    "        \n",
    "        if q_segment is not None and kv_segment is not None:\n",
    "            return q_segment == kv_segment\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    return mask_func\n",
    "\n",
    "\n",
    "def smart_sample_positions(segment_positions, original_length, total_length, max_positions=30):\n",
    "    \"\"\"\n",
    "    智能采样位置以展示掩码结构\n",
    "    Smart sampling of positions to show mask structure\n",
    "    \n",
    "    Args:\n",
    "        segment_positions: List of (start, end) tuples\n",
    "        original_length: Length of original content\n",
    "        total_length: Total sequence length\n",
    "        max_positions: Maximum number of positions to display\n",
    "        \n",
    "    Returns:\n",
    "        positions: List of sampled position indices\n",
    "    \"\"\"\n",
    "    positions = set()\n",
    "    \n",
    "    # 1. 添加分段边界 / Add segment boundaries\n",
    "    for start, end in segment_positions:\n",
    "        positions.add(start)\n",
    "        positions.add(end - 1)\n",
    "        # 在分段中间添加一些位置 / Add some positions in the middle\n",
    "        segment_len = end - start\n",
    "        if segment_len > 4:\n",
    "            positions.add(start + segment_len // 3)\n",
    "            positions.add(start + 2 * segment_len // 3)\n",
    "    \n",
    "    # 2. 添加原始长度边界 / Add original_length boundary\n",
    "    if original_length < total_length:\n",
    "        positions.add(original_length - 1)\n",
    "        positions.add(original_length)\n",
    "    \n",
    "    # 3. 添加生成的令牌位置 / Add generated token positions\n",
    "    if total_length > original_length:\n",
    "        gen_count = min(5, total_length - original_length)\n",
    "        for i in range(gen_count):\n",
    "            positions.add(original_length + i)\n",
    "    \n",
    "    # 4. 添加最后位置 / Add last position\n",
    "    positions.add(total_length - 1)\n",
    "    \n",
    "    # 5. 填充剩余位置 / Fill remaining positions\n",
    "    positions_list = sorted(list(positions))\n",
    "    while len(positions_list) < max_positions and len(positions_list) < total_length:\n",
    "        # 找到最大间隙 / Find largest gap\n",
    "        max_gap = 0\n",
    "        max_gap_idx = 0\n",
    "        for i in range(len(positions_list) - 1):\n",
    "            gap = positions_list[i+1] - positions_list[i]\n",
    "            if gap > max_gap:\n",
    "                max_gap = gap\n",
    "                max_gap_idx = i\n",
    "        \n",
    "        if max_gap <= 1:\n",
    "            break\n",
    "        \n",
    "        # 在最大间隙中插入中点 / Insert midpoint in largest gap\n",
    "        mid = (positions_list[max_gap_idx] + positions_list[max_gap_idx + 1]) // 2\n",
    "        positions_list.insert(max_gap_idx + 1, mid)\n",
    "    \n",
    "    return sorted(positions_list[:max_positions])\n",
    "\n",
    "\n",
    "def visualize_attention_mask(config, title_suffix=''):\n",
    "    \"\"\"\n",
    "    可视化注意力掩码矩阵\n",
    "    Visualize attention mask matrix\n",
    "    \n",
    "    Args:\n",
    "        config: Configuration dictionary\n",
    "        title_suffix: Additional title text\n",
    "    \"\"\"\n",
    "    # 创建分段位置 / Create segment positions\n",
    "    segment_positions, original_length, total_length = create_segment_positions(config)\n",
    "    \n",
    "    # 创建掩码函数 / Create mask function\n",
    "    mask_func = create_attention_mask_function(segment_positions, original_length)\n",
    "    \n",
    "    # 确定显示位置 / Determine display positions\n",
    "    if config['display_mode'] == 'full' or total_length <= 25:\n",
    "        positions = list(range(total_length))\n",
    "        display_info = f\"Full display: {total_length}×{total_length}\"\n",
    "    else:\n",
    "        max_pos = config.get('max_display_positions', 30)\n",
    "        positions = smart_sample_positions(segment_positions, original_length, \n",
    "                                          total_length, max_pos)\n",
    "        display_info = f\"Sampled display: showing {len(positions)} of {total_length} positions\"\n",
    "    \n",
    "    # 创建掩码矩阵 / Create mask matrix\n",
    "    n_pos = len(positions)\n",
    "    mask_matrix = np.zeros((n_pos, n_pos))\n",
    "    for i, q in enumerate(positions):\n",
    "        for j, kv in enumerate(positions):\n",
    "            mask_matrix[i, j] = 1 if mask_func(0, 0, q, kv) else 0\n",
    "    \n",
    "    # 绘制掩码矩阵 / Plot mask matrix\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    \n",
    "    # 使用颜色映射 / Use color mapping\n",
    "    cmap = plt.cm.colors.ListedColormap(['white', '#2E7D32'])  # white=cannot attend, green=can attend\n",
    "    im = ax.imshow(mask_matrix, cmap=cmap, aspect='auto', interpolation='nearest')\n",
    "    \n",
    "    # 设置刻度 / Set ticks\n",
    "    ax.set_xticks(range(n_pos))\n",
    "    ax.set_yticks(range(n_pos))\n",
    "    ax.set_xticklabels([str(p) for p in positions], rotation=45, ha='right', fontsize=8)\n",
    "    ax.set_yticklabels([str(p) for p in positions], fontsize=8)\n",
    "    \n",
    "    # 添加网格 / Add grid\n",
    "    ax.set_xticks(np.arange(n_pos) - 0.5, minor=True)\n",
    "    ax.set_yticks(np.arange(n_pos) - 0.5, minor=True)\n",
    "    ax.grid(which='minor', color='gray', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "    \n",
    "    # 添加分段边界线 / Add segment boundary lines\n",
    "    for start, end in segment_positions:\n",
    "        if start in positions:\n",
    "            idx = positions.index(start)\n",
    "            ax.axhline(y=idx - 0.5, color='red', linewidth=2, linestyle='--', alpha=0.6)\n",
    "            ax.axvline(x=idx - 0.5, color='red', linewidth=2, linestyle='--', alpha=0.6)\n",
    "    \n",
    "    # 标记生成开始位置 / Mark generation start\n",
    "    if original_length in positions:\n",
    "        idx = positions.index(original_length)\n",
    "        ax.axhline(y=idx - 0.5, color='blue', linewidth=2, linestyle='--', alpha=0.6)\n",
    "        ax.axvline(x=idx - 0.5, color='blue', linewidth=2, linestyle='--', alpha=0.6)\n",
    "    \n",
    "    # 设置标签 / Set labels\n",
    "    ax.set_xlabel('Key/Value Position (KV)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Query Position (Q)', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # 标题 / Title\n",
    "    title = f'Attention Mask Visualization{title_suffix}\\n{display_info}'\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    # 添加颜色条 / Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    cbar.set_ticks([0.25, 0.75])\n",
    "    cbar.set_ticklabels(['Cannot Attend\\n不可关注', 'Can Attend\\n可关注'])\n",
    "    \n",
    "    # 添加信息文本 / Add info text\n",
    "    info_text = (\n",
    "        f\"Configuration / 配置:\\n\"\n",
    "        f\"  • Paraphrases / 改写数: {config['num_paraphrases']}\\n\"\n",
    "        f\"  • Tokens per paraphrase / 每段令牌数: {config['tokens_per_paraphrase']}\\n\"\n",
    "        f\"  • Original length / 原始长度: {original_length}\\n\"\n",
    "        f\"  • Generated tokens / 生成令牌数: {config['num_generated_tokens']}\\n\"\n",
    "        f\"  • Total length / 总长度: {total_length}\\n\\n\"\n",
    "        f\"Legend / 图例:\\n\"\n",
    "        f\"  ━━ Red dashed / 红色虚线: Segment boundary / 分段边界\\n\"\n",
    "        f\"  ━━ Blue dashed / 蓝色虚线: Generation start / 生成开始\"\n",
    "    )\n",
    "    \n",
    "    # 在右侧添加文本框 / Add text box on the right\n",
    "    plt.gcf().text(0.98, 0.5, info_text, \n",
    "                   fontsize=9, \n",
    "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3),\n",
    "                   verticalalignment='center',\n",
    "                   horizontalalignment='left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, mask_matrix, positions\n",
    "\n",
    "\n",
    "print(\"函数定义完成！/ Function definitions complete!\")\n",
    "print(\"现在可以使用不同的配置来可视化注意力掩码 / Now you can visualize attention masks with different configs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 场景 1：小型示例（完整展示）/ Scenario 1: Small Example (Full Display)\n",
    "\n",
    "适合理解基本原理 / Good for understanding basic principles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化小型示例 / Visualize small example\n",
    "fig1, matrix1, pos1 = visualize_attention_mask(\n",
    "    MASK_CONFIG_SMALL, \n",
    "    title_suffix=' - Small Example / 小型示例'\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# 打印统计信息 / Print statistics\n",
    "print(f\"\\n小型示例统计 / Small Example Statistics:\")\n",
    "print(f\"  矩阵大小 / Matrix size: {matrix1.shape}\")\n",
    "print(f\"  可关注的位置数 / Attention-allowed positions: {np.sum(matrix1):.0f}\")\n",
    "print(f\"  总位置数 / Total positions: {matrix1.size}\")\n",
    "print(f\"  注意力比例 / Attention ratio: {np.sum(matrix1)/matrix1.size*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 场景 2：中型示例（智能采样）/ Scenario 2: Medium Example (Smart Sampling)\n",
    "\n",
    "展示采样策略如何保持结构 / Shows how sampling strategy preserves structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化中型示例 / Visualize medium example\n",
    "fig2, matrix2, pos2 = visualize_attention_mask(\n",
    "    MASK_CONFIG_MEDIUM,\n",
    "    title_suffix=' - Medium Example / 中型示例'\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# 打印统计信息 / Print statistics\n",
    "print(f\"\\n中型示例统计 / Medium Example Statistics:\")\n",
    "print(f\"  显示位置数 / Displayed positions: {len(pos2)}\")\n",
    "print(f\"  矩阵大小 / Matrix size: {matrix2.shape}\")\n",
    "print(f\"  可关注的位置数 / Attention-allowed positions: {np.sum(matrix2):.0f}\")\n",
    "print(f\"  注意力比例 / Attention ratio: {np.sum(matrix2)/matrix2.size*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 场景 3：大型示例（真实场景）/ Scenario 3: Large Example (Realistic Scenario)\n",
    "\n",
    "模拟真实使用场景 / Simulates realistic use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化大型示例 / Visualize large example\n",
    "fig3, matrix3, pos3 = visualize_attention_mask(\n",
    "    MASK_CONFIG_LARGE,\n",
    "    title_suffix=' - Large Example / 大型示例 (Realistic)'\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# 打印统计信息 / Print statistics\n",
    "segment_pos, orig_len, total_len = create_segment_positions(MASK_CONFIG_LARGE)\n",
    "print(f\"\\n大型示例统计 / Large Example Statistics:\")\n",
    "print(f\"  实际总长度 / Actual total length: {total_len}\")\n",
    "print(f\"  显示位置数 / Displayed positions: {len(pos3)}\")\n",
    "print(f\"  采样率 / Sampling rate: {len(pos3)/total_len*100:.1f}%\")\n",
    "print(f\"  矩阵大小 / Matrix size: {matrix3.shape}\")\n",
    "print(f\"  可关注的位置数 / Attention-allowed positions: {np.sum(matrix3):.0f}\")\n",
    "print(f\"  注意力比例 / Attention ratio: {np.sum(matrix3)/matrix3.size*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第三部分：自定义配置实验 / Part 3: Custom Configuration Experimentation\n",
    "\n",
    "### 修改参数进行实验 / Modify parameters for experimentation\n",
    "\n",
    "你可以在下面的代码单元格中修改任何参数来探索不同的场景：\n",
    "You can modify any parameters in the cell below to explore different scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 自定义配置 / Custom Configuration\n",
    "# ============================================================================\n",
    "# 修改下面的参数来创建你自己的场景！\n",
    "# Modify the parameters below to create your own scenario!\n",
    "\n",
    "CUSTOM_CONFIG = {\n",
    "    'num_paraphrases': 4,              # 尝试修改这个值！/ Try changing this!\n",
    "    'tokens_per_paraphrase': 30,       # 尝试修改这个值！/ Try changing this!\n",
    "    'separator_tokens': 3,             \n",
    "    'num_generated_tokens': 10,        # 尝试修改这个值！/ Try changing this!\n",
    "    'display_mode': 'sampled',         # 可选: 'full' 或 'sampled' / Options: 'full' or 'sampled'\n",
    "    'max_display_positions': 30,       \n",
    "}\n",
    "\n",
    "# 可视化自定义配置 / Visualize custom configuration\n",
    "fig_custom, matrix_custom, pos_custom = visualize_attention_mask(\n",
    "    CUSTOM_CONFIG,\n",
    "    title_suffix=' - Custom Configuration / 自定义配置'\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# 打印详细信息 / Print detailed information\n",
    "segment_pos_custom, orig_len_custom, total_len_custom = create_segment_positions(CUSTOM_CONFIG)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"自定义配置详细信息 / Custom Configuration Details\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n分段信息 / Segment Information:\")\n",
    "for i, (start, end) in enumerate(segment_pos_custom):\n",
    "    print(f\"  Segment {i+1}: positions {start:3d} - {end-1:3d} (length: {end-start})\")\n",
    "\n",
    "print(f\"\\n长度统计 / Length Statistics:\")\n",
    "print(f\"  原始内容长度 / Original content length: {orig_len_custom}\")\n",
    "print(f\"  生成令牌数 / Generated tokens: {CUSTOM_CONFIG['num_generated_tokens']}\")\n",
    "print(f\"  总长度 / Total length: {total_len_custom}\")\n",
    "\n",
    "print(f\"\\n显示信息 / Display Information:\")\n",
    "print(f\"  显示位置数 / Displayed positions: {len(pos_custom)}\")\n",
    "print(f\"  采样率 / Sampling rate: {len(pos_custom)/total_len_custom*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n注意力统计 / Attention Statistics:\")\n",
    "print(f\"  矩阵大小 / Matrix size: {matrix_custom.shape}\")\n",
    "print(f\"  可关注位置 / Attention-allowed: {np.sum(matrix_custom):.0f}\")\n",
    "print(f\"  总位置数 / Total positions: {matrix_custom.size}\")\n",
    "print(f\"  注意力比例 / Attention ratio: {np.sum(matrix_custom)/matrix_custom.size*100:.1f}%\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第四部分：注意力模式分析 / Part 4: Attention Pattern Analysis\n",
    "\n",
    "### 对比不同阶段的注意力模式 / Compare attention patterns in different phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_attention_patterns(config):\n",
    "    \"\"\"\n",
    "    分析不同查询位置的注意力模式\n",
    "    Analyze attention patterns for different query positions\n",
    "    \"\"\"\n",
    "    segment_positions, original_length, total_length = create_segment_positions(config)\n",
    "    mask_func = create_attention_mask_function(segment_positions, original_length)\n",
    "    \n",
    "    # 选择代表性查询位置 / Select representative query positions\n",
    "    query_positions = [\n",
    "        segment_positions[0][0] + 5,  # 第1段中间 / Middle of segment 1\n",
    "        segment_positions[-1][0] + 5,  # 最后一段中间 / Middle of last segment\n",
    "        original_length,               # 第一个生成令牌 / First generated token\n",
    "        total_length - 1,              # 最后一个生成令牌 / Last generated token\n",
    "    ]\n",
    "    \n",
    "    labels = [\n",
    "        f'Segment 1 (pos {query_positions[0]})',\n",
    "        f'Last Segment (pos {query_positions[1]})',\n",
    "        f'First Generated (pos {query_positions[2]})',\n",
    "        f'Last Generated (pos {query_positions[3]})'\n",
    "    ]\n",
    "    \n",
    "    # 创建子图 / Create subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, (q_pos, label) in enumerate(zip(query_positions, labels)):\n",
    "        # 计算这个查询位置对所有键位置的注意力 / Calculate attention for all key positions\n",
    "        attention_pattern = np.array([mask_func(0, 0, q_pos, kv) for kv in range(total_length)])\n",
    "        \n",
    "        # 绘制柱状图 / Plot bar chart\n",
    "        ax = axes[idx]\n",
    "        colors = ['green' if val else 'lightgray' for val in attention_pattern]\n",
    "        ax.bar(range(total_length), attention_pattern, color=colors, width=1.0, edgecolor='none')\n",
    "        \n",
    "        # 标记分段边界 / Mark segment boundaries\n",
    "        for start, end in segment_positions:\n",
    "            ax.axvline(x=start, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "        ax.axvline(x=original_length, color='blue', linestyle='--', linewidth=2, alpha=0.7)\n",
    "        \n",
    "        # 设置标签和标题 / Set labels and title\n",
    "        ax.set_xlabel('Key/Value Position')\n",
    "        ax.set_ylabel('Can Attend')\n",
    "        ax.set_title(f'Attention Pattern for {label}', fontweight='bold')\n",
    "        ax.set_ylim(-0.1, 1.1)\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # 添加统计信息 / Add statistics\n",
    "        attend_count = np.sum(attention_pattern)\n",
    "        attend_ratio = attend_count / total_length * 100\n",
    "        ax.text(0.98, 0.95, f'Attend: {attend_count:.0f}/{total_length} ({attend_ratio:.1f}%)',\n",
    "                transform=ax.transAxes, ha='right', va='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.suptitle('Attention Patterns Analysis / 注意力模式分析', \n",
    "                 fontsize=16, fontweight='bold', y=1.00)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "# 分析中型示例的注意力模式 / Analyze attention patterns for medium example\n",
    "fig_patterns = analyze_attention_patterns(MASK_CONFIG_MEDIUM)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n注意力模式解读 / Attention Pattern Interpretation:\")\n",
    "print(\"  • 编码阶段（前两个图）：每个分段只关注自己 / Encoding phase (first two plots): Each segment attends only to itself\")\n",
    "print(\"  • 生成阶段（后两个图）：生成的令牌可以关注所有内容 / Generation phase (last two plots): Generated tokens attend to all content\")\n",
    "print(\"  • 这种设计允许融合来自所有改写的信息 / This design allows fusion of information from all paraphrases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第五部分：掩码矩阵导出 / Part 5: Mask Matrix Export\n",
    "\n",
    "### 保存图片和数据 / Save figures and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 创建输出目录 / Create output directory\n",
    "output_dir = 'attention_mask_outputs'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"保存可视化结果到 / Saving visualizations to: {output_dir}/\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 保存流程图 / Save flowchart\n",
    "if 'fig' in locals():\n",
    "    flowchart_path = os.path.join(output_dir, 'flowchart.png')\n",
    "    fig.savefig(flowchart_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"✓ 流程图已保存 / Flowchart saved: {flowchart_path}\")\n",
    "\n",
    "# 保存注意力掩码可视化 / Save attention mask visualizations\n",
    "if 'fig1' in locals():\n",
    "    fig1.savefig(os.path.join(output_dir, 'mask_small.png'), dpi=300, bbox_inches='tight')\n",
    "    print(f\"✓ 小型示例掩码已保存 / Small example mask saved\")\n",
    "\n",
    "if 'fig2' in locals():\n",
    "    fig2.savefig(os.path.join(output_dir, 'mask_medium.png'), dpi=300, bbox_inches='tight')\n",
    "    print(f\"✓ 中型示例掩码已保存 / Medium example mask saved\")\n",
    "\n",
    "if 'fig3' in locals():\n",
    "    fig3.savefig(os.path.join(output_dir, 'mask_large.png'), dpi=300, bbox_inches='tight')\n",
    "    print(f\"✓ 大型示例掩码已保存 / Large example mask saved\")\n",
    "\n",
    "if 'fig_custom' in locals():\n",
    "    fig_custom.savefig(os.path.join(output_dir, 'mask_custom.png'), dpi=300, bbox_inches='tight')\n",
    "    print(f\"✓ 自定义掩码已保存 / Custom mask saved\")\n",
    "\n",
    "# 保存注意力模式分析 / Save attention pattern analysis\n",
    "if 'fig_patterns' in locals():\n",
    "    fig_patterns.savefig(os.path.join(output_dir, 'attention_patterns.png'), dpi=300, bbox_inches='tight')\n",
    "    print(f\"✓ 注意力模式分析已保存 / Attention patterns saved\")\n",
    "\n",
    "# 保存掩码矩阵为 numpy 数组 / Save mask matrices as numpy arrays\n",
    "if 'matrix_custom' in locals():\n",
    "    np.save(os.path.join(output_dir, 'mask_matrix_custom.npy'), matrix_custom)\n",
    "    print(f\"✓ 自定义掩码矩阵已保存 / Custom mask matrix saved (numpy)\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n所有文件已保存到目录 / All files saved to: {output_dir}/\")\n",
    "print(f\"\\n可以使用以下代码加载掩码矩阵 / Load mask matrix with:\")\n",
    "print(f\"  matrix = np.load('{output_dir}/mask_matrix_custom.npy')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 总结 / Summary\n",
    "\n",
    "### 本笔记本提供的功能 / Features Provided by This Notebook:\n",
    "\n",
    "1. **完整的代码流程图** - 展示从输入到输出的整个处理流程 / **Complete code flowchart** - Shows entire processing pipeline from input to output\n",
    "\n",
    "2. **精确的注意力掩码可视化** - 三种预设场景 + 自定义配置 / **Precise attention mask visualization** - Three preset scenarios + custom configuration\n",
    "\n",
    "3. **智能采样策略** - 处理大型序列时保持结构可见性 / **Smart sampling strategy** - Maintains structure visibility for large sequences\n",
    "\n",
    "4. **注意力模式分析** - 对比编码和生成阶段的注意力行为 / **Attention pattern analysis** - Compares encoding and generation phase behavior\n",
    "\n",
    "5. **易于修改的参数** - 所有配置集中在配置字典中 / **Easy-to-modify parameters** - All configs centralized in dictionaries\n",
    "\n",
    "6. **导出功能** - 保存所有可视化和数据 / **Export functionality** - Save all visualizations and data\n",
    "\n",
    "### 如何使用 / How to Use:\n",
    "\n",
    "1. **查看预设示例**：运行所有单元格查看三种预设场景 / **View preset examples**: Run all cells to see three preset scenarios\n",
    "\n",
    "2. **自定义实验**：修改 `CUSTOM_CONFIG` 字典中的参数 / **Custom experimentation**: Modify parameters in `CUSTOM_CONFIG` dictionary\n",
    "\n",
    "3. **调整外观**：修改 `FLOWCHART_CONFIG` 来改变流程图样式 / **Adjust appearance**: Modify `FLOWCHART_CONFIG` to change flowchart style\n",
    "\n",
    "4. **保存结果**：运行最后一个单元格导出所有可视化 / **Save results**: Run last cell to export all visualizations\n",
    "\n",
    "### 关键见解 / Key Insights:\n",
    "\n",
    "- **编码阶段**：每个改写在其自己的分段内隔离处理 / **Encoding phase**: Each paraphrase processed in isolation within its segment\n",
    "- **生成阶段**：新令牌可以关注所有先前内容，实现融合 / **Generation phase**: New tokens attend to all previous content, enabling fusion\n",
    "- **掩码形状**：清晰展示块对角结构（编码）+ 全关注（生成）/ **Mask shape**: Clearly shows block-diagonal structure (encoding) + full attention (generation)\n",
    "\n",
    "---\n",
    "\n",
    "**需要帮助？/ Need Help?**\n",
    "\n",
    "- 查看 `flex_attention_generate.py` 了解实现细节 / See `flex_attention_generate.py` for implementation details\n",
    "- 查看 `test_mask_visualization.py` 了解更多测试示例 / See `test_mask_visualization.py` for more test examples\n",
    "- 查看 `docs/ARCHITECTURE.md` 了解架构说明 / See `docs/ARCHITECTURE.md` for architecture documentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flexattention",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
