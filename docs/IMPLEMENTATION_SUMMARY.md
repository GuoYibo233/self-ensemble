# Implementation Summary

## Overview

This document summarizes the FlexAttention-based ensemble generation implementation, which concatenates multiple paraphrases and uses FlexAttention for fusion while ensuring each paraphrase's tokens only attend to themselves during encoding and allowing fusion during generation.

## Reused from generate.py

### 1. Fully Reused Functions (No Modification)

| Function | Original Location | Purpose |
|----------|-------------------|---------|
| `init_spacy()` | Line 19 | Initialize spacy for lemmatization |
| `lemmaize_predicts()` | Line 23 | Lemmatize predictions |
| `lemmaize_chunk()` | Line 28 | Batch lemmatization |
| `append_lemmas()` | Line 36 | Add lemmatized results to DataFrame |

These 4 evaluation-related functions are reused without modification.

### 2. Reused Code Patterns

#### 2.1 Model Configuration
```python
# Original location: generate.py lines 47-50, 78-81
tokenizer.pad_token_id = tokenizer.eos_token_id
model.generation_config.temperature = None
model.generation_config.top_p = None
model.generation_config.pad_token_id = tokenizer.eos_token_id
```

#### 2.2 Input Tokenization
```python
# Original location: generate.py lines 52-55
inputs = tokenizer(
    prompts, return_tensors="pt", 
    padding=True, truncation=True, 
    padding_side='left', return_attention_mask=True
).to(model.device)
```

#### 2.3 Generation Loop Structure
```python
# Original location: generate.py lines 59-70
for step in range(max_new_tokens):
    logits = model(inputs["input_ids"]).logits[:, -1, :]
    next_token = torch.argmax(logits, dim=-1).unsqueeze(1)
    inputs["input_ids"] = torch.cat([inputs["input_ids"], next_token], dim=1)
```

#### 2.4 Dataset Loading
```python
# Original location: generate.py lines 157-164
if args.dataset == "webqa":
    dataset = WebQADataset(model_name=args.model)
elif args.dataset == "myriadlama":
    dataset = MyriadLamaDataset(model_name=args.model)
```

#### 2.5 Model Loading
```python
# Original location: generate.py lines 178-180, 240-242
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(
    model_path, device_map=args.device, torch_dtype="auto"
)
tokenizer.pad_token = tokenizer.eos_token
```

### 3. Reused Overall Flow

1. Argument parsing â† Reused from generate.py
2. Dataset loading â† Fully reused
3. Model loading â† Fully reused
4. Few-shot construction â† Fully reused
5. Main processing loop â† Reused structure
6. Lemmatization (optional) â† Fully reused
7. Save results â† Fully reused

## New Implementation

### 1. New Functions

#### 1.1 `concatenate_paraphrases_with_positions()`
**Purpose**: Concatenate multiple paraphrases into a single prompt and track token positions

**Inputs**:
- `prompts`: List of 5 paraphrases
- `tokenizer`: HuggingFace tokenizer
- `separator`: Separator token (default `" [SEP] "`)

**Outputs**:
- `concatenated_text`: Concatenated text
- `segment_positions`: Position list for each paraphrase, e.g., `[(0, 120), (125, 245), ...]`
- `total_length`: Total token count

#### 1.2 `create_segment_isolation_mask()`
**Purpose**: Create FlexAttention mask function for segment isolation

**Inputs**:
- `segment_positions`: Segment position list
- `original_length`: Original concatenated sequence length

**Output**:
- `mask_mod`: Mask function `(b, h, q_idx, kv_idx) -> bool`

**Mask Rules**:
1. **Causal constraint**: Return `False` when `q_idx < kv_idx` (cannot attend to future)
2. **Segment isolation** (encoding): Original sequence tokens only attend to same segment
3. **Fusion** (generation): New tokens (`q_idx >= original_length`) can attend to all previous tokens

**Test Results**: âœ… 19/19 test cases passed

#### 1.3 `FlexAttentionWrapper` Class
**Purpose**: Integrate FlexAttention into existing model via monkey patching

**Methods**:
- `patch_model()`: Replace all attention layers with FlexAttention
- `unpatch_model()`: Restore original attention implementation
- `create_patched_forward()`: Create custom forward function with FlexAttention

#### 1.4 `flex_attention_generation()`
**Purpose**: Main generation function coordinating entire FlexAttention generation flow

**Flow**:
1. Set model configuration (reused pattern)
2. **NEW**: Concatenate paraphrases and track positions
3. Tokenize input (reused pattern)
4. Create FlexAttention wrapper
5. Generation loop (reused structure):
   - **NEW**: Create mask for current sequence length
   - **NEW**: Patch model with FlexAttention
   - Forward pass
   - **NEW**: Unpatch model
   - Select next token (reused pattern)
6. Decode output (reused pattern)

## Technical Comparison

| Aspect | generate.py | flex_attention_generate.py |
|--------|-------------|----------------------------|
| **Input Processing** | Process each paraphrase separately | Concatenate into single sequence |
| **Attention** | Standard self-attention | FlexAttention + segment isolation |
| **Fusion Method** | Logit averaging/max | Attention-based fusion |
| **Model Modification** | None | Temporary monkey patching |
| **Position Tracking** | Not needed | Track token positions per segment |
| **Generation** | Generate independently then fuse | Unified generation with automatic fusion |

## Core Innovation

### 1. Attention-based Fusion
Traditional methods (generate.py) fuse at **logits level**:
```
Paraphrase 1 â†’ Model â†’ Logits 1 â”
Paraphrase 2 â†’ Model â†’ Logits 2 â”œâ†’ Averaging/Max â†’ Final Token
Paraphrase 3 â†’ Model â†’ Logits 3 â”˜
```

FlexAttention method fuses at **attention level**:
```
[Para1 [SEP] Para2 [SEP] Para3] â†’ Model with FlexAttention â†’ Token
                                   â†‘
                            Isolated encoding + Fused generation
```

### 2. Segment Isolation Attention Mask

```
Encoding phase (original sequence):
  Para1: âœ“âœ“âœ“âœ—âœ—âœ—âœ—âœ—âœ—  (Only attend to self)
  Para2: âœ—âœ—âœ—âœ“âœ“âœ“âœ—âœ—âœ—  (Only attend to self)
  Para3: âœ—âœ—âœ—âœ—âœ—âœ—âœ“âœ“âœ“  (Only attend to self)

Generation phase (new tokens):
  Gen1:  âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“  (Attend to all segments)
  Gen2:  âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“ (Attend to all segments + previous generated token)
```

### 3. Design Rationale

**Encoding Phase Isolation**:
- Ensure each paraphrase is independently encoded
- Maintain diversity
- Avoid information leakage

**Generation Phase Fusion**:
- New tokens can extract information from all paraphrases
- Similar to ensemble effect
- Automatically learn optimal fusion

## Code Reuse Statistics

Total approximately **554 lines** of code:
- **Reused** from generate.py: ~**300 lines** (54%)
- **New implementation** FlexAttention: ~**254 lines** (46%)

Including:
- Fully reused functions: 4
- Reused code patterns: 10+
- New functions: 4
- New classes: 1

## Summary

### Reused Core Components
1. âœ… All evaluation-related functions (lemmatization)
2. âœ… Dataset loading and processing flow
3. âœ… Model loading and configuration
4. âœ… Few-shot prompt construction
5. âœ… Basic generation loop structure
6. âœ… Result storage and file management
7. âœ… Command-line argument interface

### New Core Components
1. ğŸ†• Paraphrase concatenation with position tracking
2. ğŸ†• FlexAttention mask creation
3. ğŸ†• Model attention layer monkey patching
4. ğŸ†• Integrated generation function

### Key Features
- **High reuse rate**: 54% of code directly reused from `generate.py`
- **Modular design**: New functionality encapsulated in independent functions/classes
- **Good compatibility**: Fully compatible with existing datasets and evaluation pipelines
- **Robust**: Complete error handling and fallback mechanisms
- **Easy to maintain**: Clear comments indicating reused vs. new code
