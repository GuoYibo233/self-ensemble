{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90245073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas\n",
    "from functools import partial\n",
    "from _utils import calculate_accuracy\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f087ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/y-guo/self-ensemble/self-ensemble')\n",
    "sys.path.insert(0, '/home/y-guo/self-ensemble/self-ensemble/notebook')\n",
    "\n",
    "# Import utils from notebook directory (has take_until_punct_or_space)\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a5108f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: For every model，For every perprompt.feather, calculate every (TT TF FT FF) with same shots and save to a new file for example, for /home/xzhao/workspace/GYB_self-ensemble/datasets/myriadlama/llama3.2_1b_it/baseline_per_prompt.0shots.feather, let it be tpath and calculate the (TT TF FT FF), here let /home/xzhao/workspace/GYB_self-ensemble/datasets/myriadlama/llama3.2_1b_it/myriadlama.logits.avg.0fshots.5samples.5paras.feather , vscode-remote://ssh-remote%2Btokyo106/home/xzhao/workspace/GYB_self-ensemble/datasets/myriadlama/llama3.2_1b_it/myriadlama.logits.avg.avglayer.layer12.alpha100.token-last.multilayer.0fshots.5samples.5paras.feather and so on be t2path, note thet the shots (shown in file neme) should be the same. read /home/xzhao/workspace/GYB_self-ensemble/datasets/myriadlama for file structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d1018063",
   "metadata": {},
   "outputs": [],
   "source": [
    "tpath = f\"/home/xzhao/workspace/GYB_self-ensemble/datasets/myriadlama/llama3.2_3b/baseline_per_prompt.0shots.feather\"\n",
    "df_perprompt = pandas.read_feather(tpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "7738bf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2path = f\"/home/xzhao/workspace/GYB_self-ensemble/datasets/myriadlama/llama3.2_3b/myriadlama.logits.avg.0fshots.5samples.5paras.feather\"\n",
    "df_logit = pandas.read_feather(t2path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c19787c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF(generations, _gold_answers):\n",
    "    generations = utils.take_until_punct_or_space(generations)\n",
    "    if len(generations) == 0:\n",
    "        return False\n",
    "    return utils.partial_match(generations, _gold_answers, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2faeb5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BASE_PATH = \"/home/y-guo/self-ensemble/myriadlama\"\n",
    "OUTPUT_BASE = \"/home/y-guo/self-ensemble/analyzeResults\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_BASE, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd3b9f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_shot_value(filename):\n",
    "    \"\"\"Extract shot value from baseline or myriadlama filename.\n",
    "    \n",
    "    Args:\n",
    "        filename: Name of the file (not full path)\n",
    "        \n",
    "    Returns:\n",
    "        Shot value as integer, or None if not found\n",
    "    \"\"\"\n",
    "    # Baseline pattern: baseline_per_prompt.{X}shots.feather\n",
    "    baseline_match = re.search(r'baseline_per_prompt\\.(\\d+)shots\\.feather', filename)\n",
    "    if baseline_match:\n",
    "        return int(baseline_match.group(1))\n",
    "    \n",
    "    # Myriadlama pattern with explicit shots: *.{X}fshots.*\n",
    "    fshots_match = re.search(r'\\.(\\d+)fshots\\.', filename)\n",
    "    if fshots_match:\n",
    "        return int(fshots_match.group(1))\n",
    "    \n",
    "    # Myriadlama pattern without shots (default to 5 shots)\n",
    "    # Pattern: myriadlama.*.5samples.*.feather (no fshots)\n",
    "    if filename.startswith('myriadlama.') and 'fshots' not in filename:\n",
    "        # Check if it has samples pattern (indicating it's a result file)\n",
    "        if re.search(r'\\d+samples\\.', filename):\n",
    "            return 5\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def match_files_by_shot(model_dir):\n",
    "    \"\"\"Match baseline and myriadlama files by shot count.\n",
    "    \n",
    "    Args:\n",
    "        model_dir: Path to model directory\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping shot_count -> {'baseline': Path or None, 'myriadlama': [Path, ...]}\n",
    "    \"\"\"\n",
    "    model_dir = Path(model_dir)\n",
    "    \n",
    "    # Group files by shot count\n",
    "    files_by_shot = defaultdict(lambda: {'baseline': None, 'myriadlama': []})\n",
    "    \n",
    "    for file_path in model_dir.glob('*.feather'):\n",
    "        filename = file_path.name\n",
    "        shot_value = extract_shot_value(filename)\n",
    "        \n",
    "        if shot_value is None:\n",
    "            continue\n",
    "        \n",
    "        if filename.startswith('baseline_per_prompt.'):\n",
    "            files_by_shot[shot_value]['baseline'] = file_path\n",
    "        elif filename.startswith('myriadlama.'):\n",
    "            files_by_shot[shot_value]['myriadlama'].append(file_path)\n",
    "    \n",
    "    return dict(files_by_shot)\n",
    "\n",
    "\n",
    "def extract_method_name(filepath):\n",
    "    \"\"\"Extract method name from myriadlama filename (keeping full detail).\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path object or string\n",
    "        \n",
    "    Returns:\n",
    "        Method name string\n",
    "    \"\"\"\n",
    "    filename = Path(filepath).name\n",
    "    # Remove 'myriadlama.' prefix and '.feather' suffix\n",
    "    method = filename.replace('myriadlama.', '').replace('.feather', '')\n",
    "    return method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c16b2302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ttff(baseline_path, method_path):\n",
    "    \"\"\"Calculate TT, TF, FT, FF metrics comparing baseline and method results.\n",
    "    \n",
    "    Args:\n",
    "        baseline_path: Path to baseline_per_prompt.*.feather file\n",
    "        method_path: Path to myriadlama.*.feather file\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with keys: TT, TF, FT, FF, total_samples, status, error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load files\n",
    "        df_baseline = pd.read_feather(baseline_path)\n",
    "        df_method = pd.read_feather(method_path)\n",
    "        \n",
    "        # Check required columns\n",
    "        required_baseline_cols = ['paraphrase', 'generation_lemmas', 'answer_lemmas']\n",
    "        required_method_cols = ['paraphrases', 'generation_lemmas', 'answer_lemmas']\n",
    "        \n",
    "        for col in required_baseline_cols:\n",
    "            if col not in df_baseline.columns:\n",
    "                return {\n",
    "                    'TT': None, 'TF': None, 'FT': None, 'FF': None,\n",
    "                    'total_samples': 0, 'status': 'error',\n",
    "                    'error': f'Missing column {col} in baseline'\n",
    "                }\n",
    "        \n",
    "        for col in required_method_cols:\n",
    "            if col not in df_method.columns:\n",
    "                return {\n",
    "                    'TT': None, 'TF': None, 'FT': None, 'FF': None,\n",
    "                    'total_samples': 0, 'status': 'error',\n",
    "                    'error': f'Missing column {col} in method file'\n",
    "                }\n",
    "        \n",
    "        # Build paraphrase lookup dictionary from baseline\n",
    "        paraphrase_dict = defaultdict(list)\n",
    "        for idx, row in df_baseline.iterrows():\n",
    "            paraphrase_dict[row[\"paraphrase\"]].append({\n",
    "                \"generation_lemmas\": row[\"generation_lemmas\"],\n",
    "                \"answer_lemmas\": row[\"answer_lemmas\"]\n",
    "            })\n",
    "        \n",
    "        # Calculate TT, TF, FT, FF\n",
    "        countBA_TT = 0\n",
    "        countBA_TF = 0\n",
    "        countBA_FT = 0\n",
    "        countBA_FF = 0\n",
    "        \n",
    "        for paras, gen_lemmas, ans_lemmas in zip(\n",
    "            df_method[\"paraphrases\"], \n",
    "            df_method[\"generation_lemmas\"], \n",
    "            df_method[\"answer_lemmas\"]):\n",
    "            \n",
    "            # Check if ANY paraphrase in baseline was correct\n",
    "            pp_anymatch = False\n",
    "            for para in paras:\n",
    "                if para in paraphrase_dict:\n",
    "                    for item in paraphrase_dict[para]:\n",
    "                        if TF(item[\"generation_lemmas\"], item[\"answer_lemmas\"]):\n",
    "                            pp_anymatch = True\n",
    "                            break\n",
    "                if pp_anymatch:\n",
    "                    break\n",
    "            \n",
    "            # Check if current method is correct\n",
    "            current_match = TF(gen_lemmas, ans_lemmas)\n",
    "            \n",
    "            # Update counts\n",
    "            countBA_TT += int(pp_anymatch and current_match)\n",
    "            countBA_TF += int(pp_anymatch and (not current_match))\n",
    "            countBA_FT += int((not pp_anymatch) and current_match)\n",
    "            countBA_FF += int((not pp_anymatch) and (not current_match))\n",
    "        \n",
    "        total = countBA_TT + countBA_TF + countBA_FT + countBA_FF\n",
    "        \n",
    "        return {\n",
    "            'TT': countBA_TT,\n",
    "            'TF': countBA_TF,\n",
    "            'FT': countBA_FT,\n",
    "            'FF': countBA_FF,\n",
    "            'total_samples': total,\n",
    "            'status': 'success',\n",
    "            'error': None\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'TT': None, 'TF': None, 'FT': None, 'FF': None,\n",
    "            'total_samples': 0, 'status': 'error',\n",
    "            'error': str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d88a8f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_model(model_name, model_dir, output_dir):\n",
    "    \"\"\"Process all shots and methods for a single model.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model\n",
    "        model_dir: Path to model directory containing feather files\n",
    "        output_dir: Path to output directory for results\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with all results for this model\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Processing model: {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Match files by shot count\n",
    "    files_by_shot = match_files_by_shot(model_dir)\n",
    "    \n",
    "    if not files_by_shot:\n",
    "        print(f\"  No files found for {model_name}\")\n",
    "        return None\n",
    "    \n",
    "    results = []\n",
    "    total_processed = 0\n",
    "    total_incomplete = 0\n",
    "    total_errors = 0\n",
    "    \n",
    "    # Process each shot configuration\n",
    "    for shot_count in sorted(files_by_shot.keys()):\n",
    "        shot_data = files_by_shot[shot_count]\n",
    "        baseline_file = shot_data['baseline']\n",
    "        method_files = shot_data['myriadlama']\n",
    "        \n",
    "        print(f\"\\n  Shot count: {shot_count}\")\n",
    "        print(f\"    Baseline: {baseline_file.name if baseline_file else 'MISSING'}\")\n",
    "        print(f\"    Method files: {len(method_files)}\")\n",
    "        \n",
    "        # Process each method file\n",
    "        for method_file in tqdm(method_files, desc=f\"    Processing {shot_count}-shot methods\", leave=False):\n",
    "            method_name = extract_method_name(method_file)\n",
    "            \n",
    "            if baseline_file is None:\n",
    "                # No baseline - mark as incomplete\n",
    "                results.append({\n",
    "                    'model': model_name,\n",
    "                    'shot_count': shot_count,\n",
    "                    'method_name': method_name,\n",
    "                    'baseline_file': 'MISSING',\n",
    "                    'method_file': method_file.name,\n",
    "                    'TT': None,\n",
    "                    'TF': None,\n",
    "                    'FT': None,\n",
    "                    'FF': None,\n",
    "                    'total_samples': None,\n",
    "                    'baseline_acc': None,\n",
    "                    'method_acc': None,\n",
    "                    'improvement': None,\n",
    "                    'status': 'incomplete',\n",
    "                    'error': 'No baseline file found for this shot count'\n",
    "                })\n",
    "                total_incomplete += 1\n",
    "            else:\n",
    "                # Calculate TTFF metrics\n",
    "                ttff = calculate_ttff(baseline_file, method_file)\n",
    "                \n",
    "                if ttff['status'] == 'success':\n",
    "                    # Calculate accuracies\n",
    "                    total = ttff['total_samples']\n",
    "                    baseline_acc = (ttff['TT'] + ttff['TF']) / total if total > 0 else 0\n",
    "                    method_acc = (ttff['TT'] + ttff['FT']) / total if total > 0 else 0\n",
    "                    improvement = method_acc - baseline_acc\n",
    "                    \n",
    "                    results.append({\n",
    "                        'model': model_name,\n",
    "                        'shot_count': shot_count,\n",
    "                        'method_name': method_name,\n",
    "                        'baseline_file': baseline_file.name,\n",
    "                        'method_file': method_file.name,\n",
    "                        'TT': ttff['TT'],\n",
    "                        'TF': ttff['TF'],\n",
    "                        'FT': ttff['FT'],\n",
    "                        'FF': ttff['FF'],\n",
    "                        'total_samples': ttff['total_samples'],\n",
    "                        'baseline_acc': f\"{baseline_acc:.4f}\",\n",
    "                        'method_acc': f\"{method_acc:.4f}\",\n",
    "                        'improvement': f\"{improvement:.4f}\",\n",
    "                        'status': 'success',\n",
    "                        'error': None\n",
    "                    })\n",
    "                    total_processed += 1\n",
    "                else:\n",
    "                    # Error during calculation\n",
    "                    results.append({\n",
    "                        'model': model_name,\n",
    "                        'shot_count': shot_count,\n",
    "                        'method_name': method_name,\n",
    "                        'baseline_file': baseline_file.name,\n",
    "                        'method_file': method_file.name,\n",
    "                        'TT': None,\n",
    "                        'TF': None,\n",
    "                        'FT': None,\n",
    "                        'FF': None,\n",
    "                        'total_samples': None,\n",
    "                        'baseline_acc': None,\n",
    "                        'method_acc': None,\n",
    "                        'improvement': None,\n",
    "                        'status': 'error',\n",
    "                        'error': ttff['error']\n",
    "                    })\n",
    "                    total_errors += 1\n",
    "    \n",
    "    print(f\"\\n  Summary for {model_name}:\")\n",
    "    print(f\"    ✓ Successfully processed: {total_processed}\")\n",
    "    print(f\"    ⚠ Incomplete (no baseline): {total_incomplete}\")\n",
    "    print(f\"    ✗ Errors: {total_errors}\")\n",
    "    print(f\"    Total results: {len(results)}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    # Save results\n",
    "    model_output_dir = Path(output_dir) / model_name\n",
    "    model_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    output_file = model_output_dir / \"ttff_results.csv\"\n",
    "    df_results.to_csv(output_file, index=False)\n",
    "    print(f\"    Saved to: {output_file}\")\n",
    "    \n",
    "    # Also save as feather for faster loading\n",
    "    output_feather = model_output_dir / \"ttff_results.feather\"\n",
    "    df_results.to_feather(output_feather)\n",
    "    print(f\"    Saved to: {output_feather}\")\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee18051b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Processing model: llama3.2_3b\n",
      "================================================================================\n",
      "\n",
      "  Shot count: 0\n",
      "    Baseline: baseline_per_prompt.0shots.feather\n",
      "    Method files: 27\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a81f7497fc164716aa9920f542ffa7a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 0-shot methods:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 1\n",
      "    Baseline: baseline_per_prompt.1shots.feather\n",
      "    Method files: 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c89f391b1d95455baf3184b8f2cc51c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 1-shot methods:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 2\n",
      "    Baseline: baseline_per_prompt.2shots.feather\n",
      "    Method files: 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b8e10f315d64b75b105d4b6ea6b747a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 2-shot methods:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 3\n",
      "    Baseline: baseline_per_prompt.3shots.feather\n",
      "    Method files: 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2db6d7c98814e1796527362d4ecfeea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 3-shot methods:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 4\n",
      "    Baseline: baseline_per_prompt.4shots.feather\n",
      "    Method files: 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01597005b6ec4d78aac29bbeef5c46a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 4-shot methods:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 5\n",
      "    Baseline: baseline_per_prompt.5shots.feather\n",
      "    Method files: 51\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2801344f3ba34a02adc90d6dc0ede87c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 5-shot methods:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 6\n",
      "    Baseline: baseline_per_prompt.6shots.feather\n",
      "    Method files: 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a7b9270b5043bcbf67dac6b9676903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 6-shot methods:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 7\n",
      "    Baseline: baseline_per_prompt.7shots.feather\n",
      "    Method files: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9593cf81c8b8463b8ad49bb092165ce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 7-shot methods:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 8\n",
      "    Baseline: baseline_per_prompt.8shots.feather\n",
      "    Method files: 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c1ad0c3d642439c97066d7c87cc7eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 8-shot methods:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 10\n",
      "    Baseline: MISSING\n",
      "    Method files: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caf35de5dc6c439bb83853e7b7ce8daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 10-shot methods:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Summary for llama3.2_3b:\n",
      "    ✓ Successfully processed: 137\n",
      "    ⚠ Incomplete (no baseline): 5\n",
      "    ✗ Errors: 0\n",
      "    Total results: 142\n",
      "    Saved to: /home/y-guo/self-ensemble/analyzeResults/llama3.2_3b/ttff_results.csv\n",
      "    Saved to: /home/y-guo/self-ensemble/analyzeResults/llama3.2_3b/ttff_results.feather\n",
      "\n",
      "First 5 results:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "shot_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "method_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "baseline_file",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "method_file",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "TT",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "TF",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "FT",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "FF",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "total_samples",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "baseline_acc",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "method_acc",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "improvement",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "status",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "error",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "90543d64-5098-4c45-85f5-a97bc16e471d",
       "rows": [
        [
         "0",
         "llama3.2_3b",
         "0",
         "modifyattn.modifyrope.singleparaqapair.0fshots.5samples.5paras",
         "baseline_per_prompt.0shots.feather",
         "myriadlama.modifyattn.modifyrope.singleparaqapair.0fshots.5samples.5paras.feather",
         "4023.0",
         "1997.0",
         "198.0",
         "3782.0",
         "10000.0",
         "0.6020",
         "0.4221",
         "-0.1799",
         "success",
         null
        ],
        [
         "1",
         "llama3.2_3b",
         "0",
         "modifyattn.modifyrope.scalescore20.singleparaqapair.0fshots.5samples.5paras",
         "baseline_per_prompt.0shots.feather",
         "myriadlama.modifyattn.modifyrope.scalescore20.singleparaqapair.0fshots.5samples.5paras.feather",
         "3472.0",
         "2548.0",
         "156.0",
         "3824.0",
         "10000.0",
         "0.6020",
         "0.3628",
         "-0.2392",
         "success",
         null
        ],
        [
         "2",
         "llama3.2_3b",
         "0",
         "logits.max.avglayer.layer21.alpha100.token-last.multilayer.0fshots.5samples.5paras",
         "baseline_per_prompt.0shots.feather",
         "myriadlama.logits.max.avglayer.layer21.alpha100.token-last.multilayer.0fshots.5samples.5paras.feather",
         "4307.0",
         "1713.0",
         "166.0",
         "3814.0",
         "10000.0",
         "0.6020",
         "0.4473",
         "-0.1547",
         "success",
         null
        ],
        [
         "3",
         "llama3.2_3b",
         "0",
         "logits.avg.avglayer.layer1.alpha100.token-last.multilayer.0fshots.5samples.5paras",
         "baseline_per_prompt.0shots.feather",
         "myriadlama.logits.avg.avglayer.layer1.alpha100.token-last.multilayer.0fshots.5samples.5paras.feather",
         "4221.0",
         "1799.0",
         "166.0",
         "3814.0",
         "10000.0",
         "0.6020",
         "0.4387",
         "-0.1633",
         "success",
         null
        ],
        [
         "4",
         "llama3.2_3b",
         "0",
         "modifyrope.singleparaqapair.0fshots.5samples.5paras",
         "baseline_per_prompt.0shots.feather",
         "myriadlama.modifyrope.singleparaqapair.0fshots.5samples.5paras.feather",
         "2919.0",
         "3101.0",
         "176.0",
         "3804.0",
         "10000.0",
         "0.6020",
         "0.3095",
         "-0.2925",
         "success",
         null
        ]
       ],
       "shape": {
        "columns": 15,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>shot_count</th>\n",
       "      <th>method_name</th>\n",
       "      <th>baseline_file</th>\n",
       "      <th>method_file</th>\n",
       "      <th>TT</th>\n",
       "      <th>TF</th>\n",
       "      <th>FT</th>\n",
       "      <th>FF</th>\n",
       "      <th>total_samples</th>\n",
       "      <th>baseline_acc</th>\n",
       "      <th>method_acc</th>\n",
       "      <th>improvement</th>\n",
       "      <th>status</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama3.2_3b</td>\n",
       "      <td>0</td>\n",
       "      <td>modifyattn.modifyrope.singleparaqapair.0fshots...</td>\n",
       "      <td>baseline_per_prompt.0shots.feather</td>\n",
       "      <td>myriadlama.modifyattn.modifyrope.singleparaqap...</td>\n",
       "      <td>4023.0</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>3782.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.6020</td>\n",
       "      <td>0.4221</td>\n",
       "      <td>-0.1799</td>\n",
       "      <td>success</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llama3.2_3b</td>\n",
       "      <td>0</td>\n",
       "      <td>modifyattn.modifyrope.scalescore20.singleparaq...</td>\n",
       "      <td>baseline_per_prompt.0shots.feather</td>\n",
       "      <td>myriadlama.modifyattn.modifyrope.scalescore20....</td>\n",
       "      <td>3472.0</td>\n",
       "      <td>2548.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>3824.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.6020</td>\n",
       "      <td>0.3628</td>\n",
       "      <td>-0.2392</td>\n",
       "      <td>success</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llama3.2_3b</td>\n",
       "      <td>0</td>\n",
       "      <td>logits.max.avglayer.layer21.alpha100.token-las...</td>\n",
       "      <td>baseline_per_prompt.0shots.feather</td>\n",
       "      <td>myriadlama.logits.max.avglayer.layer21.alpha10...</td>\n",
       "      <td>4307.0</td>\n",
       "      <td>1713.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>3814.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.6020</td>\n",
       "      <td>0.4473</td>\n",
       "      <td>-0.1547</td>\n",
       "      <td>success</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llama3.2_3b</td>\n",
       "      <td>0</td>\n",
       "      <td>logits.avg.avglayer.layer1.alpha100.token-last...</td>\n",
       "      <td>baseline_per_prompt.0shots.feather</td>\n",
       "      <td>myriadlama.logits.avg.avglayer.layer1.alpha100...</td>\n",
       "      <td>4221.0</td>\n",
       "      <td>1799.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>3814.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.6020</td>\n",
       "      <td>0.4387</td>\n",
       "      <td>-0.1633</td>\n",
       "      <td>success</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llama3.2_3b</td>\n",
       "      <td>0</td>\n",
       "      <td>modifyrope.singleparaqapair.0fshots.5samples.5...</td>\n",
       "      <td>baseline_per_prompt.0shots.feather</td>\n",
       "      <td>myriadlama.modifyrope.singleparaqapair.0fshots...</td>\n",
       "      <td>2919.0</td>\n",
       "      <td>3101.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>3804.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.6020</td>\n",
       "      <td>0.3095</td>\n",
       "      <td>-0.2925</td>\n",
       "      <td>success</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         model  shot_count                                        method_name  \\\n",
       "0  llama3.2_3b           0  modifyattn.modifyrope.singleparaqapair.0fshots...   \n",
       "1  llama3.2_3b           0  modifyattn.modifyrope.scalescore20.singleparaq...   \n",
       "2  llama3.2_3b           0  logits.max.avglayer.layer21.alpha100.token-las...   \n",
       "3  llama3.2_3b           0  logits.avg.avglayer.layer1.alpha100.token-last...   \n",
       "4  llama3.2_3b           0  modifyrope.singleparaqapair.0fshots.5samples.5...   \n",
       "\n",
       "                        baseline_file  \\\n",
       "0  baseline_per_prompt.0shots.feather   \n",
       "1  baseline_per_prompt.0shots.feather   \n",
       "2  baseline_per_prompt.0shots.feather   \n",
       "3  baseline_per_prompt.0shots.feather   \n",
       "4  baseline_per_prompt.0shots.feather   \n",
       "\n",
       "                                         method_file      TT      TF     FT  \\\n",
       "0  myriadlama.modifyattn.modifyrope.singleparaqap...  4023.0  1997.0  198.0   \n",
       "1  myriadlama.modifyattn.modifyrope.scalescore20....  3472.0  2548.0  156.0   \n",
       "2  myriadlama.logits.max.avglayer.layer21.alpha10...  4307.0  1713.0  166.0   \n",
       "3  myriadlama.logits.avg.avglayer.layer1.alpha100...  4221.0  1799.0  166.0   \n",
       "4  myriadlama.modifyrope.singleparaqapair.0fshots...  2919.0  3101.0  176.0   \n",
       "\n",
       "       FF  total_samples baseline_acc method_acc improvement   status error  \n",
       "0  3782.0        10000.0       0.6020     0.4221     -0.1799  success  None  \n",
       "1  3824.0        10000.0       0.6020     0.3628     -0.2392  success  None  \n",
       "2  3814.0        10000.0       0.6020     0.4473     -0.1547  success  None  \n",
       "3  3814.0        10000.0       0.6020     0.4387     -0.1633  success  None  \n",
       "4  3804.0        10000.0       0.6020     0.3095     -0.2925  success  None  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test on one model first\n",
    "test_model = \"llama3.2_3b\"\n",
    "test_model_dir = Path(BASE_PATH) / test_model\n",
    "\n",
    "if test_model_dir.exists():\n",
    "    df_test = process_model(test_model, test_model_dir, OUTPUT_BASE)\n",
    "    if df_test is not None:\n",
    "        print(f\"\\nFirst 5 results:\")\n",
    "        display(df_test.head())\n",
    "else:\n",
    "    print(f\"Test model directory not found: {test_model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4138c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_models(base_path, output_base):\n",
    "    \"\"\"Process all models in the base path directory.\n",
    "    \n",
    "    Args:\n",
    "        base_path: Path to directory containing model subdirectories\n",
    "        output_base: Path to output directory for results\n",
    "    \"\"\"\n",
    "    base_path = Path(base_path)\n",
    "    \n",
    "    # Find all model directories that have baseline files\n",
    "    model_dirs = []\n",
    "    for item in base_path.iterdir():\n",
    "        if item.is_dir():\n",
    "            # Check if directory has any baseline files\n",
    "            baseline_files = list(item.glob('baseline_per_prompt.*.feather'))\n",
    "            if baseline_files:\n",
    "                model_dirs.append(item)\n",
    "    \n",
    "    print(f\"Found {len(model_dirs)} models with baseline files:\")\n",
    "    for model_dir in sorted(model_dirs):\n",
    "        print(f\"  - {model_dir.name}\")\n",
    "    \n",
    "    # Process each model\n",
    "    all_results = []\n",
    "    for model_dir in tqdm(model_dirs, desc=\"Processing models\"):\n",
    "        model_name = model_dir.name\n",
    "        try:\n",
    "            df_model = process_model(model_name, model_dir, output_base)\n",
    "            if df_model is not None:\n",
    "                all_results.append(df_model)\n",
    "        except Exception as e:\n",
    "            print(f\"\\n✗ ERROR processing {model_name}: {e}\")\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    # Combine all results\n",
    "    if all_results:\n",
    "        df_all = pd.concat(all_results, ignore_index=True)\n",
    "        \n",
    "        # Save combined results\n",
    "        combined_output = Path(output_base) / \"all_models_ttff_results.csv\"\n",
    "        df_all.to_csv(combined_output, index=False)\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Combined results saved to: {combined_output}\")\n",
    "        print(f\"Total rows: {len(df_all)}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        return df_all\n",
    "    else:\n",
    "        print(\"\\nNo results to combine.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9bc10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 models with baseline files:\n",
      "  - llama3.1_8b\n",
      "  - llama3.1_8b_it\n",
      "  - llama3.2_1b\n",
      "  - llama3.2_1b_it\n",
      "  - llama3.2_3b\n",
      "  - llama3.2_3b_it\n",
      "  - qwen2.5_14b\n",
      "  - qwen2.5_14b_it\n",
      "  - qwen2.5_3b\n",
      "  - qwen2.5_3b_it\n",
      "  - qwen2.5_7b\n",
      "  - qwen2.5_7b_it\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "440dc5a58d3744e6b415d70eefd7ad4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing models:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Processing model: qwen2.5_14b_it\n",
      "================================================================================\n",
      "\n",
      "  Shot count: 0\n",
      "    Baseline: baseline_per_prompt.0shots.feather\n",
      "    Method files: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded0d15895cf439ab9ff815ab84a1d20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 0-shot methods:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 1\n",
      "    Baseline: baseline_per_prompt.1shots.feather\n",
      "    Method files: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f4ad294f5a34e83af53eeb8faeb5db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 1-shot methods:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 2\n",
      "    Baseline: baseline_per_prompt.2shots.feather\n",
      "    Method files: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa19db6a9ee4a5dbebd06547b1de803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 2-shot methods:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 3\n",
      "    Baseline: baseline_per_prompt.3shots.feather\n",
      "    Method files: 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc4e2d9c97ff4ccb9f9911258299964b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 3-shot methods:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 4\n",
      "    Baseline: baseline_per_prompt.4shots.feather\n",
      "    Method files: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4a5447073534ce1b04a99e03442f380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 4-shot methods:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 5\n",
      "    Baseline: baseline_per_prompt.5shots.feather\n",
      "    Method files: 37\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c598419851745ebb8692235b005a60d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 5-shot methods:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 6\n",
      "    Baseline: baseline_per_prompt.6shots.feather\n",
      "    Method files: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1857953cf19430f8a76affaa2213ecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 6-shot methods: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 8\n",
      "    Baseline: baseline_per_prompt.8shots.feather\n",
      "    Method files: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b45f015e5c624572a36e9e5b679961af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 8-shot methods: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Summary for qwen2.5_14b_it:\n",
      "    ✓ Successfully processed: 52\n",
      "    ⚠ Incomplete (no baseline): 0\n",
      "    ✗ Errors: 0\n",
      "    Total results: 52\n",
      "    Saved to: /home/y-guo/self-ensemble/analyzeResults/qwen2.5_14b_it/ttff_results.csv\n",
      "    Saved to: /home/y-guo/self-ensemble/analyzeResults/qwen2.5_14b_it/ttff_results.feather\n",
      "\n",
      "================================================================================\n",
      "Processing model: llama3.2_1b_it\n",
      "================================================================================\n",
      "\n",
      "  Shot count: 0\n",
      "    Baseline: baseline_per_prompt.0shots.feather\n",
      "    Method files: 13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6cbef47982141ac8d21f30cfcc584b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 0-shot methods:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 1\n",
      "    Baseline: baseline_per_prompt.1shots.feather\n",
      "    Method files: 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b0ae125023248b9a07e95c8124dc2e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 1-shot methods:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 2\n",
      "    Baseline: baseline_per_prompt.2shots.feather\n",
      "    Method files: 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a1dbe9e3f74d9ca791631ed2c10d9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 2-shot methods:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 3\n",
      "    Baseline: baseline_per_prompt.3shots.feather\n",
      "    Method files: 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03f4ba49159b4d358b082a582295b10a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 3-shot methods:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 4\n",
      "    Baseline: baseline_per_prompt.4shots.feather\n",
      "    Method files: 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "898c0146416a4f879bd90b3238921924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 4-shot methods:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 5\n",
      "    Baseline: baseline_per_prompt.5shots.feather\n",
      "    Method files: 64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0cbcc7cb694b85a4074417dbc8c2ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 5-shot methods:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 6\n",
      "    Baseline: baseline_per_prompt.6shots.feather\n",
      "    Method files: 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a1d71e8aac4406db97edec99d27d0d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 6-shot methods:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 8\n",
      "    Baseline: baseline_per_prompt.8shots.feather\n",
      "    Method files: 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9590080e75e4bc18c435e7fdcacaa67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 8-shot methods:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 10\n",
      "    Baseline: MISSING\n",
      "    Method files: 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0998e5f737a42a28bb19fa12ddd1d00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 10-shot methods:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Summary for llama3.2_1b_it:\n",
      "    ✓ Successfully processed: 132\n",
      "    ⚠ Incomplete (no baseline): 8\n",
      "    ✗ Errors: 0\n",
      "    Total results: 140\n",
      "    Saved to: /home/y-guo/self-ensemble/analyzeResults/llama3.2_1b_it/ttff_results.csv\n",
      "    Saved to: /home/y-guo/self-ensemble/analyzeResults/llama3.2_1b_it/ttff_results.feather\n",
      "\n",
      "================================================================================\n",
      "Processing model: qwen2.5_3b_it\n",
      "================================================================================\n",
      "\n",
      "  Shot count: 0\n",
      "    Baseline: baseline_per_prompt.0shots.feather\n",
      "    Method files: 13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c350fdd2460f42f9a150f5b37f5a58d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 0-shot methods:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 1\n",
      "    Baseline: baseline_per_prompt.1shots.feather\n",
      "    Method files: 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d247b5854e44d3f992081b584ff3064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 1-shot methods:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 2\n",
      "    Baseline: baseline_per_prompt.2shots.feather\n",
      "    Method files: 11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a457ddb89a744a998bc405b24345f16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 2-shot methods:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 3\n",
      "    Baseline: baseline_per_prompt.3shots.feather\n",
      "    Method files: 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7da94cae484688aeed740aa5479d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 3-shot methods:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 4\n",
      "    Baseline: baseline_per_prompt.4shots.feather\n",
      "    Method files: 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0954426c79d94c7e82489a8c3127f5d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 4-shot methods:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 5\n",
      "    Baseline: baseline_per_prompt.5shots.feather\n",
      "    Method files: 69\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13efafb104da4e3885f3ad44fade656a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 5-shot methods:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 6\n",
      "    Baseline: baseline_per_prompt.6shots.feather\n",
      "    Method files: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9946fe6d8354f2897cfbc4e9949313c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 6-shot methods:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 8\n",
      "    Baseline: baseline_per_prompt.8shots.feather\n",
      "    Method files: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d5a91a8f74447ed9ac46906ade86475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 8-shot methods:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 10\n",
      "    Baseline: MISSING\n",
      "    Method files: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "542bc72da16f4ae2855a80ddc7d4f2a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 10-shot methods:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Summary for qwen2.5_3b_it:\n",
      "    ✓ Successfully processed: 121\n",
      "    ⚠ Incomplete (no baseline): 3\n",
      "    ✗ Errors: 0\n",
      "    Total results: 124\n",
      "    Saved to: /home/y-guo/self-ensemble/analyzeResults/qwen2.5_3b_it/ttff_results.csv\n",
      "    Saved to: /home/y-guo/self-ensemble/analyzeResults/qwen2.5_3b_it/ttff_results.feather\n",
      "\n",
      "================================================================================\n",
      "Processing model: llama3.2_1b\n",
      "================================================================================\n",
      "\n",
      "  Shot count: 0\n",
      "    Baseline: baseline_per_prompt.0shots.feather\n",
      "    Method files: 19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8a368ad7a2c47649607de092c344b0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 0-shot methods:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 1\n",
      "    Baseline: baseline_per_prompt.1shots.feather\n",
      "    Method files: 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adc421bfb3084d0cbe9344012083557b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 1-shot methods:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 2\n",
      "    Baseline: baseline_per_prompt.2shots.feather\n",
      "    Method files: 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff210b7ecc24408963fa3108a39f454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 2-shot methods:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 3\n",
      "    Baseline: baseline_per_prompt.3shots.feather\n",
      "    Method files: 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22576dbc5b024c2fa7dde2072ff8f362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 3-shot methods:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 4\n",
      "    Baseline: baseline_per_prompt.4shots.feather\n",
      "    Method files: 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f2c8ad9c124a5c8c3c58e002c12931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 4-shot methods:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 5\n",
      "    Baseline: baseline_per_prompt.5shots.feather\n",
      "    Method files: 60\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab163ca1d8c447b9ba22ac67a4397db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 5-shot methods:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 6\n",
      "    Baseline: baseline_per_prompt.6shots.feather\n",
      "    Method files: 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07c74421fea949868271f5b9839c519e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 6-shot methods:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 7\n",
      "    Baseline: baseline_per_prompt.7shots.feather\n",
      "    Method files: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbbe3bc0884c4f4f8409d3b4c185037f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 7-shot methods:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 8\n",
      "    Baseline: baseline_per_prompt.8shots.feather\n",
      "    Method files: 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2755a27dce77453ea93d39f61f846e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 8-shot methods:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 9\n",
      "    Baseline: baseline_per_prompt.9shots.feather\n",
      "    Method files: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff2bd6423ed44c3fb0cecd992ba6305c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 9-shot methods:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 10\n",
      "    Baseline: MISSING\n",
      "    Method files: 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bca3533ced9a4774b1983b796e9a9e99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 10-shot methods:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Summary for llama3.2_1b:\n",
      "    ✓ Successfully processed: 145\n",
      "    ⚠ Incomplete (no baseline): 9\n",
      "    ✗ Errors: 0\n",
      "    Total results: 154\n",
      "    Saved to: /home/y-guo/self-ensemble/analyzeResults/llama3.2_1b/ttff_results.csv\n",
      "    Saved to: /home/y-guo/self-ensemble/analyzeResults/llama3.2_1b/ttff_results.feather\n",
      "\n",
      "================================================================================\n",
      "Processing model: qwen2.5_3b\n",
      "================================================================================\n",
      "\n",
      "  Shot count: 0\n",
      "    Baseline: baseline_per_prompt.0shots.feather\n",
      "    Method files: 20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e178237b8394811ada6e6888eb929dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 0-shot methods:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 1\n",
      "    Baseline: baseline_per_prompt.1shots.feather\n",
      "    Method files: 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e57886e15ef4d0ca30adcb803ace9eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 1-shot methods:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 2\n",
      "    Baseline: baseline_per_prompt.2shots.feather\n",
      "    Method files: 11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f54255c5d54a491bbca9e55a5a81fb86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 2-shot methods:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 3\n",
      "    Baseline: baseline_per_prompt.3shots.feather\n",
      "    Method files: 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a220ded8c3426abc79ac42f3fa095b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 3-shot methods:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 4\n",
      "    Baseline: baseline_per_prompt.4shots.feather\n",
      "    Method files: 11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b123b12b072f4438ae29a658a690a5e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 4-shot methods:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 5\n",
      "    Baseline: baseline_per_prompt.5shots.feather\n",
      "    Method files: 40\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed3e245f46d54ff38ad0f5e7b9cd9571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 5-shot methods:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 6\n",
      "    Baseline: baseline_per_prompt.6shots.feather\n",
      "    Method files: 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0ae2c23d82d403d91cdf88c5112b649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 6-shot methods:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 8\n",
      "    Baseline: baseline_per_prompt.8shots.feather\n",
      "    Method files: 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67983a15023d41d5b0b003ff6a7b9998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 8-shot methods:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 10\n",
      "    Baseline: MISSING\n",
      "    Method files: 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c357cd97398471598d82343aff3d965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 10-shot methods:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Summary for qwen2.5_3b:\n",
      "    ✓ Successfully processed: 90\n",
      "    ⚠ Incomplete (no baseline): 8\n",
      "    ✗ Errors: 23\n",
      "    Total results: 121\n",
      "    Saved to: /home/y-guo/self-ensemble/analyzeResults/qwen2.5_3b/ttff_results.csv\n",
      "    Saved to: /home/y-guo/self-ensemble/analyzeResults/qwen2.5_3b/ttff_results.feather\n",
      "\n",
      "================================================================================\n",
      "Processing model: qwen2.5_7b\n",
      "================================================================================\n",
      "\n",
      "  Shot count: 0\n",
      "    Baseline: baseline_per_prompt.0shots.feather\n",
      "    Method files: 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "824754c119c542658a98d98d5b8699ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 0-shot methods:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 1\n",
      "    Baseline: baseline_per_prompt.1shots.feather\n",
      "    Method files: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "344ee1400665430d8833cdd0cb127017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 1-shot methods:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 2\n",
      "    Baseline: baseline_per_prompt.2shots.feather\n",
      "    Method files: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c0b77dc7d6f437aaaecc91104108483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 2-shot methods:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 3\n",
      "    Baseline: baseline_per_prompt.3shots.feather\n",
      "    Method files: 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25c965aa84d14682b7563dd37d9a8109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 3-shot methods:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 4\n",
      "    Baseline: baseline_per_prompt.4shots.feather\n",
      "    Method files: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "615e460b4e9140edb31f7b27c269fe48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 4-shot methods:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 5\n",
      "    Baseline: baseline_per_prompt.5shots.feather\n",
      "    Method files: 38\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0098e8bba6b2468cbe2e52cce0917a5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 5-shot methods:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 6\n",
      "    Baseline: baseline_per_prompt.6shots.feather\n",
      "    Method files: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de68fec4a9bd4b2faaedc7074e3ce656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 6-shot methods: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 8\n",
      "    Baseline: baseline_per_prompt.8shots.feather\n",
      "    Method files: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "129c7464a47a494499efaff137a5030b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 8-shot methods: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Summary for qwen2.5_7b:\n",
      "    ✓ Successfully processed: 68\n",
      "    ⚠ Incomplete (no baseline): 0\n",
      "    ✗ Errors: 0\n",
      "    Total results: 68\n",
      "    Saved to: /home/y-guo/self-ensemble/analyzeResults/qwen2.5_7b/ttff_results.csv\n",
      "    Saved to: /home/y-guo/self-ensemble/analyzeResults/qwen2.5_7b/ttff_results.feather\n",
      "\n",
      "================================================================================\n",
      "Processing model: qwen2.5_7b_it\n",
      "================================================================================\n",
      "\n",
      "  Shot count: 0\n",
      "    Baseline: baseline_per_prompt.0shots.feather\n",
      "    Method files: 11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "437627b99f9f4ef89e222fddee32dfc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 0-shot methods:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 1\n",
      "    Baseline: baseline_per_prompt.1shots.feather\n",
      "    Method files: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c718d50694bb46039a0dd4d152f21957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 1-shot methods:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 2\n",
      "    Baseline: baseline_per_prompt.2shots.feather\n",
      "    Method files: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "029c306a5c974754bf20778ed20c2833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 2-shot methods:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Shot count: 3\n",
      "    Baseline: baseline_per_prompt.3shots.feather\n",
      "    Method files: 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c445f7343a424ec8844059e1e99bf377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Processing 3-shot methods:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run full batch processing for all models\n",
    "df_all_results = process_all_models(BASE_PATH, OUTPUT_BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "56b294a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TT: 3811, TF: 2209, FT: 100, FF: 3880\n"
     ]
    }
   ],
   "source": [
    "# compute TT, TF, FT, FF\n",
    "from collections import defaultdict\n",
    "\n",
    "paraphrase_dict = defaultdict(list)\n",
    "\n",
    "for idx, row in df_perprompt.iterrows():\n",
    "    paraphrase_dict[row[\"paraphrase\"]].append({\n",
    "        \"generation_lemmas\": row[\"generation_lemmas\"],\n",
    "        \"answer_lemmas\": row[\"answer_lemmas\"]\n",
    "    })\n",
    "countBA_TT = 0\n",
    "countBA_TF = 0\n",
    "countBA_FT = 0\n",
    "countBA_FF = 0\n",
    "\n",
    "for paras, gen_lemmas, ans_lemmas in zip(df_logit[\"paraphrases\"], df_logit[\"generation_lemmas\"], df_logit[\"answer_lemmas\"]):\n",
    "    # 直接从字典查找，避免 isin 查询\n",
    "    items = []\n",
    "    pp_anymatch = False\n",
    "    for para in paras:\n",
    "        if para in paraphrase_dict:\n",
    "            for item in paraphrase_dict[para]:\n",
    "                items.append([lemma for lemma in item[\"generation_lemmas\"].tolist()])\n",
    "                if TF(item[\"generation_lemmas\"], item[\"answer_lemmas\"]):\n",
    "                    pp_anymatch = True\n",
    "                    break\n",
    "        if pp_anymatch:\n",
    "            break\n",
    "    \n",
    "    current_match = TF(gen_lemmas, ans_lemmas)\n",
    "    countBA_TT += int(pp_anymatch and current_match)\n",
    "    countBA_TF += int(pp_anymatch and (not current_match))\n",
    "    countBA_FT += int((not pp_anymatch) and current_match)\n",
    "    countBA_FF += int((not pp_anymatch) and (not current_match))\n",
    "    # if (not pp_anymatch) and current_match:\n",
    "    #     print(\"No paraphrase match but current match:\")\n",
    "    #     print(gen_lemmas)\n",
    "    #     print(ans_lemmas)\n",
    "    #     for item in items:\n",
    "    #         print(item)        \n",
    "        \n",
    "\n",
    "print(f\"TT: {countBA_TT}, TF: {countBA_TF}, FT: {countBA_FT}, FF: {countBA_FF}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74165d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flexattention",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
