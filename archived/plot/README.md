# FlexAttention å¯è§†åŒ–è„šæœ¬ / FlexAttention Visualization Scripts

æœ¬ç›®å½•åŒ…å«ç”¨äºå¯è§†åŒ– FlexAttention ä»£ç æµç¨‹å’Œæ³¨æ„åŠ›æ©ç çš„è„šæœ¬ã€‚
This directory contains scripts for visualizing FlexAttention code flowchart and attention masks.

---

## ğŸ“Š ä¸»è¦æ–‡ä»¶ / Main Files

### 1. `flowchart_and_attention_mask_visualization.ipynb`

**å®Œæ•´çš„ Jupyter ç¬”è®°æœ¬ï¼Œæä¾›äº¤äº’å¼å¯è§†åŒ–**
**Complete Jupyter notebook with interactive visualizations**

#### åŠŸèƒ½ç‰¹æ€§ / Features:

- âœ… **ä»£ç æµç¨‹å›¾** - å±•ç¤ºä»è¾“å…¥åˆ°è¾“å‡ºçš„å®Œæ•´å¤„ç†æµç¨‹
  - **Code flowchart** - Shows complete processing pipeline from input to output

- âœ… **æ³¨æ„åŠ›æ©ç å¯è§†åŒ–** - ç²¾ç¡®å±•ç¤ºä¸åŒåœºæ™¯ä¸‹çš„æ©ç çŸ©é˜µå½¢çŠ¶
  - **Attention mask visualization** - Precisely shows mask matrix shapes in different scenarios

- âœ… **å¯äº¤äº’å‚æ•°** - æ‰€æœ‰é…ç½®é›†ä¸­åœ¨é…ç½®å­—å…¸ä¸­ï¼Œæ–¹ä¾¿ä¿®æ”¹å’Œå®éªŒ
  - **Interactive parameters** - All configs centralized for easy modification and experimentation

- âœ… **æ™ºèƒ½é‡‡æ ·** - å¤„ç†å¤§å‹åºåˆ—æ—¶ä¿æŒç»“æ„å¯è§æ€§
  - **Smart sampling** - Maintains structure visibility for large sequences

- âœ… **æ³¨æ„åŠ›æ¨¡å¼åˆ†æ** - å¯¹æ¯”ç¼–ç å’Œç”Ÿæˆé˜¶æ®µçš„æ³¨æ„åŠ›è¡Œä¸º
  - **Attention pattern analysis** - Compares encoding vs generation phase behavior

- âœ… **å¯¼å‡ºåŠŸèƒ½** - ä¿å­˜æ‰€æœ‰å¯è§†åŒ–ä¸ºé«˜åˆ†è¾¨ç‡å›¾ç‰‡
  - **Export functionality** - Save all visualizations as high-resolution images

#### ä½¿ç”¨æ–¹æ³• / Usage:

```bash
# 1. å®‰è£…ä¾èµ– / Install dependencies
pip install jupyter matplotlib numpy

# 2. å¯åŠ¨ Jupyter / Start Jupyter
jupyter notebook flowchart_and_attention_mask_visualization.ipynb

# 3. è¿è¡Œæ‰€æœ‰å•å…ƒæ ¼æŸ¥çœ‹é¢„è®¾ç¤ºä¾‹ / Run all cells to see preset examples
# 4. ä¿®æ”¹é…ç½®å­—å…¸è¿›è¡Œè‡ªå®šä¹‰å®éªŒ / Modify config dictionaries for custom experiments
```

#### ä¸‰ç§é¢„è®¾åœºæ™¯ / Three Preset Scenarios:

1. **å°å‹ç¤ºä¾‹ (Small Example)** - 3ä¸ªæ”¹å†™ï¼Œæ¯ä¸ª15ä»¤ç‰Œï¼Œå®Œæ•´å±•ç¤º
   - 3 paraphrases, 15 tokens each, full display
   - é€‚åˆç†è§£åŸºæœ¬åŸç† / Good for understanding basic principles

2. **ä¸­å‹ç¤ºä¾‹ (Medium Example)** - 5ä¸ªæ”¹å†™ï¼Œæ¯ä¸ª25ä»¤ç‰Œï¼Œæ™ºèƒ½é‡‡æ ·
   - 5 paraphrases, 25 tokens each, smart sampling
   - å±•ç¤ºé‡‡æ ·ç­–ç•¥ / Shows sampling strategy

3. **å¤§å‹ç¤ºä¾‹ (Large Example)** - 5ä¸ªæ”¹å†™ï¼Œæ¯ä¸ª50ä»¤ç‰Œï¼ŒçœŸå®åœºæ™¯
   - 5 paraphrases, 50 tokens each, realistic scenario
   - æ¨¡æ‹ŸçœŸå®ä½¿ç”¨ / Simulates real usage

#### è‡ªå®šä¹‰é…ç½®ç¤ºä¾‹ / Custom Configuration Example:

```python
CUSTOM_CONFIG = {
    'num_paraphrases': 4,              # æ”¹å†™æ•°é‡ / Number of paraphrases
    'tokens_per_paraphrase': 30,       # æ¯ä¸ªæ”¹å†™çš„ä»¤ç‰Œæ•° / Tokens per paraphrase
    'separator_tokens': 3,             # åˆ†éš”ç¬¦ä»¤ç‰Œæ•° / Separator tokens
    'num_generated_tokens': 10,        # ç”Ÿæˆçš„ä»¤ç‰Œæ•° / Generated tokens
    'display_mode': 'sampled',         # 'full' æˆ– 'sampled' / 'full' or 'sampled'
    'max_display_positions': 30,       # æœ€å¤§æ˜¾ç¤ºä½ç½®æ•° / Max display positions
}
```

---

### 2. `test_visualization.py`

**ç‹¬ç«‹æµ‹è¯•è„šæœ¬ï¼ŒéªŒè¯å¯è§†åŒ–å‡½æ•°çš„æ­£ç¡®æ€§**
**Standalone test script to verify visualization functions**

#### ç”¨é€” / Purpose:

- éªŒè¯æ³¨æ„åŠ›æ©ç å‡½æ•°çš„æ­£ç¡®æ€§ / Verify attention mask function correctness
- æµ‹è¯•å…³é”®å±æ€§ï¼ˆå› æœçº¦æŸã€åˆ†æ®µéš”ç¦»ã€èåˆå…³æ³¨ï¼‰/ Test key properties (causal, isolation, fusion)
- æ— éœ€ Jupyter å³å¯è¿è¡Œ / Runs without Jupyter

#### è¿è¡Œæ–¹æ³• / How to Run:

```bash
# å®‰è£…ä¾èµ– / Install dependencies
pip install numpy

# è¿è¡Œæµ‹è¯• / Run tests
python3 test_visualization.py
```

#### é¢„æœŸè¾“å‡º / Expected Output:

```
âœ“ å› æœçº¦æŸè¿è§„ / Causal violations: 0 (should be 0)
âœ“ ç¼–ç é˜¶æ®µè·¨æ®µå…³æ³¨ / Encoding cross-segment attention: 0 (should be 0)
âœ“ ç¬¬ä¸€ä¸ªç”Ÿæˆä»¤ç‰Œå¯å…³æ³¨ä½ç½® / First generated token attends to: 50/50
âœ“ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼/ All tests passed!
```

---

## ğŸ¨ å¯è§†åŒ–ç¤ºä¾‹ / Visualization Examples

### ä»£ç æµç¨‹å›¾ / Code Flowchart

æµç¨‹å›¾å±•ç¤ºäº† FlexAttention çš„å®Œæ•´å¤„ç†æµç¨‹ï¼š
The flowchart shows the complete FlexAttention processing pipeline:

1. **è¾“å…¥å‡†å¤‡** - åŠ è½½é—®é¢˜å’Œæ”¹å†™ / **Input preparation** - Load question and paraphrases
2. **æ‹¼æ¥å¤„ç†** - æ‹¼æ¥æ”¹å†™å¹¶è¿½è¸ªä½ç½® / **Concatenation** - Concatenate with position tracking
3. **åˆ›å»ºæ©ç ** - ç”Ÿæˆ FlexAttention æ©ç å‡½æ•° / **Create mask** - Generate FlexAttention mask function
4. **æ¨¡å‹æ‰“è¡¥ä¸** - ä½¿ç”¨ FlexAttention æ›¿æ¢æ³¨æ„åŠ›å±‚ / **Patch model** - Replace attention layers with FlexAttention
5. **ç¼–ç é˜¶æ®µ** - åˆ†æ®µéš”ç¦»æ³¨æ„åŠ› / **Encoding phase** - Segment-isolated attention
6. **ç”Ÿæˆå¾ªç¯** - è‡ªå›å½’ç”Ÿæˆï¼Œèåˆå…³æ³¨ / **Generation loop** - Auto-regressive generation with fusion
7. **è§£ç è¾“å‡º** - è¿”å›ç”Ÿæˆçš„æ–‡æœ¬ / **Decode output** - Return generated text

### æ³¨æ„åŠ›æ©ç çŸ©é˜µ / Attention Mask Matrix

æ©ç çŸ©é˜µå±•ç¤ºäº†æ³¨æ„åŠ›æ¨¡å¼ï¼š
The mask matrix shows attention patterns:

- **ç»¿è‰²æ–¹å— (â– )** = å¯ä»¥å…³æ³¨ / Can attend
- **ç™½è‰²æ–¹å— (Â·)** = ä¸å¯å…³æ³¨ / Cannot attend
- **çº¢è‰²è™šçº¿** = åˆ†æ®µè¾¹ç•Œ / Segment boundary
- **è“è‰²è™šçº¿** = ç”Ÿæˆå¼€å§‹ / Generation start

#### ç¼–ç é˜¶æ®µæ¨¡å¼ / Encoding Phase Pattern:
```
         Seg1  Seg2  Seg3  Seg4  Seg5
  Seg1   â– â–     Â·Â·    Â·Â·    Â·Â·    Â·Â·     (Isolated)
  Seg2   Â·Â·    â– â–     Â·Â·    Â·Â·    Â·Â·     (Isolated)
  Seg3   Â·Â·    Â·Â·    â– â–     Â·Â·    Â·Â·     (Isolated)
  Seg4   Â·Â·    Â·Â·    Â·Â·    â– â–     Â·Â·     (Isolated)
  Seg5   Â·Â·    Â·Â·    Â·Â·    Â·Â·    â– â–      (Isolated)
```

#### ç”Ÿæˆé˜¶æ®µæ¨¡å¼ / Generation Phase Pattern:
```
         Seg1  Seg2  Seg3  Seg4  Seg5  Gen
  Gen1   â– â–     â– â–     â– â–     â– â–     â– â–     â–    (Fusion)
  Gen2   â– â–     â– â–     â– â–     â– â–     â– â–     â– â–   (Fusion)
```

---

## ğŸ“ å…³é”®æ¦‚å¿µ / Key Concepts

### åˆ†æ®µéš”ç¦» / Segment Isolation

åœ¨ç¼–ç é˜¶æ®µï¼Œæ¯ä¸ªæ”¹å†™ï¼ˆparaphraseï¼‰åœ¨å…¶è‡ªå·±çš„åˆ†æ®µå†…ç‹¬ç«‹å¤„ç†ï¼Œäº’ä¸å¹²æ‰°ï¼š
During encoding, each paraphrase is processed independently within its segment:

- **ç›®çš„** - ä¿æŒæ¯ä¸ªæ”¹å†™çš„ç‹¬ç«‹æ€§ / **Purpose** - Maintain independence of each paraphrase
- **å®ç°** - æ©ç å‡½æ•°åªå…è®¸åŒä¸€åˆ†æ®µå†…çš„ä»¤ç‰Œç›¸äº’å…³æ³¨ / **Implementation** - Mask only allows within-segment attention
- **æ•ˆæœ** - é˜²æ­¢ä¸åŒæ”¹å†™ä¹‹é—´çš„ä¿¡æ¯æ³„éœ² / **Effect** - Prevents information leakage between paraphrases

### èåˆç”Ÿæˆ / Fusion Generation

åœ¨ç”Ÿæˆé˜¶æ®µï¼Œæ–°ç”Ÿæˆçš„ä»¤ç‰Œå¯ä»¥å…³æ³¨æ‰€æœ‰ä¹‹å‰çš„å†…å®¹ï¼ˆæ‰€æœ‰æ”¹å†™ï¼‰ï¼š
During generation, newly generated tokens can attend to all previous content (all paraphrases):

- **ç›®çš„** - èåˆæ¥è‡ªå¤šä¸ªæ”¹å†™çš„ä¿¡æ¯ / **Purpose** - Fuse information from multiple paraphrases
- **å®ç°** - ç”Ÿæˆä»¤ç‰Œçš„æ©ç å‡½æ•°å…è®¸å…³æ³¨æ‰€æœ‰ä½ç½® / **Implementation** - Generated tokens' mask allows attending to all positions
- **æ•ˆæœ** - ç”Ÿæˆæ›´å‡†ç¡®ã€æ›´é²æ£’çš„ç­”æ¡ˆ / **Effect** - Generate more accurate and robust answers

### å› æœçº¦æŸ / Causal Constraint

æ‰€æœ‰æ³¨æ„åŠ›éƒ½å¿…é¡»éµå®ˆå› æœçº¦æŸï¼ˆä¸èƒ½å…³æ³¨æœªæ¥ï¼‰ï¼š
All attention must respect the causal constraint (cannot attend to future):

- **è§„åˆ™** - `q_idx >= kv_idx` / **Rule** - `q_idx >= kv_idx`
- **åŸå› ** - ä¿æŒè‡ªå›å½’ç”Ÿæˆçš„æ­£ç¡®æ€§ / **Reason** - Maintain auto-regressive generation correctness
- **éªŒè¯** - æµ‹è¯•è„šæœ¬ä¼šéªŒè¯æ­¤çº¦æŸ / **Verification** - Test script verifies this constraint

---

## ğŸ”§ ä¿®æ”¹å’Œæ‰©å±• / Modification and Extension

### ä¿®æ”¹æµç¨‹å›¾å¤–è§‚ / Modify Flowchart Appearance

åœ¨ç¬”è®°æœ¬ä¸­ä¿®æ”¹ `FLOWCHART_CONFIG` å­—å…¸ï¼š
Modify the `FLOWCHART_CONFIG` dictionary in the notebook:

```python
FLOWCHART_CONFIG = {
    'figure_size': (14, 16),          # è°ƒæ•´å›¾è¡¨å¤§å° / Adjust figure size
    'box_width': 3.5,                 # è°ƒæ•´æ¡†å®½åº¦ / Adjust box width
    'box_height': 0.6,                # è°ƒæ•´æ¡†é«˜åº¦ / Adjust box height
    'vertical_spacing': 1.2,          # è°ƒæ•´å‚ç›´é—´è· / Adjust spacing
    
    # ä¿®æ”¹é¢œè‰²æ–¹æ¡ˆ / Modify color scheme
    'color_input': '#E3F2FD',         
    'color_processing': '#FFF3E0',    
    'color_attention': '#F3E5F5',     
    'color_generation': '#E8F5E9',    
    'color_output': '#FCE4EC',        
}
```

### æ·»åŠ æ–°çš„æµ‹è¯•åœºæ™¯ / Add New Test Scenarios

åˆ›å»ºæ–°çš„é…ç½®å­—å…¸ï¼š
Create a new configuration dictionary:

```python
MY_CUSTOM_CONFIG = {
    'num_paraphrases': 7,              # æ›´å¤šæ”¹å†™ / More paraphrases
    'tokens_per_paraphrase': 40,       # æ›´é•¿çš„æ”¹å†™ / Longer paraphrases
    'separator_tokens': 5,             # æ›´é•¿çš„åˆ†éš”ç¬¦ / Longer separator
    'num_generated_tokens': 20,        # æ›´å¤šç”Ÿæˆä»¤ç‰Œ / More generated tokens
    'display_mode': 'sampled',         
    'max_display_positions': 40,       # æ˜¾ç¤ºæ›´å¤šä½ç½® / Show more positions
}

# ä½¿ç”¨æ–°é…ç½® / Use new config
fig, matrix, positions = visualize_attention_mask(MY_CUSTOM_CONFIG)
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£ / Related Documentation

- **å®ç°ç»†èŠ‚** - `../flex_attention_generate.py` - FlexAttention å®ç°ä»£ç 
  - **Implementation** - FlexAttention implementation code

- **æ¶æ„è¯´æ˜** - `../docs/ARCHITECTURE.md` - æ¶æ„å›¾å’Œè¯´æ˜
  - **Architecture** - Architecture diagrams and explanations

- **æµ‹è¯•ç¤ºä¾‹** - `../test_mask_visualization.py` - æ›´å¤šæµ‹è¯•ç¤ºä¾‹
  - **Test examples** - More test examples

- **ä½¿ç”¨æŒ‡å—** - `../FLEXATTENTION_USAGE.md` - ä½¿ç”¨è¯´æ˜
  - **Usage guide** - Usage instructions

---

## â“ å¸¸è§é—®é¢˜ / FAQ

### Q: å¦‚ä½•åœ¨æ²¡æœ‰ Jupyter çš„æƒ…å†µä¸‹æŸ¥çœ‹å¯è§†åŒ–ï¼Ÿ
### Q: How to view visualizations without Jupyter?

**A:** è¿è¡Œç¬”è®°æœ¬åï¼Œä½¿ç”¨å¯¼å‡ºåŠŸèƒ½ä¿å­˜å›¾ç‰‡ï¼š
**A:** After running the notebook, use export functionality to save images:

```python
# åœ¨ç¬”è®°æœ¬çš„æœ€åä¸€ä¸ªå•å…ƒæ ¼è¿è¡Œ / Run in the last cell of notebook
# å›¾ç‰‡ä¼šä¿å­˜åˆ° attention_mask_outputs/ ç›®å½•
# Images will be saved to attention_mask_outputs/ directory
```

### Q: å¦‚ä½•éªŒè¯æ©ç å‡½æ•°çš„æ­£ç¡®æ€§ï¼Ÿ
### Q: How to verify mask function correctness?

**A:** è¿è¡Œæµ‹è¯•è„šæœ¬ï¼š
**A:** Run the test script:

```bash
python3 test_visualization.py
```

### Q: å¯ä»¥ç”¨äºå…¶ä»–æ¨¡å‹å—ï¼Ÿ
### Q: Can this be used for other models?

**A:** æ˜¯çš„ï¼åªéœ€ä¿®æ”¹é…ç½®å‚æ•°æ¥åŒ¹é…ä½ çš„æ¨¡å‹ï¼š
**A:** Yes! Just modify the configuration parameters to match your model:

- `num_paraphrases` - æ”¹å†™æ•°é‡ / Number of paraphrases
- `tokens_per_paraphrase` - æ¯ä¸ªæ”¹å†™çš„å¹³å‡é•¿åº¦ / Average length per paraphrase
- `num_generated_tokens` - æœŸæœ›ç”Ÿæˆçš„é•¿åº¦ / Expected generation length

---

## ğŸš€ å¿«é€Ÿå¼€å§‹ / Quick Start

```bash
# 1. å®‰è£…ä¾èµ– / Install dependencies
pip install jupyter matplotlib numpy

# 2. å¯åŠ¨ç¬”è®°æœ¬ / Start notebook
cd plot/
jupyter notebook flowchart_and_attention_mask_visualization.ipynb

# 3. è¿è¡Œæ‰€æœ‰å•å…ƒæ ¼ / Run all cells
# åœ¨ Jupyter ç•Œé¢ä¸­: Cell -> Run All
# In Jupyter interface: Cell -> Run All

# 4. æŸ¥çœ‹ç”Ÿæˆçš„å¯è§†åŒ– / View generated visualizations
# 5. ä¿®æ”¹ CUSTOM_CONFIG è¿›è¡Œå®éªŒ / Modify CUSTOM_CONFIG to experiment
```

---

## ğŸ“§ æ”¯æŒ / Support

å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·æŸ¥çœ‹ï¼š
If you encounter issues, please check:

1. **ä¾èµ–å®‰è£…** - ç¡®ä¿å®‰è£…äº†æ‰€æœ‰å¿…éœ€çš„åŒ… / **Dependencies** - Ensure all required packages are installed
2. **Python ç‰ˆæœ¬** - å»ºè®®ä½¿ç”¨ Python 3.8+ / **Python version** - Recommended Python 3.8+
3. **æµ‹è¯•è„šæœ¬** - è¿è¡Œ `test_visualization.py` éªŒè¯åŠŸèƒ½ / **Test script** - Run `test_visualization.py` to verify functions

---

**æœ€åæ›´æ–° / Last Updated**: 2024-10-28
