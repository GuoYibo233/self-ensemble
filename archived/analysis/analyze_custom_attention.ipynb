{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eff82461",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14deb5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "from utils import partial_match, partial_match_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a4f49b",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6f4f360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Base directory found: /net/tokyo100-10g/data/str01_01/y-guo/datasets/myriadlama/llama3.1_8b/\n"
     ]
    }
   ],
   "source": [
    "# Configure your dataset path here\n",
    "MODEL_NAME = \"llama3.1_8b\"  # Change to your model name\n",
    "BASE_DIR = f\"/net/tokyo100-10g/data/str01_01/y-guo/datasets/myriadlama/llama3.1_8b/\"\n",
    "\n",
    "# Check if path exists\n",
    "if not os.path.exists(BASE_DIR):\n",
    "    print(f\"‚ö†Ô∏è  Warning: Base directory not found: {BASE_DIR}\")\n",
    "    print(f\"   Please update BASE_DIR variable\")\n",
    "else:\n",
    "    print(f\"‚úÖ Base directory found: {BASE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b7f729",
   "metadata": {},
   "source": [
    "## 3. Lemmatization Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "808613a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spacy loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Import spacy for lemmatization\n",
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    print(\"‚úÖ Spacy loaded successfully\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  Spacy not found. Install with: pip install spacy\")\n",
    "    print(\"   Then download model: python -m spacy download en_core_web_lg\")\n",
    "    nlp = None\n",
    "except OSError:\n",
    "    print(\"‚ö†Ô∏è  Spacy model not found. Download with: python -m spacy download en_core_web_lg\")\n",
    "    nlp = None\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"Lemmatize a text string.\"\"\"\n",
    "    if nlp is None:\n",
    "        return text.lower().split()\n",
    "    doc = nlp(text)\n",
    "    return [token.lemma_.lower() for token in doc]\n",
    "\n",
    "def lemmatize_answers(answers):\n",
    "    \"\"\"Lemmatize a list of answer strings.\"\"\"\n",
    "    if isinstance(answers, str):\n",
    "        # Parse string representation of list\n",
    "        import ast\n",
    "        try:\n",
    "            answers = ast.literal_eval(answers)\n",
    "        except:\n",
    "            answers = [answers]\n",
    "    return [lemmatize_text(ans) for ans in answers]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079975b3",
   "metadata": {},
   "source": [
    "## 4. Discover All Custom Attention Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14a22bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Discovered Files\n",
      "======================================================================\n",
      "\n",
      "Total files found: 5\n",
      "\n",
      "üéØ Custom Attention Results:\n",
      "   - custom_1paras\n",
      "   - custom_5paras\n",
      "\n",
      "üìä Baseline Results:\n",
      "   - baseline_baseline_per_prompt\n",
      "\n",
      "‚ö° FlexAttention Results:\n",
      "   - flex_myriadlama_flex_1paras\n",
      "   - flex_myriadlama_flex_5paras\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def discover_custom_attention_files(base_dir):\n",
    "    \"\"\"\n",
    "    Discover all custom attention result files.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary mapping file names to file paths\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    if not os.path.exists(base_dir):\n",
    "        print(f\"‚ùå Base directory not found: {base_dir}\")\n",
    "        return results\n",
    "    \n",
    "    # Search for custom attention files (CSV)\n",
    "    custom_files = glob(os.path.join(base_dir, \"myriadlama_custom_*paras.csv\"))\n",
    "    \n",
    "    for file_path in custom_files:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        # Extract number of paraphrases from filename\n",
    "        # e.g., myriadlama_custom_1paras.csv -> 1\n",
    "        import re\n",
    "        match = re.search(r'custom_(\\d+)paras', file_name)\n",
    "        if match:\n",
    "            num_paras = int(match.group(1))\n",
    "            results[f\"custom_{num_paras}paras\"] = file_path\n",
    "    \n",
    "    # Also search for baseline and flex attention files for comparison\n",
    "    baseline_files = glob(os.path.join(base_dir, \"*baseline*.csv\")) + glob(os.path.join(base_dir, \"*baseline*.feather\"))\n",
    "    flex_files = glob(os.path.join(base_dir, \"*flex*.csv\")) + glob(os.path.join(base_dir, \"*flex*.feather\"))\n",
    "    \n",
    "    for file_path in baseline_files:\n",
    "        file_name = os.path.basename(file_path).replace('.csv', '').replace('.feather', '')\n",
    "        results[f\"baseline_{file_name}\"] = file_path\n",
    "    \n",
    "    for file_path in flex_files:\n",
    "        file_name = os.path.basename(file_path).replace('.csv', '').replace('.feather', '')\n",
    "        results[f\"flex_{file_name}\"] = file_path\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Discover all files\n",
    "all_files = discover_custom_attention_files(BASE_DIR)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Discovered Files\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nTotal files found: {len(all_files)}\\n\")\n",
    "\n",
    "# Categorize by type\n",
    "custom_files = {k: v for k, v in all_files.items() if k.startswith('custom_')}\n",
    "baseline_files = {k: v for k, v in all_files.items() if k.startswith('baseline_')}\n",
    "flex_files = {k: v for k, v in all_files.items() if k.startswith('flex_')}\n",
    "\n",
    "if custom_files:\n",
    "    print(\"üéØ Custom Attention Results:\")\n",
    "    for method in sorted(custom_files.keys()):\n",
    "        print(f\"   - {method}\")\n",
    "\n",
    "if baseline_files:\n",
    "    print(\"\\nüìä Baseline Results:\")\n",
    "    for method in sorted(baseline_files.keys()):\n",
    "        print(f\"   - {method}\")\n",
    "\n",
    "if flex_files:\n",
    "    print(\"\\n‚ö° FlexAttention Results:\")\n",
    "    for method in sorted(flex_files.keys()):\n",
    "        print(f\"   - {method}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bbafcf",
   "metadata": {},
   "source": [
    "## 5. Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c2904ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading all files...\n",
      "‚úÖ Loaded custom_5paras: 2000 samples\n",
      "‚úÖ Loaded custom_1paras: 2000 samples\n",
      "‚úÖ Loaded baseline_baseline_per_prompt: 10000 samples\n",
      "‚úÖ Loaded flex_myriadlama_flex_5paras: 2000 samples\n",
      "‚úÖ Loaded flex_myriadlama_flex_1paras: 100 samples\n"
     ]
    }
   ],
   "source": [
    "def load_file(file_path):\n",
    "    \"\"\"\n",
    "    Load a result file (CSV or Feather).\n",
    "    \"\"\"\n",
    "    if file_path.endswith('.csv'):\n",
    "        return pd.read_csv(file_path)\n",
    "    elif file_path.endswith('.feather'):\n",
    "        return pd.read_feather(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file_path}\")\n",
    "\n",
    "def load_all_files(all_files):\n",
    "    \"\"\"\n",
    "    Load all discovered files.\n",
    "    \"\"\"\n",
    "    loaded_data = {}\n",
    "    \n",
    "    print(\"\\nLoading all files...\")\n",
    "    for name, file_path in all_files.items():\n",
    "        try:\n",
    "            df = load_file(file_path)\n",
    "            loaded_data[name] = df\n",
    "            print(f\"‚úÖ Loaded {name}: {len(df)} samples\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading {name}: {e}\")\n",
    "    \n",
    "    return loaded_data\n",
    "\n",
    "# Load all files\n",
    "loaded_data = load_all_files(all_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cdfce7",
   "metadata": {},
   "source": [
    "## 6. Apply Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f72e4ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying lemmatization...\n",
      "Processing custom_5paras...\n",
      "Processing custom_1paras...\n",
      "Processing baseline_baseline_per_prompt...\n",
      "   Lemmatizing predictions...\n",
      "   Lemmatizing answers...\n",
      "   Lemmatizing answers...\n",
      "Processing flex_myriadlama_flex_5paras...\n",
      "   Lemmatizing predictions...\n",
      "Processing flex_myriadlama_flex_5paras...\n",
      "   Lemmatizing predictions...\n",
      "   Lemmatizing answers...\n",
      "   Lemmatizing answers...\n",
      "Processing flex_myriadlama_flex_1paras...\n",
      "   Lemmatizing predictions...\n",
      "Processing flex_myriadlama_flex_1paras...\n",
      "   Lemmatizing predictions...\n",
      "   Lemmatizing answers...\n",
      "   Lemmatizing answers...\n",
      "\n",
      "‚úÖ Lemmatization complete\n",
      "\n",
      "‚úÖ Lemmatization complete\n"
     ]
    }
   ],
   "source": [
    "def apply_lemmatization_to_df(df):\n",
    "    \"\"\"\n",
    "    Apply lemmatization to a DataFrame if not already present.\n",
    "    \"\"\"\n",
    "    if nlp is None:\n",
    "        print(\"‚ö†Ô∏è  Spacy not available, skipping lemmatization\")\n",
    "        return df\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Lemmatize predictions\n",
    "    if 'prediction' in df.columns and 'predict_lemma' not in df.columns:\n",
    "        print(\"   Lemmatizing predictions...\")\n",
    "        df['predict_lemma'] = df['prediction'].apply(\n",
    "            lambda x: lemmatize_text(str(x)) if x is not None and str(x).strip() else []\n",
    "        )\n",
    "    \n",
    "    # Lemmatize answers\n",
    "    if 'answers' in df.columns and 'answer_lemmas' not in df.columns:\n",
    "        print(\"   Lemmatizing answers...\")\n",
    "        df['answer_lemmas'] = df['answers'].apply(\n",
    "            lambda x: lemmatize_answers(x) if x is not None else []\n",
    "        )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply lemmatization to all loaded data\n",
    "print(\"\\nApplying lemmatization...\")\n",
    "for name in loaded_data.keys():\n",
    "    print(f\"Processing {name}...\")\n",
    "    loaded_data[name] = apply_lemmatization_to_df(loaded_data[name])\n",
    "\n",
    "print(\"\\n‚úÖ Lemmatization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0301a5",
   "metadata": {},
   "source": [
    "## 7. Calculate Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441a6fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating accuracies...\n",
      "‚úÖ custom_5paras: 0.5125 (51.25%)\n",
      "‚úÖ custom_1paras: 0.5860 (58.60%)\n",
      "‚ö†Ô∏è  baseline_baseline_per_prompt: Could not calculate accuracy\n",
      "‚ö†Ô∏è  flex_myriadlama_flex_5paras: Could not calculate accuracy\n",
      "‚ö†Ô∏è  flex_myriadlama_flex_1paras: Could not calculate accuracy\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(df):\n",
    "    \"\"\"\n",
    "    Calculate accuracy for a result DataFrame.\n",
    "    \"\"\"\n",
    "    if 'predict_lemma' not in df.columns or 'answer_lemmas' not in df.columns:\n",
    "        return None\n",
    "    \n",
    "    # Ensure proper format\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    predictions = df_copy['predict_lemma'].tolist()\n",
    "    answers = df_copy['answer_lemmas'].tolist()\n",
    "    \n",
    "    try:\n",
    "        accuracy = partial_match_scores(predictions, answers)\n",
    "        return accuracy\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating accuracy: {e}\")\n",
    "        return None\n",
    "\n",
    "# Calculate accuracies\n",
    "accuracies = {}\n",
    "\n",
    "print(\"\\nCalculating accuracies...\")\n",
    "for name, df in loaded_data.items():\n",
    "    acc = calculate_accuracy(df)\n",
    "    if acc is not None:\n",
    "        accuracies[name] = acc\n",
    "        print(f\"‚úÖ {name}: {acc:.4f} ({acc*100:.2f}%)\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  {name}: Could not calculate accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320a3e55",
   "metadata": {},
   "source": [
    "## 8. Generate Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4e8a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_comparison_table(accuracies, loaded_data):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive comparison table.\n",
    "    \"\"\"\n",
    "    comparison_data = []\n",
    "    \n",
    "    for method in accuracies.keys():\n",
    "        df = loaded_data[method]\n",
    "        acc = accuracies[method]\n",
    "        \n",
    "        # Determine category\n",
    "        if method.startswith('custom_'):\n",
    "            category = 'Custom Attention'\n",
    "            # Extract number of paraphrases\n",
    "            import re\n",
    "            match = re.search(r'custom_(\\d+)paras', method)\n",
    "            num_paras = int(match.group(1)) if match else 'N/A'\n",
    "        elif method.startswith('baseline_'):\n",
    "            category = 'Baseline'\n",
    "            num_paras = 'N/A'\n",
    "        elif method.startswith('flex_'):\n",
    "            category = 'FlexAttention'\n",
    "            num_paras = 'N/A'\n",
    "        else:\n",
    "            category = 'Other'\n",
    "            num_paras = 'N/A'\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'Method': method,\n",
    "            'Category': category,\n",
    "            'Num_Paraphrases': num_paras,\n",
    "            'Accuracy': acc,\n",
    "            'Accuracy_Pct': acc * 100,\n",
    "            'Total_Samples': len(df)\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Sort by accuracy (descending)\n",
    "    comparison_df = comparison_df.sort_values('Accuracy', ascending=False)\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "# Generate and display comparison table\n",
    "if accuracies:\n",
    "    comparison_table = generate_comparison_table(accuracies, loaded_data)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPARISON TABLE\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    display(comparison_table)\n",
    "    \n",
    "    # Statistical summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STATISTICAL SUMMARY\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Best method overall\n",
    "    best_idx = comparison_table['Accuracy'].idxmax()\n",
    "    best_method = comparison_table.loc[best_idx]\n",
    "    print(f\"üèÜ Best Method Overall: {best_method['Method']}\")\n",
    "    print(f\"   Accuracy: {best_method['Accuracy']:.4f} ({best_method['Accuracy_Pct']:.2f}%)\")\n",
    "    print(f\"   Category: {best_method['Category']}\")\n",
    "    \n",
    "    # Best in each category\n",
    "    for category in comparison_table['Category'].unique():\n",
    "        cat_df = comparison_table[comparison_table['Category'] == category]\n",
    "        if len(cat_df) > 0:\n",
    "            best_in_cat = cat_df.iloc[0]\n",
    "            print(f\"\\nüìä Best {category}: {best_in_cat['Method']}\")\n",
    "            print(f\"   Accuracy: {best_in_cat['Accuracy']:.4f} ({best_in_cat['Accuracy_Pct']:.2f}%)\")\n",
    "    \n",
    "    # Custom attention scaling analysis\n",
    "    custom_results = comparison_table[comparison_table['Category'] == 'Custom Attention']\n",
    "    if len(custom_results) > 1:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"CUSTOM ATTENTION SCALING ANALYSIS\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        custom_results_sorted = custom_results.sort_values('Num_Paraphrases')\n",
    "        print(\"Accuracy vs Number of Paraphrases:\")\n",
    "        for _, row in custom_results_sorted.iterrows():\n",
    "            print(f\"   {row['Num_Paraphrases']} paraphrases: {row['Accuracy']:.4f} ({row['Accuracy_Pct']:.2f}%)\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No accuracies available for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a1c7c7",
   "metadata": {},
   "source": [
    "## 9. Detailed Examples from Custom Attention Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe71fc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_detailed_examples(method_name, df, num_examples=5, show_correct=True, show_incorrect=True):\n",
    "    \"\"\"\n",
    "    Show detailed examples from a result file.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"DETAILED EXAMPLES: {method_name}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Check if we can determine correctness\n",
    "    can_check = 'predict_lemma' in df.columns and 'answer_lemmas' in df.columns\n",
    "    \n",
    "    if can_check:\n",
    "        # Add correctness\n",
    "        df_copy = df.copy()\n",
    "        correctness = []\n",
    "        for pred, ans in zip(df_copy['predict_lemma'], df_copy['answer_lemmas']):\n",
    "            try:\n",
    "                is_correct = partial_match(pred, ans)\n",
    "                correctness.append(is_correct)\n",
    "            except:\n",
    "                correctness.append(None)\n",
    "        df_copy['is_correct'] = correctness\n",
    "        \n",
    "        # Filter\n",
    "        if show_correct and show_incorrect:\n",
    "            filtered_df = df_copy\n",
    "        elif show_correct:\n",
    "            filtered_df = df_copy[df_copy['is_correct'] == True]\n",
    "        elif show_incorrect:\n",
    "            filtered_df = df_copy[df_copy['is_correct'] == False]\n",
    "        else:\n",
    "            filtered_df = df_copy\n",
    "    else:\n",
    "        filtered_df = df\n",
    "    \n",
    "    # Sample\n",
    "    sample_df = filtered_df.head(num_examples)\n",
    "    \n",
    "    for idx, (i, row) in enumerate(sample_df.iterrows(), 1):\n",
    "        print(f\"\\n{'‚îÄ'*80}\")\n",
    "        print(f\"Example {idx}/{len(sample_df)}\")\n",
    "        print(f\"{'‚îÄ'*80}\\n\")\n",
    "        \n",
    "        if 'uuid' in row:\n",
    "            print(f\"UUID: {row['uuid']}\")\n",
    "        \n",
    "        if 'templates' in row:\n",
    "            templates = row['templates']\n",
    "            if pd.notna(templates):\n",
    "                print(f\"\\nTemplates: {templates}\")\n",
    "        \n",
    "        if 'prediction' in row:\n",
    "            print(f\"\\nPrediction: {row['prediction']}\")\n",
    "        \n",
    "        if 'answers' in row:\n",
    "            print(f\"Correct Answers: {row['answers']}\")\n",
    "        \n",
    "        if 'predict_lemma' in row:\n",
    "            print(f\"\\nPrediction (lemma): {row['predict_lemma']}\")\n",
    "        \n",
    "        if 'answer_lemmas' in row:\n",
    "            print(f\"Answer Lemmas: {row['answer_lemmas']}\")\n",
    "        \n",
    "        if can_check and 'is_correct' in row:\n",
    "            status = \"‚úÖ CORRECT\" if row['is_correct'] else \"‚ùå INCORRECT\"\n",
    "            print(f\"\\nStatus: {status}\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\\n\")\n",
    "\n",
    "# Show examples from custom attention results\n",
    "if custom_files and loaded_data:\n",
    "    # Show examples from first custom file\n",
    "    method_name = list(custom_files.keys())[0]\n",
    "    if method_name in loaded_data:\n",
    "        show_detailed_examples(method_name, loaded_data[method_name], num_examples=5)\n",
    "else:\n",
    "    print(\"No custom attention files loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cdcedb",
   "metadata": {},
   "source": [
    "## 10. Cross-Method Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed763ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_same_examples(methods, loaded_data, num_examples=3):\n",
    "    \"\"\"\n",
    "    Compare the same examples across different methods.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"CROSS-METHOD COMPARISON\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Get common UUIDs\n",
    "    common_uuids = None\n",
    "    for method in methods:\n",
    "        if method not in loaded_data:\n",
    "            continue\n",
    "        df = loaded_data[method]\n",
    "        if 'uuid' not in df.columns:\n",
    "            print(f\"‚ö†Ô∏è  Method {method} does not have UUID column\")\n",
    "            return\n",
    "        \n",
    "        uuids = set(df['uuid'].unique())\n",
    "        if common_uuids is None:\n",
    "            common_uuids = uuids\n",
    "        else:\n",
    "            common_uuids = common_uuids.intersection(uuids)\n",
    "    \n",
    "    common_uuids = list(common_uuids)[:num_examples]\n",
    "    \n",
    "    for idx, uuid in enumerate(common_uuids, 1):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Example {idx}/{len(common_uuids)} - UUID: {uuid}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        for method in methods:\n",
    "            if method not in loaded_data:\n",
    "                continue\n",
    "            \n",
    "            df = loaded_data[method]\n",
    "            row = df[df['uuid'] == uuid].iloc[0]\n",
    "            \n",
    "            print(f\"\\n{'‚îÄ'*40}\")\n",
    "            print(f\"Method: {method}\")\n",
    "            print(f\"{'‚îÄ'*40}\\n\")\n",
    "            \n",
    "            if 'prediction' in row:\n",
    "                print(f\"Prediction: {row['prediction']}\")\n",
    "            \n",
    "            if 'predict_lemma' in row and 'answer_lemmas' in row:\n",
    "                try:\n",
    "                    is_correct = partial_match(row['predict_lemma'], row['answer_lemmas'])\n",
    "                    status = \"‚úÖ CORRECT\" if is_correct else \"‚ùå INCORRECT\"\n",
    "                    print(f\"Status: {status}\")\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        # Show correct answer once\n",
    "        first_method = methods[0]\n",
    "        if first_method in loaded_data:\n",
    "            df = loaded_data[first_method]\n",
    "            row = df[df['uuid'] == uuid].iloc[0]\n",
    "            if 'answers' in row:\n",
    "                print(f\"\\n{'‚îÄ'*40}\")\n",
    "                print(f\"Correct Answers: {row['answers']}\")\n",
    "                print(f\"{'‚îÄ'*40}\")\n",
    "\n",
    "# Compare custom attention with baseline/flex\n",
    "if len(loaded_data) >= 2:\n",
    "    methods_to_compare = list(loaded_data.keys())[:3]  # Compare first 3 methods\n",
    "    compare_same_examples(methods_to_compare, loaded_data, num_examples=3)\n",
    "else:\n",
    "    print(\"Need at least 2 methods to compare\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a29d5d",
   "metadata": {},
   "source": [
    "## 11. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3285b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Base Directory: {BASE_DIR}\\n\")\n",
    "\n",
    "print(f\"Total files found: {len(all_files)}\")\n",
    "print(f\"Successfully loaded: {len(loaded_data)}\")\n",
    "print(f\"With accuracy metrics: {len(accuracies)}\\n\")\n",
    "\n",
    "if accuracies:\n",
    "    best_method = max(accuracies.items(), key=lambda x: x[1])\n",
    "    worst_method = min(accuracies.items(), key=lambda x: x[1])\n",
    "    avg_accuracy = sum(accuracies.values()) / len(accuracies)\n",
    "    \n",
    "    print(f\"üèÜ Best: {best_method[0]} ({best_method[1]:.4f})\")\n",
    "    print(f\"üìâ Worst: {worst_method[0]} ({worst_method[1]:.4f})\")\n",
    "    print(f\"üìä Average: {avg_accuracy:.4f}\")\n",
    "    print(f\"üìà Range: {worst_method[1]:.4f} - {best_method[1]:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flexattention",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
