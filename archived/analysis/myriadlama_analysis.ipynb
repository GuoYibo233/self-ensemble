{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MyriadLAMA Results Analysis\n",
    "\n",
    "This notebook provides comprehensive analysis tools for MyriadLAMA generation results.\n",
    "\n",
    "## Features:\n",
    "1. **One-click search** for all generation files and baselines\n",
    "2. **Built-in lemmatization** functionality\n",
    "3. **Comparison tables** generation\n",
    "4. **Detailed examples** for in-depth analysis\n",
    "\n",
    "## Usage:\n",
    "1. Set the `DATASET_ROOT` variable to your dataset path\n",
    "2. Run all cells to load results and generate analysis\n",
    "3. Use the provided functions to explore specific results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "from utils import partial_match, partial_match_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset root found: /net/tokyo100-10g/data/str01_01/y-guo/datasets/myriadlama/llama3.1_8b/myriadlama_custom_1paras.csv\n"
     ]
    }
   ],
   "source": [
    "# Configure your dataset path here\n",
    "DATASET_NAME = \"myriadlama\"\n",
    "MODEL_NAME = \"qwen2.5_7b_it\"  # Change to your model name\n",
    "DATASET_ROOT = f\"/net/tokyo100-10g/data/str01_01/y-guo/datasets/myriadlama/llama3.1_8b/myriadlama_custom_1paras.csv\"\n",
    "\n",
    "# Check if path exists\n",
    "if not os.path.exists(DATASET_ROOT):\n",
    "    print(f\"‚ö†Ô∏è  Warning: Dataset root not found: {DATASET_ROOT}\")\n",
    "    print(f\"   Please update DATASET_ROOT variable\")\n",
    "else:\n",
    "    print(f\"‚úÖ Dataset root found: {DATASET_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Lemmatization Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spacy loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Import spacy for lemmatization\n",
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    print(\"‚úÖ Spacy loaded successfully\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  Spacy not found. Install with: pip install spacy\")\n",
    "    print(\"   Then download model: python -m spacy download en_core_web_lg\")\n",
    "    nlp = None\n",
    "except OSError:\n",
    "    print(\"‚ö†Ô∏è  Spacy model not found. Download with: python -m spacy download en_core_web_lg\")\n",
    "    nlp = None\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"Lemmatize a text string.\"\"\"\n",
    "    if nlp is None:\n",
    "        return text.lower().split()\n",
    "    doc = nlp(text)\n",
    "    return [token.lemma_.lower() for token in doc]\n",
    "\n",
    "def lemmatize_answers(answers):\n",
    "    \"\"\"Lemmatize a list of answer strings.\"\"\"\n",
    "    return [lemmatize_text(ans) for ans in answers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. File Discovery - One-Click Search for All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Discovered Result Files\n",
      "======================================================================\n",
      "\n",
      "Total files found: 0\n",
      "\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def discover_all_results(dataset_root):\n",
    "    \"\"\"\n",
    "    Discover all generation result files in the dataset directory.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary mapping method names to file paths\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    if not os.path.exists(dataset_root):\n",
    "        print(f\"‚ùå Dataset root not found: {dataset_root}\")\n",
    "        return results\n",
    "    \n",
    "    # Search for all .feather files\n",
    "    feather_files = glob(os.path.join(dataset_root, \"*.feather\"))\n",
    "    \n",
    "    for file_path in feather_files:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        method_name = file_name.replace(\".feather\", \"\")\n",
    "        results[method_name] = file_path\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Discover all results\n",
    "all_results = discover_all_results(DATASET_ROOT)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Discovered Result Files\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nTotal files found: {len(all_results)}\\n\")\n",
    "\n",
    "# Categorize results\n",
    "baselines = [k for k in all_results.keys() if 'baseline' in k]\n",
    "ensembles = [k for k in all_results.keys() if 'ensemble' in k]\n",
    "flex_attention = [k for k in all_results.keys() if 'flex_attention' in k]\n",
    "others = [k for k in all_results.keys() if k not in baselines + ensembles + flex_attention]\n",
    "\n",
    "if baselines:\n",
    "    print(\"üìä Baselines:\")\n",
    "    for method in sorted(baselines):\n",
    "        print(f\"   - {method}\")\n",
    "\n",
    "if ensembles:\n",
    "    print(\"\\nüîÑ Ensemble Methods:\")\n",
    "    for method in sorted(ensembles):\n",
    "        print(f\"   - {method}\")\n",
    "\n",
    "if flex_attention:\n",
    "    print(\"\\n‚ö° FlexAttention Methods:\")\n",
    "    for method in sorted(flex_attention):\n",
    "        print(f\"   - {method}\")\n",
    "\n",
    "if others:\n",
    "    print(\"\\nüìÅ Other Results:\")\n",
    "    for method in sorted(others):\n",
    "        print(f\"   - {method}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load and Process Results\n",
    "\n",
    "This cell loads all discovered result files and automatically applies lemmatization to files that don't have it.\n",
    "\n",
    "**Automatic Lemmatization:**\n",
    "- Enabled by default for files missing `predict_lemma` or `answer_lemmas` columns\n",
    "- Requires Spacy to be loaded (see Section 3)\n",
    "- Set `apply_lemmatization=False` to disable if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading all result files...\n"
     ]
    }
   ],
   "source": [
    "def load_result_file(file_path, apply_lemmatization=False):\n",
    "    \"\"\"\n",
    "    Load a result file and optionally apply lemmatization.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the .feather file\n",
    "        apply_lemmatization: Whether to apply lemmatization if not present\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with results\n",
    "    \"\"\"\n",
    "    df = pd.read_feather(file_path)\n",
    "    \n",
    "    # Check if lemmatization is needed\n",
    "    needs_lemmatization = apply_lemmatization and (\n",
    "        'predict_lemma' not in df.columns or \n",
    "        'answer_lemmas' not in df.columns\n",
    "    )\n",
    "    \n",
    "    if needs_lemmatization and nlp is not None:\n",
    "        print(f\"   Applying lemmatization to {os.path.basename(file_path)}...\")\n",
    "        \n",
    "        if 'prediction' in df.columns and 'predict_lemma' not in df.columns:\n",
    "            df['predict_lemma'] = df['prediction'].apply(lemmatize_text)\n",
    "        \n",
    "        if 'answers' in df.columns and 'answer_lemmas' not in df.columns:\n",
    "            df['answer_lemmas'] = df['answers'].apply(lemmatize_answers)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_all_results(all_results, apply_lemmatization=False):\n",
    "    \"\"\"\n",
    "    Load all discovered result files.\n",
    "    \n",
    "    Args:\n",
    "        all_results: Dictionary of method names to file paths\n",
    "        apply_lemmatization: Whether to apply lemmatization\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary mapping method names to DataFrames\n",
    "    \"\"\"\n",
    "    loaded_results = {}\n",
    "    \n",
    "    print(\"\\nLoading all result files...\")\n",
    "    for method, file_path in all_results.items():\n",
    "        try:\n",
    "            df = load_result_file(file_path, apply_lemmatization)\n",
    "            loaded_results[method] = df\n",
    "            print(f\"‚úÖ Loaded {method}: {len(df)} samples\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading {method}: {e}\")\n",
    "    \n",
    "    return loaded_results\n",
    "\n",
    "# Load all results (automatic lemmatization enabled by default for files without lemmas)\n",
    "# Set apply_lemmatization=False if you want to skip automatic lemmatization\n",
    "loaded_results = load_all_results(all_results, apply_lemmatization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Calculate Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating accuracies...\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(df):\n",
    "    \"\"\"\n",
    "    Calculate accuracy for a result DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with results\n",
    "    \n",
    "    Returns:\n",
    "        float: Accuracy score (0-1)\n",
    "    \"\"\"\n",
    "    if 'predict_lemma' not in df.columns or 'answer_lemmas' not in df.columns:\n",
    "        return None\n",
    "    \n",
    "    # Process answer_lemmas to ensure proper format\n",
    "    df_copy = df.copy()\n",
    "    df_copy['answer_lemmas'] = df_copy['answer_lemmas'].apply(\n",
    "        lambda xs: [list(x) if not isinstance(x, list) else x for x in xs] \n",
    "        if isinstance(xs, list) else xs\n",
    "    )\n",
    "    \n",
    "    predictions = df_copy['predict_lemma'].tolist()\n",
    "    answers = df_copy['answer_lemmas'].tolist()\n",
    "    \n",
    "    try:\n",
    "        accuracy = partial_match_scores(predictions, answers)\n",
    "        return accuracy\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating accuracy: {e}\")\n",
    "        return None\n",
    "\n",
    "# Calculate accuracies for all loaded results\n",
    "accuracies = {}\n",
    "\n",
    "print(\"\\nCalculating accuracies...\")\n",
    "for method, df in loaded_results.items():\n",
    "    acc = calculate_accuracy(df)\n",
    "    if acc is not None:\n",
    "        accuracies[method] = acc\n",
    "        print(f\"‚úÖ {method}: {acc:.3f}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  {method}: Lemmatization not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_comparison_table(accuracies, loaded_results):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive comparison table.\n",
    "    \n",
    "    Args:\n",
    "        accuracies: Dictionary of method names to accuracy scores\n",
    "        loaded_results: Dictionary of method names to DataFrames\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Comparison table\n",
    "    \"\"\"\n",
    "    comparison_data = []\n",
    "    \n",
    "    for method in accuracies.keys():\n",
    "        df = loaded_results[method]\n",
    "        acc = accuracies[method]\n",
    "        \n",
    "        # Determine method category\n",
    "        if 'baseline' in method:\n",
    "            category = 'Baseline'\n",
    "        elif 'ensemble' in method:\n",
    "            category = 'Ensemble'\n",
    "        elif 'flex_attention' in method:\n",
    "            category = 'FlexAttention'\n",
    "        else:\n",
    "            category = 'Other'\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'Method': method,\n",
    "            'Category': category,\n",
    "            'Accuracy': acc,\n",
    "            'Total Samples': len(df),\n",
    "            'Unique Questions': df['uuid'].nunique() if 'uuid' in df.columns else 'N/A'\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Sort by accuracy (descending)\n",
    "    comparison_df = comparison_df.sort_values('Accuracy', ascending=False)\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "# Generate comparison table\n",
    "if accuracies:\n",
    "    comparison_table = generate_comparison_table(accuracies, loaded_results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPARISON TABLE\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Display the table\n",
    "    display(comparison_table)\n",
    "    \n",
    "    # Calculate improvements over best baseline\n",
    "    baseline_methods = comparison_table[comparison_table['Category'] == 'Baseline']\n",
    "    if not baseline_methods.empty:\n",
    "        best_baseline_acc = baseline_methods['Accuracy'].max()\n",
    "        print(f\"\\nüìä Best Baseline Accuracy: {best_baseline_acc:.3f}\")\n",
    "        \n",
    "        print(\"\\nüöÄ Improvements over Best Baseline:\")\n",
    "        for _, row in comparison_table.iterrows():\n",
    "            if row['Category'] != 'Baseline':\n",
    "                improvement = row['Accuracy'] - best_baseline_acc\n",
    "                pct_improvement = (improvement / best_baseline_acc) * 100\n",
    "                print(f\"   {row['Method']:<40} {improvement:+.4f} ({pct_improvement:+.2f}%)\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No accuracies available for comparison. Results may need lemmatization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate Detailed Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_detailed_examples(method_name, df, num_examples=5, show_correct=True, show_incorrect=True):\n",
    "    \"\"\"\n",
    "    Generate detailed examples from a result file for analysis.\n",
    "    \n",
    "    Args:\n",
    "        method_name: Name of the method\n",
    "        df: DataFrame with results\n",
    "        num_examples: Number of examples to show\n",
    "        show_correct: Whether to show correct predictions\n",
    "        show_incorrect: Whether to show incorrect predictions\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"DETAILED EXAMPLES: {method_name}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Check if we can determine correctness\n",
    "    can_check_correctness = (\n",
    "        'predict_lemma' in df.columns and \n",
    "        'answer_lemmas' in df.columns\n",
    "    )\n",
    "    \n",
    "    if can_check_correctness:\n",
    "        # Add correctness column\n",
    "        df_with_correctness = df.copy()\n",
    "        df_with_correctness['answer_lemmas'] = df_with_correctness['answer_lemmas'].apply(\n",
    "            lambda xs: [list(x) if not isinstance(x, list) else x for x in xs] \n",
    "            if isinstance(xs, list) else xs\n",
    "        )\n",
    "        \n",
    "        correctness = []\n",
    "        for pred, ans in zip(df_with_correctness['predict_lemma'], df_with_correctness['answer_lemmas']):\n",
    "            try:\n",
    "                is_correct = partial_match(pred, ans)\n",
    "                correctness.append(is_correct)\n",
    "            except:\n",
    "                correctness.append(None)\n",
    "        \n",
    "        df_with_correctness['is_correct'] = correctness\n",
    "        \n",
    "        # Filter based on correctness preference\n",
    "        if show_correct and show_incorrect:\n",
    "            filtered_df = df_with_correctness\n",
    "        elif show_correct:\n",
    "            filtered_df = df_with_correctness[df_with_correctness['is_correct'] == True]\n",
    "        elif show_incorrect:\n",
    "            filtered_df = df_with_correctness[df_with_correctness['is_correct'] == False]\n",
    "        else:\n",
    "            filtered_df = df_with_correctness\n",
    "    else:\n",
    "        filtered_df = df\n",
    "    \n",
    "    # Sample examples\n",
    "    sample_df = filtered_df.head(num_examples)\n",
    "    \n",
    "    for idx, (i, row) in enumerate(sample_df.iterrows(), 1):\n",
    "        print(f\"\\n{'‚îÄ'*80}\")\n",
    "        print(f\"Example {idx}/{num_examples}\")\n",
    "        print(f\"{'‚îÄ'*80}\\n\")\n",
    "        \n",
    "        # Display UUID if available\n",
    "        if 'uuid' in row:\n",
    "            print(f\"UUID: {row['uuid']}\")\n",
    "        \n",
    "        # Display question\n",
    "        if 'question' in row:\n",
    "            print(f\"\\nOriginal Question:\\n{row['question']}\")\n",
    "        \n",
    "        # Display paraphrases if available\n",
    "        if 'paraphrase' in row:\n",
    "            para_val = row['paraphrase']\n",
    "            if not (isinstance(para_val, float) and pd.isna(para_val)):\n",
    "                print(f\"\\nParaphrase:\\n{para_val}\")\n",
    "        elif 'paraphrases' in row:\n",
    "            paraphrases = row['paraphrases']\n",
    "            if not (isinstance(paraphrases, float) and pd.isna(paraphrases)):\n",
    "                if isinstance(paraphrases, (list, tuple)):\n",
    "                    print(f\"\\nParaphrases:\")\n",
    "                    for i, para in enumerate(paraphrases, 1):\n",
    "                        print(f\"  {i}. {para}\")\n",
    "                else:\n",
    "                    print(f\"\\nParaphrases:\\n{paraphrases}\")\n",
    "        \n",
    "        # Display prompt (truncated)\n",
    "        if 'prompt' in row:\n",
    "            prompt_val = row['prompt']\n",
    "            if not (isinstance(prompt_val, float) and pd.isna(prompt_val)):\n",
    "                prompt_preview = str(prompt_val)[:300] + \"...\" if len(str(prompt_val)) > 300 else str(prompt_val)\n",
    "                print(f\"\\nPrompt (preview):\\n{prompt_preview}\")\n",
    "        \n",
    "        # Display generation\n",
    "        if 'generation' in row:\n",
    "            gen_val = row['generation']\n",
    "            if not (isinstance(gen_val, float) and pd.isna(gen_val)):\n",
    "                gen_preview = str(gen_val)[:200] + \"...\" if len(str(gen_val)) > 200 else str(gen_val)\n",
    "                print(f\"\\nGeneration:\\n{gen_preview}\")\n",
    "        \n",
    "        # Display prediction\n",
    "        if 'prediction' in row:\n",
    "            print(f\"\\nPrediction: {row['prediction']}\")\n",
    "        \n",
    "        # Display correct answers\n",
    "        if 'answers' in row:\n",
    "            print(f\"\\nCorrect Answers: {row['answers']}\")\n",
    "        \n",
    "        # Display lemmatized versions\n",
    "        if 'predict_lemma' in row:\n",
    "            pred_lemma = row['predict_lemma']\n",
    "            if not (isinstance(pred_lemma, float) and pd.isna(pred_lemma)):\n",
    "                print(f\"\\nPrediction (lemmatized): {pred_lemma}\")\n",
    "        \n",
    "        if 'answer_lemmas' in row:\n",
    "            ans_lemmas = row['answer_lemmas']\n",
    "            if not (isinstance(ans_lemmas, float) and pd.isna(ans_lemmas)):\n",
    "                print(f\"Answer Lemmas: {ans_lemmas}\")\n",
    "        \n",
    "        # Display correctness\n",
    "        if can_check_correctness and 'is_correct' in row:\n",
    "            status = \"‚úÖ CORRECT\" if row['is_correct'] else \"‚ùå INCORRECT\"\n",
    "            print(f\"\\nStatus: {status}\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 View Examples from a Specific Method\n",
    "\n",
    "Choose a method from the loaded results and generate detailed examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available methods\n",
    "print(\"Available methods for detailed analysis:\")\n",
    "for i, method in enumerate(loaded_results.keys(), 1):\n",
    "    print(f\"{i}. {method}\")\n",
    "\n",
    "# Select a method to analyze (change this)\n",
    "METHOD_TO_ANALYZE = list(loaded_results.keys())[0] if loaded_results else None\n",
    "\n",
    "if METHOD_TO_ANALYZE and METHOD_TO_ANALYZE in loaded_results:\n",
    "    # Generate examples\n",
    "    # Parameters:\n",
    "    #   num_examples: number of examples to show\n",
    "    #   show_correct: show correct predictions\n",
    "    #   show_incorrect: show incorrect predictions\n",
    "    generate_detailed_examples(\n",
    "        METHOD_TO_ANALYZE, \n",
    "        loaded_results[METHOD_TO_ANALYZE],\n",
    "        num_examples=5,\n",
    "        show_correct=True,\n",
    "        show_incorrect=True\n",
    "    )\n",
    "else:\n",
    "    print(\"No methods available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Compare Examples Across Methods\n",
    "\n",
    "View the same examples from different methods for side-by-side comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_examples_across_methods(methods, loaded_results, num_examples=3, uuid_filter=None):\n",
    "    \"\"\"\n",
    "    Compare the same examples across different methods.\n",
    "    \n",
    "    Args:\n",
    "        methods: List of method names to compare\n",
    "        loaded_results: Dictionary of loaded results\n",
    "        num_examples: Number of examples to compare\n",
    "        uuid_filter: Optional list of specific UUIDs to compare\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"CROSS-METHOD COMPARISON\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Get common UUIDs across all methods\n",
    "    common_uuids = None\n",
    "    for method in methods:\n",
    "        if method not in loaded_results:\n",
    "            continue\n",
    "        df = loaded_results[method]\n",
    "        if 'uuid' not in df.columns:\n",
    "            print(f\"‚ö†Ô∏è  Method {method} does not have UUID column\")\n",
    "            return\n",
    "        \n",
    "        uuids = set(df['uuid'].unique())\n",
    "        if common_uuids is None:\n",
    "            common_uuids = uuids\n",
    "        else:\n",
    "            common_uuids = common_uuids.intersection(uuids)\n",
    "    \n",
    "    if uuid_filter:\n",
    "        common_uuids = [u for u in uuid_filter if u in common_uuids]\n",
    "    else:\n",
    "        common_uuids = list(common_uuids)[:num_examples]\n",
    "    \n",
    "    for idx, uuid in enumerate(common_uuids, 1):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Example {idx}/{len(common_uuids)} - UUID: {uuid}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        for method in methods:\n",
    "            if method not in loaded_results:\n",
    "                continue\n",
    "            \n",
    "            df = loaded_results[method]\n",
    "            row = df[df['uuid'] == uuid].iloc[0]\n",
    "            \n",
    "            print(f\"\\n{'‚îÄ'*40}\")\n",
    "            print(f\"Method: {method}\")\n",
    "            print(f\"{'‚îÄ'*40}\\n\")\n",
    "            \n",
    "            if 'prediction' in row:\n",
    "                print(f\"Prediction: {row['prediction']}\")\n",
    "            \n",
    "            if 'predict_lemma' in row:\n",
    "                pred_lemma = row['predict_lemma']\n",
    "                if not (isinstance(pred_lemma, float) and pd.isna(pred_lemma)):\n",
    "                    print(f\"Prediction (lemmatized): {pred_lemma}\")\n",
    "            \n",
    "            # Show correctness if available\n",
    "            if 'predict_lemma' in row and 'answer_lemmas' in row:\n",
    "                try:\n",
    "                    answer_lemmas = row['answer_lemmas']\n",
    "                    if isinstance(answer_lemmas, list):\n",
    "                        answer_lemmas = [list(x) if not isinstance(x, list) else x for x in answer_lemmas]\n",
    "                    is_correct = partial_match(row['predict_lemma'], answer_lemmas)\n",
    "                    status = \"‚úÖ CORRECT\" if is_correct else \"‚ùå INCORRECT\"\n",
    "                    print(f\"Status: {status}\")\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        # Show correct answer once\n",
    "        first_method = methods[0]\n",
    "        if first_method in loaded_results:\n",
    "            df = loaded_results[first_method]\n",
    "            row = df[df['uuid'] == uuid].iloc[0]\n",
    "            if 'answers' in row:\n",
    "                print(f\"\\n{'‚îÄ'*40}\")\n",
    "                print(f\"Correct Answers: {row['answers']}\")\n",
    "                print(f\"{'‚îÄ'*40}\")\n",
    "\n",
    "# Example usage: Compare baseline with flex_attention\n",
    "if len(loaded_results) >= 2:\n",
    "    methods_to_compare = list(loaded_results.keys())[:2]  # Change this to compare specific methods\n",
    "    compare_examples_across_methods(methods_to_compare, loaded_results, num_examples=3)\n",
    "else:\n",
    "    print(\"Need at least 2 methods to compare\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Detailed Analysis to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_detailed_analysis(method_name, df, output_dir=\".\"):\n",
    "    \"\"\"\n",
    "    Export detailed analysis to CSV for external review.\n",
    "    \n",
    "    Args:\n",
    "        method_name: Name of the method\n",
    "        df: DataFrame with results\n",
    "        output_dir: Directory to save the CSV file\n",
    "    \"\"\"\n",
    "    # Create analysis DataFrame\n",
    "    analysis_data = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        item = {\n",
    "            'Index': idx,\n",
    "            'UUID': row.get('uuid', 'N/A'),\n",
    "            'Question': row.get('question', 'N/A'),\n",
    "            'Prediction': row.get('prediction', 'N/A'),\n",
    "            'Correct_Answers': str(row.get('answers', 'N/A')),\n",
    "        }\n",
    "        \n",
    "        # Add lemmatized versions\n",
    "        if 'predict_lemma' in df.columns:\n",
    "            pred_lemma = row.get('predict_lemma')\n",
    "            if not (isinstance(pred_lemma, float) and pd.isna(pred_lemma)):\n",
    "                item['Prediction_Lemma'] = str(pred_lemma)\n",
    "        \n",
    "        if 'answer_lemmas' in df.columns:\n",
    "            ans_lemmas = row.get('answer_lemmas')\n",
    "            if not (isinstance(ans_lemmas, float) and pd.isna(ans_lemmas)):\n",
    "                item['Answer_Lemmas'] = str(ans_lemmas)\n",
    "        \n",
    "        # Check correctness\n",
    "        if 'predict_lemma' in df.columns and 'answer_lemmas' in df.columns:\n",
    "            try:\n",
    "                answer_lemmas = row['answer_lemmas']\n",
    "                if isinstance(answer_lemmas, list):\n",
    "                    answer_lemmas = [list(x) if not isinstance(x, list) else x for x in answer_lemmas]\n",
    "                is_correct = partial_match(row['predict_lemma'], answer_lemmas)\n",
    "                item['Is_Correct'] = is_correct\n",
    "            except:\n",
    "                item['Is_Correct'] = 'N/A'\n",
    "        \n",
    "        analysis_data.append(item)\n",
    "    \n",
    "    analysis_df = pd.DataFrame(analysis_data)\n",
    "    \n",
    "    # Export to CSV\n",
    "    output_file = os.path.join(output_dir, f\"{method_name}_detailed_analysis.csv\")\n",
    "    analysis_df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"‚úÖ Detailed analysis exported to: {output_file}\")\n",
    "    print(f\"   Total entries: {len(analysis_df)}\")\n",
    "    \n",
    "    return analysis_df\n",
    "\n",
    "# Example: Export detailed analysis for a specific method\n",
    "if loaded_results:\n",
    "    method_to_export = list(loaded_results.keys())[0]  # Change this\n",
    "    export_detailed_analysis(method_to_export, loaded_results[method_to_export], output_dir=DATASET_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary_statistics():\n",
    "    \"\"\"\n",
    "    Print comprehensive summary statistics.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"SUMMARY STATISTICS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    print(f\"Dataset: {DATASET_NAME}\")\n",
    "    print(f\"Model: {MODEL_NAME}\")\n",
    "    print(f\"Dataset Root: {DATASET_ROOT}\\n\")\n",
    "    \n",
    "    print(f\"Total result files found: {len(all_results)}\")\n",
    "    print(f\"Successfully loaded: {len(loaded_results)}\")\n",
    "    print(f\"With accuracy metrics: {len(accuracies)}\\n\")\n",
    "    \n",
    "    if accuracies:\n",
    "        best_method = max(accuracies.items(), key=lambda x: x[1])\n",
    "        worst_method = min(accuracies.items(), key=lambda x: x[1])\n",
    "        avg_accuracy = sum(accuracies.values()) / len(accuracies)\n",
    "        \n",
    "        print(f\"üèÜ Best Method: {best_method[0]} ({best_method[1]:.3f})\")\n",
    "        print(f\"üìâ Worst Method: {worst_method[0]} ({worst_method[1]:.3f})\")\n",
    "        print(f\"üìä Average Accuracy: {avg_accuracy:.3f}\")\n",
    "        print(f\"üìà Range: {worst_method[1]:.3f} - {best_method[1]:.3f}\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\\n\")\n",
    "\n",
    "print_summary_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Custom Analysis Functions\n",
    "\n",
    "Add your own custom analysis functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your custom analysis code here\n",
    "# Example: Analyze error patterns, visualize results, etc.\n",
    "\n",
    "def analyze_error_patterns(method_name, df):\n",
    "    \"\"\"\n",
    "    Analyze patterns in incorrect predictions.\n",
    "    \n",
    "    Args:\n",
    "        method_name: Name of the method\n",
    "        df: DataFrame with results\n",
    "    \"\"\"\n",
    "    if 'predict_lemma' not in df.columns or 'answer_lemmas' not in df.columns:\n",
    "        print(\"Lemmatization required for error analysis\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ERROR PATTERN ANALYSIS: {method_name}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Calculate correctness\n",
    "    df_copy = df.copy()\n",
    "    df_copy['answer_lemmas'] = df_copy['answer_lemmas'].apply(\n",
    "        lambda xs: [list(x) if not isinstance(x, list) else x for x in xs] \n",
    "        if isinstance(xs, list) else xs\n",
    "    )\n",
    "    \n",
    "    correctness = []\n",
    "    for pred, ans in zip(df_copy['predict_lemma'], df_copy['answer_lemmas']):\n",
    "        try:\n",
    "            is_correct = partial_match(pred, ans)\n",
    "            correctness.append(is_correct)\n",
    "        except:\n",
    "            correctness.append(None)\n",
    "    \n",
    "    df_copy['is_correct'] = correctness\n",
    "    \n",
    "    # Statistics\n",
    "    total = len(df_copy)\n",
    "    correct = sum(1 for c in correctness if c is True)\n",
    "    incorrect = sum(1 for c in correctness if c is False)\n",
    "    unknown = sum(1 for c in correctness if c is None)\n",
    "    \n",
    "    print(f\"Total samples: {total}\")\n",
    "    print(f\"Correct: {correct} ({correct/total*100:.1f}%)\")\n",
    "    print(f\"Incorrect: {incorrect} ({incorrect/total*100:.1f}%)\")\n",
    "    if unknown > 0:\n",
    "        print(f\"Unknown: {unknown} ({unknown/total*100:.1f}%)\")\n",
    "    \n",
    "    # Show some incorrect examples\n",
    "    incorrect_df = df_copy[df_copy['is_correct'] == False].head(5)\n",
    "    \n",
    "    if len(incorrect_df) > 0:\n",
    "        print(f\"\\nüîç Sample Incorrect Predictions:\\n\")\n",
    "        for idx, (i, row) in enumerate(incorrect_df.iterrows(), 1):\n",
    "            print(f\"{idx}. Predicted: {row['prediction']} | Correct: {row['answers']}\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\\n\")\n",
    "\n",
    "# Example usage\n",
    "if loaded_results:\n",
    "    method_to_analyze = list(loaded_results.keys())[0]\n",
    "    analyze_error_patterns(method_to_analyze, loaded_results[method_to_analyze])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flexattention",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
