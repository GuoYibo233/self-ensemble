#!/usr/bin/env python3
"""
æµ‹è¯•FlexAttention ensembleç”Ÿæˆçš„ç®€å•è„šæœ¬
"""

import sys
import os
sys.path.append('/home/y-guo/self-ensemble/self-ensemble')

from flex_attention_generate import flex_attention_generation
from dataset import WebQADataset
from constants import MODEL_PATHs
from transformers import AutoTokenizer, AutoModelForCausalLM

def test_flex_attention():
    print("ğŸ§ª Testing FlexAttention Ensemble Generation")
    print("=" * 50)
    
    # è®¾ç½®æ¨¡å‹
    model_name = "llama3.2_3b_it"
    model_path = MODEL_PATHs.get(model_name)
    
    print(f"Loading model: {model_name}")
    print(f"Model path: {model_path}")
    
    # åŠ è½½æ•°æ®é›†
    print("Loading WebQA dataset...")
    dataset = WebQADataset(model_name=model_name)
    dataloader = dataset.get_dataloader(batch_size=1, shuffle=False)
    
    # è·å–ç¬¬ä¸€ä¸ªæ ·æœ¬
    for uuids, answers, all_paraphrases in dataloader:
        print(f"âœ… Got sample UUID: {uuids[0]}")
        print(f"âœ… Answers: {answers[0]}")
        print(f"âœ… Number of paraphrases: {len(all_paraphrases)}")
        
        # é€‰æ‹©å‰5ä¸ªparaphrases
        selected_paraphrases = all_paraphrases[:5]
        paraphrases_for_question = [para[0] for para in selected_paraphrases]
        
        print("\nğŸ“ Selected paraphrases:")
        for i, para in enumerate(paraphrases_for_question):
            print(f"   {i+1}. {para}")
            
        # æ„å»ºprompts
        few_shot_context = dataset.get_few_shot_examples()
        prompts = []
        for paraphrase in paraphrases_for_question:
            prompt = dataset.construct_prompts(few_shot_context, [paraphrase])
            prompts.append(prompt[0])
            
        print(f"\nğŸ”¥ Testing FlexAttention generation with {len(prompts)} prompts...")
        
        # è¿™é‡Œä¼šè‡ªåŠ¨åŠ è½½æ¨¡å‹å’Œtokenizer
        global model, tokenizer
        print("Loading tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        tokenizer.pad_token = tokenizer.eos_token
        
        print("Loading model...")
        model = AutoModelForCausalLM.from_pretrained(
            model_path, 
            device_map="auto", 
            torch_dtype="auto"
        )
        
        # è¿è¡ŒFlexAttentionç”Ÿæˆ
        result = flex_attention_generation(prompts, tokenizer, model, max_new_tokens=10)
        
        print(f"\nâœ… Generated result: {result}")
        print("\nğŸ‰ Test completed successfully!")
        break
        
if __name__ == "__main__":
    test_flex_attention()