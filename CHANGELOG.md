# Changelog - Mask Matrix and FlexAttention Improvements

æœ¬æ–‡æ¡£è®°å½•æ¯æ¬¡æäº¤çš„è¯¦ç»†å˜æ›´å†…å®¹ / This document tracks detailed changes for each commit

---

## Latest Update - Documentation Consolidation
**æ›´æ–°æ—¶é—´ / Update Time**: 2025-10-13
**æäº¤ä¿¡æ¯ / Commit**: Consolidate FlexAttention debug documentation and update changelog

### ğŸ“š æ–‡æ¡£æ•´åˆ / Documentation Consolidation
**ç›®çš„**: æ¶ˆé™¤å†—ä½™ï¼Œåˆ›å»ºå•ä¸€æƒå¨æ–‡æ¡£æ¥æº

#### å®Œæˆçš„æ•´åˆå·¥ä½œ
1. **åˆå¹¶è°ƒè¯•æ–‡æ¡£** - å°† `CHANGELOG_FLEXATTENTION_DEBUG.md` çš„è¯¦ç»†æŠ€æœ¯å†…å®¹æ•´åˆåˆ°æœ¬æ–‡ä»¶
2. **é›†æˆä¿®å¤æ€»ç»“** - å°† `FLEXATTENTION_FIX_SUMMARY.md` çš„æ ¸å¿ƒè¦ç‚¹æ•´åˆåˆ°ç›¸åº”ç« èŠ‚
3. **ç®€åŒ–å¯¼èˆª** - æ›´æ–° `DEBUG_INDEX.md` ä¸ºæ¸…æ™°çš„æ–‡æ¡£å¯¼èˆªé¡µé¢
4. **æ›´æ–°ä¸»æ–‡æ¡£** - åœ¨ `README.md` ä¸­æ·»åŠ æŒ‡å‘ç»Ÿä¸€æ–‡æ¡£çš„é“¾æ¥

#### æ–‡æ¡£ç»“æ„ä¼˜åŒ–
```
ä¹‹å‰ (Before):
â”œâ”€â”€ CHANGELOG.md (éƒ¨åˆ†å†å²)
â”œâ”€â”€ CHANGELOG_FLEXATTENTION_DEBUG.md (è¯¦ç»†è°ƒè¯•)
â”œâ”€â”€ FLEXATTENTION_FIX_SUMMARY.md (ä¿®å¤æ€»ç»“)
â””â”€â”€ DEBUG_INDEX.md (ç´¢å¼•)

ç°åœ¨ (After):
â”œâ”€â”€ CHANGELOG.md (å®Œæ•´å†å²ï¼ŒåŒ…å«æ‰€æœ‰è°ƒè¯•ç»†èŠ‚) âœ… å•ä¸€æ¥æº
â”œâ”€â”€ DEBUG_INDEX.md (ç®€åŒ–å¯¼èˆª) âœ… æŒ‡å‘CHANGELOG
â””â”€â”€ README.md (æ›´æ–°é“¾æ¥) âœ… æŒ‡å‘CHANGELOG
```

#### å¥½å¤„
- âœ… ä¿¡æ¯ä¸åˆ†æ•£ - æ‰€æœ‰å˜æ›´å†å²åœ¨ä¸€ä¸ªæ–‡ä»¶ä¸­
- âœ… æ˜“äºæŸ¥æ‰¾ - ä¸éœ€è¦åœ¨å¤šä¸ªæ–‡ä»¶é—´è·³è½¬
- âœ… æ˜“äºç»´æŠ¤ - åªéœ€æ›´æ–°ä¸€ä¸ªæƒå¨æ–‡æ¡£
- âœ… é¿å…ä¸ä¸€è‡´ - æ¶ˆé™¤å¤šå¤„ç»´æŠ¤å¯¼è‡´çš„ä¿¡æ¯å·®å¼‚

### ğŸ“ æœ¬æ¬¡æäº¤å˜æ›´ / Changes in This Commit
```
Modified:
â”œâ”€â”€ CHANGELOG.md (æ·»åŠ æ•´åˆè®°å½•å’Œå®Œæ•´FlexAttentionè°ƒè¯•å†…å®¹)
â”œâ”€â”€ DEBUG_INDEX.md (æ›´æ–°ä¸ºå¯¼èˆªé¡µ)
â””â”€â”€ README.md (æ·»åŠ æ–‡æ¡£é“¾æ¥)

Removed (å†…å®¹å·²æ•´åˆ):
â”œâ”€â”€ CHANGELOG_FLEXATTENTION_DEBUG.md
â””â”€â”€ FLEXATTENTION_FIX_SUMMARY.md
```

---

## FlexAttention Implementation Debug and Fix Session
**è°ƒè¯•æ—¶é—´ / Debug Session**: 2025-10-13 to 2025-10-14
**åŸå§‹æäº¤ / Original Commit**: 22dfe1f (tried to fix generate attention)

### ğŸ› é‡å¤§ä¿®å¤ / Critical Fixes

#### FlexAttentionä¸LLaMA 3.2 GQAæ¶æ„å…¼å®¹æ€§
- **é—®é¢˜**: FlexAttentionWrapperæ— æ³•æ­£ç¡®å¤„ç†LLaMA 3.2çš„Grouped Query Attentionæ¶æ„
- **å‘ç°**: LLaMA 3.2ä½¿ç”¨24ä¸ªQueryå¤´ä½†åªæœ‰8ä¸ªKey-Valueå¤´ï¼ˆ3:1æ¯”ä¾‹ï¼‰
- **ä¿®å¤**: æ·»åŠ GQAå¼ é‡æ‰©å±•é€»è¾‘ï¼Œæ­£ç¡®å¤„ç†KVå¤´åˆ°Qå¤´çš„æ˜ å°„

#### PyTorch FlexAttention vmapç¼–è¯‘é—®é¢˜  
- **é—®é¢˜**: `mask_mod`å‡½æ•°ä¸­çš„å¤æ‚æ§åˆ¶æµå¯¼è‡´vmapç¼–è¯‘å¤±è´¥
- **é”™è¯¯**: `RuntimeError: vmap: data-dependent control flow not supported`  
- **ä¿®å¤**: ç®€åŒ–maskå‡½æ•°ï¼Œç§»é™¤æ•°æ®ä¾èµ–çš„å¾ªç¯å’Œæ¡ä»¶åˆ†æ”¯

#### Transformers 4.55.2æ¥å£å˜æ›´
- **é—®é¢˜**: æ–¹æ³•ç­¾åå’Œè¿”å›å€¼æ ¼å¼ä¸åŒ¹é…
- **å‘ç°**: `LlamaAttention.forward`ç°åœ¨éœ€è¦`position_embeddings`å‚æ•°
- **ä¿®å¤**: æ›´æ–°å‚æ•°å¤„ç†å’Œè¿”å›å€¼æ ¼å¼

### ğŸ“‹ ä¿®æ”¹çš„æ–‡ä»¶ / Modified Files
```
flex_attention_generate.py:
â”œâ”€â”€ FlexAttentionWrapper.create_patched_forward() - å®Œå…¨é‡æ„  
â”œâ”€â”€ create_flex_attention_mask() - ç®€åŒ–å®ç°
â””â”€â”€ æ·»åŠ GQAæ”¯æŒå’Œé”™è¯¯å¤„ç†

æ–°å¢æ–‡ä»¶:
â””â”€â”€ CHANGELOG_FLEXATTENTION_DEBUG.md - è¯¦ç»†è°ƒè¯•æ—¥å¿—
```

### ğŸ”§ æŠ€æœ¯ç»†èŠ‚ / Technical Details

#### å…³é”®å‘ç° - LLaMA 3.2 GQAæ¶æ„
```python
# LLaMA 3.2 3B Instructæ¶æ„ç‰¹ç‚¹
num_attention_heads = 24      # Query heads  
num_key_value_heads = 8       # Key-Value heads  
head_dim = 128               # æ¯ä¸ªå¤´çš„ç»´åº¦
ratio = 24 // 8 = 3          # Q:KV = 3:1

# å¿…éœ€çš„å¼ é‡æ‰©å±•ä»£ç 
if num_key_value_heads != num_heads:
    key_states = key_states.repeat_interleave(3, dim=1) 
    value_states = value_states.repeat_interleave(3, dim=1)
```

#### FlexAttentioné™åˆ¶
- âŒ ä¸æ”¯æŒæ•°æ®ä¾èµ–çš„æ§åˆ¶æµï¼ˆå¾ªç¯ã€å¤æ‚æ¡ä»¶ï¼‰
- âŒ mask_modå‡½æ•°å¿…é¡»å¯é™æ€ç¼–è¯‘
- âœ… åŸºæœ¬çš„å¼ é‡è¿ç®—å’Œç®€å•æ¯”è¾ƒå¯ä»¥ä½¿ç”¨

### âš ï¸ å½“å‰çŠ¶æ€ / Current Status
- âœ… **å·²ä¿®å¤**: FlexAttentionåŸºæœ¬åŠŸèƒ½å¯æ­£å¸¸è¿è¡Œ
- âš ï¸ **é™åˆ¶**: å¤æ‚çš„segment isolation maskingæš‚æ—¶ç®€åŒ–
- ğŸ”„ **å¾…ç»­**: åŸå§‹è¯·æ±‚çš„å¯è§†åŒ–æ”¹è¿›å°šæœªå®Œæˆ

### ğŸ“Š è¯¦ç»†è°ƒè¯•è¿‡ç¨‹ / Detailed Debug Process

#### æ”¶é›†åˆ°çš„ç¯å¢ƒä¿¡æ¯
```bash
Python: 3.10.x (condaç¯å¢ƒ: flexattention)
PyTorch: 2.5.0 nightly (æ”¯æŒFlexAttention)
Transformers: 4.55.2
æ¨¡å‹: meta-llama/Llama-3.2-3B-Instruct

# LLaMA 3.2æ¶æ„ç‰¹å¾
num_attention_heads: 24 (Query heads)
num_key_value_heads: 8 (Key-Value heads - GQA)
head_dim: 128
hidden_size: 24 * 128 = 3072
```

#### é‡åˆ°çš„7ç§ä¸»è¦é”™è¯¯

**é”™è¯¯1**: æ–¹æ³•ç»‘å®šé—®é¢˜
```python
# é”™è¯¯ä¿¡æ¯
FlexAttentionWrapper.create_patched_forward.<locals>.patched_forward() 
got multiple values for argument 'hidden_states'

# æ ¹å› : patched_forwardç¬¬ä¸€ä¸ªå‚æ•°è®¾è®¡é”™è¯¯
# ä¿®å¤: ç›´æ¥æ¥æ”¶forwardçš„æ‰€æœ‰å‚æ•°ï¼Œç§»é™¤self_attnå‚æ•°
```

**é”™è¯¯2**: å±æ€§è®¿é—®è·¯å¾„å˜æ›´
```python
# é”™è¯¯
AttributeError: 'LlamaAttention' object has no attribute 'num_heads'

# ä¿®å¤
- æ—§: self_attn.num_heads
+ æ–°: self_attn.config.num_attention_heads
```

**é”™è¯¯3**: GQAå¼ é‡ç»´åº¦ä¸åŒ¹é…
```python
# é”™è¯¯
RuntimeError: shape '[1, 613, 24, 128]' is invalid for input of size 631808

# æ ¹å› : KV heads(8) != Q heads(24)ï¼Œéœ€è¦æ‰©å±•
# ä¿®å¤: æ·»åŠ repeat_interleaveé€»è¾‘
```

**é”™è¯¯4**: vmapç¼–è¯‘å¤±è´¥
```python
# é”™è¯¯
RuntimeError: vmap: data-dependent control flow not supported

# æ ¹å› : mask_modå‡½æ•°åŒ…å«å¤æ‚å¾ªç¯å’Œæ¡ä»¶åˆ¤æ–­
# ä¿®å¤: ç®€åŒ–ä¸ºåŸºæœ¬å› æœmasking: q_idx >= kv_idx
```

**é”™è¯¯5**: position_embeddingså‚æ•°ç¼ºå¤±
```python
# é”™è¯¯
TypeError: LlamaAttention.forward() missing 1 required positional argument: 
'position_embeddings'

# æ ¹å› : Transformers 4.55.2æ–°å¢å¿…éœ€å‚æ•°
# ä¿®å¤: ä»kwargsä¸­è·å–å¹¶ä¼ é€’position_embeddings
```

**é”™è¯¯6**: è¿”å›å€¼æ ¼å¼ä¸åŒ¹é…
```python
# é”™è¯¯
ValueError: too many values to unpack (expected 2)

# æ ¹å› : è¿”å›å€¼æ•°é‡å’Œæ ¼å¼ä¸åŸforwardä¸ä¸€è‡´  
# ä¿®å¤: ä¸¥æ ¼åŒ¹é…è¿”å›å€¼æ ¼å¼
```

**é”™è¯¯7**: å¼ é‡å½¢çŠ¶é”™è¯¯ä¼ æ’­
```python
# ç°è±¡: å¤šä¸ªä¸‹æ¸¸é”™è¯¯
# æ ¹å› : ä¸Šæ¸¸GQAæ‰©å±•ä¸æ­£ç¡®å¯¼è‡´shapeä¸€è·¯ä¼ é€’é”™è¯¯
# ä¿®å¤: æ­£ç¡®å®ç°KVå¤´æ‰©å±•ï¼Œç¡®ä¿tensor shapeä¸€è‡´æ€§
```

#### å…³é”®ä»£ç ä¿®æ”¹

**ä¿®æ”¹1: GQAæ”¯æŒ**
```python
# åœ¨patched_forwardä¸­æ·»åŠ 
num_heads = self_attn.config.num_attention_heads
num_key_value_heads = self_attn.config.num_key_value_heads

if num_key_value_heads != num_heads:
    repeat_factor = num_heads // num_key_value_heads
    key_states = key_states.repeat_interleave(repeat_factor, dim=1)
    value_states = value_states.repeat_interleave(repeat_factor, dim=1)
```

**ä¿®æ”¹2: ç®€åŒ–maskå‡½æ•°**
```python
# æ—§ç‰ˆæœ¬ (å¤æ‚ï¼Œå¯¼è‡´vmapå¤±è´¥)
def mask_mod(b, h, q_idx, kv_idx):
    for seg in segments:
        if seg['start'] <= q_idx < seg['end']:
            if seg['start'] <= kv_idx < seg['end']:
                return True
    return q_idx >= kv_idx  # æ•°æ®ä¾èµ–çš„æ§åˆ¶æµ

# æ–°ç‰ˆæœ¬ (ç®€åŒ–ï¼Œvmapå…¼å®¹)
def mask_mod(b, h, q_idx, kv_idx):
    return q_idx >= kv_idx  # çº¯tensoræ¯”è¾ƒ
```

**ä¿®æ”¹3: å‚æ•°å’Œè¿”å›å€¼å¤„ç†**
```python
def patched_forward(
    hidden_states,
    position_embeddings,  # æ–°å¢
    attention_mask=None,
    position_ids=None,
    past_key_value=None,
    output_attentions=False,
    use_cache=False,
    cache_position=None,
    **kwargs
):
    # è§£åŒ…position_embeddings
    cos, sin = position_embeddings
    
    # ... FlexAttentioné€»è¾‘ ...
    
    # è¿”å›ä¸åŸforwardå®Œå…¨ä¸€è‡´çš„æ ¼å¼
    if output_attentions:
        return attn_output, attn_weights, past_key_value
    return attn_output, None, past_key_value
```

#### å­¦ä¹ åˆ°çš„ç»éªŒ

1. **GQAæ¶æ„è¦æ±‚**: LLaMA 3.2ä½¿ç”¨GQAï¼Œå¿…é¡»æ­£ç¡®æ‰©å±•KV headsåˆ°Q headsæ•°é‡
2. **FlexAttentioné™åˆ¶**: vmapç¼–è¯‘å™¨ä¸æ”¯æŒæ•°æ®ä¾èµ–çš„æ§åˆ¶æµï¼Œmaskå‡½æ•°å¿…é¡»ç®€å•
3. **APIå…¼å®¹æ€§**: Transformersç‰ˆæœ¬å‡çº§å¯èƒ½æ”¹å˜æ ¸å¿ƒæ¥å£ï¼Œéœ€è¦é€‚é…
4. **é”™è¯¯ä¼ æ’­**: ä¸Šæ¸¸tensor shapeé”™è¯¯ä¼šå¯¼è‡´ä¸€ç³»åˆ—ä¸‹æ¸¸é”™è¯¯ï¼Œéœ€è¿½æº¯æ ¹å› 
5. **è°ƒè¯•ç­–ç•¥**: ä»æœ€åº•å±‚é”™è¯¯å¼€å§‹ä¿®å¤ï¼Œé€å±‚å‘ä¸Šè§£å†³

### ğŸ“ˆ ä¿®æ”¹ç»Ÿè®¡ / Modification Statistics
```
Files modified: 1 (flex_attention_generate.py)
Functions rewritten: 2 (patched_forward, mask_mod)
Lines added: ~40 (GQA support + error handling + API updates)
Lines removed: ~20 (complex masking logic)
Net change: +20 lines
```

### âœ… éªŒè¯ç»“æœ / Verification Results
- âœ… åŸºç¡€FlexAttentionè°ƒç”¨æˆåŠŸ
- âœ… LLaMA 3.2 GQAæ¨¡å‹å…¼å®¹
- âœ… é”™è¯¯å¤„ç†å’Œé™çº§æœºåˆ¶æ­£å¸¸
- âš ï¸ å¤æ‚segment isolationæš‚æ—¶ç®€åŒ–ï¼ˆå› vmapé™åˆ¶ï¼‰

### ğŸ”— ç›¸å…³èµ„æº / Related Resources
- PyTorch FlexAttentionæ–‡æ¡£: https://pytorch.org/docs/stable/nn.attention.flex_attention.html
- LLaMA 3.2 æ¨¡å‹å¡: https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct
- Transformers 4.55.2å‘å¸ƒè¯´æ˜: https://github.com/huggingface/transformers/releases/tag/v4.55.2

---

## Commit 16164ef - Update documentation for max_samples and analysis tools
**æäº¤æ—¶é—´ / Date**: 2025-10-13

### æ–‡ä»¶å˜æ›´ / Files Changed
- âœ… `README.md` - æ·»åŠ æ–°åŠŸèƒ½ä½¿ç”¨è¯´æ˜
- âœ… `docs/QUICK_REFERENCE.md` - æ›´æ–°APIå‚è€ƒ

### å…·ä½“æ”¹åŠ¨ / Specific Changes

#### README.md
**æ–°å¢å†…å®¹**:
- æ·»åŠ  `--max_samples` å‚æ•°ä½¿ç”¨ç¤ºä¾‹
- æ·»åŠ Analysiså·¥å…·ä½¿ç”¨è¯´æ˜ï¼ˆå‘½ä»¤è¡Œå’ŒJupyterï¼‰
- æ›´æ–°æ–‡æ¡£ç´¢å¼•ï¼ŒåŒ…å« `FLEXATTENTION_USAGE.md` å’Œ `IMPROVEMENTS_SUMMARY.md`
- æ›´æ–°ä»“åº“ç»“æ„å›¾ï¼Œæ·»åŠ  `analysis/` ç›®å½•
- æ›´æ–°æœ€åä¿®æ”¹æ—¥æœŸä¸º 2025-10-13

**ç¤ºä¾‹ä»£ç **:
```bash
# é™åˆ¶ç”Ÿæˆæ ·æœ¬æ•°
python3 flex_attention_generate.py --max_samples 100

# åˆ†æç»“æœ
python3 analysis/analyze_flexattention.py --dataset webqa --model llama3.2_3b_it
```

#### docs/QUICK_REFERENCE.md
**æ–°å¢å†…å®¹**:
- æ·»åŠ  `--max_samples` å‚æ•°è¯´æ˜
- æ·»åŠ analysiså‘½ä»¤ç¤ºä¾‹
- æ·»åŠ æŒ‡å‘ `FLEXATTENTION_USAGE.md` çš„é“¾æ¥

### å½±å“èŒƒå›´ / Impact
- ğŸŸ¢ **æ–‡æ¡£æ›´æ–°** - æ‰€æœ‰æ–‡æ¡£ä¸ä»£ç åŒæ­¥
- ğŸŸ¢ **å‘åå…¼å®¹** - ä¸å½±å“ç°æœ‰åŠŸèƒ½

---

## Commit 98cb294 - Add max_samples parameter and FlexAttention analysis tools
**æäº¤æ—¶é—´ / Date**: 2025-10-13

### æ–‡ä»¶å˜æ›´ / Files Changed
- âœ… `flex_attention_generate.py` - æ·»åŠ  `--max_samples` å‚æ•°
- âœ… `analysis/analyze_flexattention.py` - æ–°æ–‡ä»¶
- âœ… `analysis/flexattention_analysis.ipynb` - æ–°æ–‡ä»¶
- âœ… `FLEXATTENTION_USAGE.md` - æ–°æ–‡ä»¶

### å…·ä½“æ”¹åŠ¨ / Specific Changes

#### flex_attention_generate.py
**æ–°å¢åŠŸèƒ½**:
1. æ·»åŠ å‘½ä»¤è¡Œå‚æ•° `--max_samples`ï¼ˆç¬¬429-432è¡Œï¼‰
2. æ·»åŠ æ ·æœ¬è®¡æ•°é€»è¾‘ï¼ˆç¬¬512è¡Œï¼‰
3. æ·»åŠ è¾¾åˆ°é™åˆ¶æ—¶çš„åœæ­¢é€»è¾‘ï¼ˆç¬¬560-563è¡Œï¼‰

**ä»£ç å˜æ›´**:
```python
# æ–°å¢å‚æ•°
parser.add_argument(
    "--max_samples", type=int, default=None,
    help="Maximum number of samples to generate (default: None, process all)"
)

# æ–°å¢é™åˆ¶æ£€æŸ¥
sample_count += len(uuids)
if args.max_samples and sample_count >= args.max_samples:
    print(f"Reached max_samples limit ({args.max_samples}), stopping generation")
    break
```

#### analysis/analyze_flexattention.py (æ–°æ–‡ä»¶)
**åŠŸèƒ½**: å‘½ä»¤è¡Œåˆ†æå·¥å…·ï¼ˆ207è¡Œä»£ç ï¼‰

**ä¸»è¦ç‰¹æ€§**:
- è®¡ç®—FlexAttentionå‡†ç¡®ç‡
- ä¸ä¼ ç»Ÿensembleæ–¹æ³•å¯¹æ¯”ï¼ˆavg, max, weighted_avg, weighted_maxï¼‰
- æ˜¾ç¤ºæ ·æœ¬ç”Ÿæˆç»“æœ
- åˆ†æä¸åŒparaphraseæ•°é‡çš„å½±å“

**ä½¿ç”¨æ–¹æ³•**:
```bash
python analysis/analyze_flexattention.py --dataset myriadlama --model qwen2.5_7b_it
python analysis/analyze_flexattention.py --dataset myriadlama --model qwen2.5_7b_it --compare_all
```

#### analysis/flexattention_analysis.ipynb (æ–°æ–‡ä»¶)
**åŠŸèƒ½**: äº¤äº’å¼Jupyteråˆ†ænotebookï¼ˆ414è¡Œä»£ç ï¼‰

**ä¸»è¦åŠŸèƒ½**:
- æ•°æ®åŠ è½½å’Œæ¢ç´¢
- å¯è§†åŒ–å¯¹æ¯”ï¼ˆæ¡å½¢å›¾ã€æŠ˜çº¿å›¾ï¼‰
- é”™è¯¯åˆ†æ
- ä¸ä¼ ç»Ÿæ–¹æ³•çš„æ€§èƒ½å¯¹æ¯”

#### FLEXATTENTION_USAGE.md (æ–°æ–‡ä»¶)
**åŠŸèƒ½**: å®Œæ•´ä½¿ç”¨æŒ‡å—ï¼ˆ252è¡Œæ–‡æ¡£ï¼‰

**åŒ…å«å†…å®¹**:
- å®Œæ•´å·¥ä½œæµç¤ºä¾‹
- å‚æ•°è¯´æ˜
- æœ€ä½³å®è·µ
- æ•…éšœæ’é™¤æŒ‡å—

### å½±å“èŒƒå›´ / Impact
- ğŸŸ¢ **æ–°åŠŸèƒ½** - å¯ä»¥é™åˆ¶ç”Ÿæˆæ ·æœ¬æ•°é‡
- ğŸŸ¢ **æ–°å·¥å…·** - å®Œæ•´çš„åˆ†æå·¥å…·é“¾
- ğŸŸ¢ **å‘åå…¼å®¹** - `--max_samples` æ˜¯å¯é€‰å‚æ•°

---

## Commit b435757 - Fix separator display in segment output
**æäº¤æ—¶é—´ / Date**: 2025-10-13

### æ–‡ä»¶å˜æ›´ / Files Changed
- âœ… `tools/debug_flexattention.py` - ä¿®å¤separatoræ˜¾ç¤º
- âœ… `test_separator_fix.py` - æ–°æ–‡ä»¶

### å…·ä½“æ”¹åŠ¨ / Specific Changes

#### tools/debug_flexattention.py
**é—®é¢˜**: "Full Sequence with Segment Markers"è¾“å‡ºä¸­ï¼Œ[SEP]è¢«segmentè¾¹ç•Œåˆ‡å‰²

**ä¿®å¤**:
- æ¯ä¸ªsegmentç°åœ¨åŒ…å«å…¶åçš„separator tokens
- é€šè¿‡æ‰©å±•èŒƒå›´åˆ°ä¸‹ä¸€ä¸ªsegmentçš„startä½ç½®å®ç°

**ä»£ç é€»è¾‘**:
```python
# ä¹‹å‰: åªæ˜¾ç¤º segment.start åˆ° segment.end
# ç°åœ¨: æ˜¾ç¤º segment.start åˆ° next_segment.startï¼ˆåŒ…å«separatorï¼‰
```

#### test_separator_fix.py (æ–°æ–‡ä»¶)
**åŠŸèƒ½**: éªŒè¯separatorä¿®å¤çš„æµ‹è¯•è„šæœ¬

### å½±å“èŒƒå›´ / Impact
- ğŸŸ¢ **Bugä¿®å¤** - [SEP]ç°åœ¨å®Œæ•´æ˜¾ç¤º
- ğŸŸ¢ **è°ƒè¯•æ”¹è¿›** - è¾“å‡ºæ›´æ¸…æ™°æ˜“è¯»

---

## Commit 20d2b67 - Add comprehensive README for all changes
**æäº¤æ—¶é—´ / Date**: 2025-10-13

### æ–‡ä»¶å˜æ›´ / Files Changed
- âœ… `CHANGES_README.md` - æ–°æ–‡ä»¶

### å…·ä½“æ”¹åŠ¨ / Specific Changes

#### CHANGES_README.md (æ–°æ–‡ä»¶)
**åŠŸèƒ½**: å¿«é€Ÿå…¥é—¨æŒ‡å—ï¼ˆ163è¡Œæ–‡æ¡£ï¼‰

**åŒ…å«å†…å®¹**:
- æ‰€æœ‰æ”¹è¿›çš„å¿«é€Ÿæ¦‚è§ˆ
- ä½¿ç”¨ç¤ºä¾‹
- éªŒè¯å‘½ä»¤
- æŠ€æœ¯ç‰¹ç‚¹è¯´æ˜

### å½±å“èŒƒå›´ / Impact
- ğŸŸ¢ **æ–‡æ¡£æ”¹è¿›** - æä¾›å¿«é€Ÿå…¥é—¨æŒ‡å—

---

## Commit 520423e - Add detailed before/after comparison document
**æäº¤æ—¶é—´ / Date**: 2025-10-13

### æ–‡ä»¶å˜æ›´ / Files Changed
- âœ… `BEFORE_AFTER_COMPARISON.md` - æ–°æ–‡ä»¶

### å…·ä½“æ”¹åŠ¨ / Specific Changes

#### BEFORE_AFTER_COMPARISON.md (æ–°æ–‡ä»¶)
**åŠŸèƒ½**: å¯è§†åŒ–å¯¹æ¯”æ–‡æ¡£ï¼ˆ247è¡Œæ–‡æ¡£ï¼‰

**åŒ…å«å†…å®¹**:
- Mask matrixæ”¹è¿›çš„å‰åå¯¹æ¯”
- Promptæ ¼å¼æ”¹è¿›çš„å‰åå¯¹æ¯”
- å¯è§†åŒ–ç¤ºä¾‹
- è¯¦ç»†çš„æ”¹è¿›è¯´æ˜

### å½±å“èŒƒå›´ / Impact
- ğŸŸ¢ **æ–‡æ¡£æ”¹è¿›** - æ¸…æ™°å±•ç¤ºæ”¹è¿›æ•ˆæœ

---

## Commit f391d86 - Add comprehensive documentation for improvements
**æäº¤æ—¶é—´ / Date**: 2025-10-13

### æ–‡ä»¶å˜æ›´ / Files Changed
- âœ… `IMPROVEMENTS_SUMMARY.md` - æ–°æ–‡ä»¶
- âœ… `test_output.txt` - æ–°æ–‡ä»¶

### å…·ä½“æ”¹åŠ¨ / Specific Changes

#### IMPROVEMENTS_SUMMARY.md (æ–°æ–‡ä»¶)
**åŠŸèƒ½**: è¯¦ç»†æŠ€æœ¯æ–‡æ¡£ï¼ˆåŒ…å«å®Œæ•´çš„æŠ€æœ¯å®ç°è¯´æ˜ï¼‰

**åŒ…å«å†…å®¹**:
- æ™ºèƒ½é‡‡æ ·ç®—æ³•è¯¦è§£
- Separatoræ ¼å¼æ”¹è¿›è¯´æ˜
- æŠ€æœ¯å®ç°ç»†èŠ‚
- ä»£ç ç¤ºä¾‹

#### test_output.txt (æ–°æ–‡ä»¶)
**åŠŸèƒ½**: æµ‹è¯•è¾“å‡ºç¤ºä¾‹

### å½±å“èŒƒå›´ / Impact
- ğŸŸ¢ **æ–‡æ¡£æ”¹è¿›** - æä¾›è¯¦ç»†æŠ€æœ¯æ–‡æ¡£

---

## Commit 91905ff - Improve mask matrix visualization and prompt formatting
**æäº¤æ—¶é—´ / Date**: 2025-10-13

### æ–‡ä»¶å˜æ›´ / Files Changed
- âœ… `flex_attention_generate.py` - æ›´æ–°é»˜è®¤separator
- âœ… `tools/debug_flexattention.py` - å¢å¼ºå¯è§†åŒ–
- âœ… `tools/example_flexattention.py` - æ›´æ–°ç¤ºä¾‹
- âœ… `test_mask_visualization.py` - æ–°æ–‡ä»¶

### å…·ä½“æ”¹åŠ¨ / Specific Changes

#### flex_attention_generate.py
**æ”¹åŠ¨**: æ›´æ–°é»˜è®¤separator
- ä» ` [SEP] ` æ”¹ä¸º `\n\n[SEP]\n\n`
- æ”¹å–„promptè¾¹ç•Œçš„è§†è§‰åˆ†éš”

**ä»£ç å˜æ›´**:
```python
# ä¹‹å‰
separator=" [SEP] "

# ç°åœ¨
separator="\n\n[SEP]\n\n"
```

#### tools/debug_flexattention.py
**æ–°å¢åŠŸèƒ½**:
1. æ™ºèƒ½é‡‡æ ·ç®—æ³• - æ˜¾ç¤º~25ä¸ªå…³é”®ä½ç½®
2. Segmentæ ‡è®°ï¼ˆS#/E#/G0ï¼‰
3. æ›´å¥½çš„ç¬¦å·ï¼ˆâ– /Â·ä»£æ›¿âœ“/âœ—ï¼‰
4. å®Œæ•´çš„attentionç»“æ„å¯è§†åŒ–

**æ”¹è¿›ç»†èŠ‚**:
- ä¼˜å…ˆæ˜¾ç¤ºæ‰€æœ‰segmentè¾¹ç•Œ
- åœ¨æ¯ä¸ªsegmentå†…é‡‡æ ·ä»£è¡¨æ€§ä½ç½®
- æ˜¾ç¤ºgenerationèµ·å§‹ä½ç½®
- å¯¹å¤§å‹åºåˆ—ï¼ˆ248+ tokensï¼‰ä¿æŒå¯è¯»æ€§

#### tools/example_flexattention.py
**æ”¹åŠ¨**: æ›´æ–°å¯è§†åŒ–å‡½æ•°ä»¥ä½¿ç”¨æ–°çš„æ™ºèƒ½é‡‡æ ·

#### test_mask_visualization.py (æ–°æ–‡ä»¶)
**åŠŸèƒ½**: å®Œæ•´æµ‹è¯•è„šæœ¬ï¼ˆæ— éœ€æ¨¡å‹ï¼‰

**æµ‹è¯•å†…å®¹**:
- éªŒè¯æ™ºèƒ½é‡‡æ ·ç®—æ³•
- æµ‹è¯•248-tokenåºåˆ—çš„å¯è§†åŒ–
- éªŒè¯segmentè¾¹ç•Œæ ‡è®°

### å½±å“èŒƒå›´ / Impact
- ğŸŸ¢ **ä¸»è¦æ”¹è¿›** - Mask matrixå¯è§†åŒ–å¤§å¹…æ”¹å–„
- ğŸŸ¢ **å¯è¯»æ€§æå‡** - Promptæ ¼å¼æ›´æ¸…æ™°
- ğŸŸ¢ **å‘åå…¼å®¹** - ä¸å½±å“ç”Ÿæˆé€»è¾‘

---

## æ€»ç»“ / Summary

### æ‰€æœ‰å˜æ›´çš„æ–‡ä»¶ç»Ÿè®¡
**ä¿®æ”¹çš„æ ¸å¿ƒæ–‡ä»¶**: 3
- `flex_attention_generate.py`
- `tools/debug_flexattention.py`
- `tools/example_flexattention.py`

**æ–°å¢çš„æ–‡ä»¶**: 9
- `test_mask_visualization.py`
- `test_separator_fix.py`
- `analysis/analyze_flexattention.py`
- `analysis/flexattention_analysis.ipynb`
- `IMPROVEMENTS_SUMMARY.md`
- `BEFORE_AFTER_COMPARISON.md`
- `CHANGES_README.md`
- `FLEXATTENTION_USAGE.md`
- `test_output.txt`

**æ›´æ–°çš„æ–‡æ¡£**: 2
- `README.md`
- `docs/QUICK_REFERENCE.md`

### åŠŸèƒ½ç»Ÿè®¡
- âœ… **6ä¸ªä¸»è¦åŠŸèƒ½æ”¹è¿›**
- âœ… **2ä¸ªBugä¿®å¤**
- âœ… **9ä¸ªæ–°æ–‡ä»¶**
- âœ… **5ä¸ªæ–‡æ¡£æ›´æ–°**
- âœ… **100%å‘åå…¼å®¹**

### ä»£ç è¡Œæ•°ç»Ÿè®¡
- **æ–°å¢ä»£ç **: ~1000è¡Œ
- **æ–°å¢æ–‡æ¡£**: ~1500è¡Œ
- **ä¿®æ”¹ä»£ç **: ~20è¡Œ

---

## Commit d09c197 - Add comprehensive CHANGELOG.md for tracking all changes
**æäº¤æ—¶é—´ / Date**: 2025-10-13

### æ–‡ä»¶å˜æ›´ / Files Changed
- âœ… `CHANGELOG.md` - æ–°æ–‡ä»¶

### å…·ä½“æ”¹åŠ¨ / Specific Changes

#### CHANGELOG.md (æ–°æ–‡ä»¶)
**åŠŸèƒ½**: è¯¦ç»†çš„å˜æ›´è¿½è¸ªæ–‡æ¡£ï¼ˆ311è¡Œæ–‡æ¡£ï¼‰

**åŒ…å«å†…å®¹**:
- æ¯ä¸ªcommitçš„è¯¦ç»†å˜æ›´è®°å½•
- æ–‡ä»¶çº§åˆ«çš„ä¿®æ”¹è¯´æ˜
- å…·ä½“ä»£ç ä¿®æ”¹å’Œç¤ºä¾‹
- å½±å“èŒƒå›´åˆ†æ
- ç»Ÿè®¡ä¿¡æ¯æ±‡æ€»

### å½±å“èŒƒå›´ / Impact
- ğŸŸ¢ **æ–‡æ¡£æ”¹è¿›** - æä¾›å®Œæ•´çš„å˜æ›´å†å²è¿½è¸ª

---

## å¾…æäº¤ - Improve error handling and diagnostics for FlexAttention
**æäº¤æ—¶é—´ / Date**: 2025-10-13 (Pending)

### æ–‡ä»¶å˜æ›´ / Files Changed
- âœ… `flex_attention_generate.py` - æ”¹è¿›é”™è¯¯å¤„ç†
- âœ… `CHANGELOG.md` - æ›´æ–°å˜æ›´è®°å½•å’Œæ•…éšœæ’é™¤

### å…·ä½“æ”¹åŠ¨ / Specific Changes

#### flex_attention_generate.py
**æ”¹è¿›**: å¢å¼ºé”™è¯¯è¯Šæ–­ä¿¡æ¯

**é—®é¢˜**: å½“FlexAttentionå¤±è´¥æ—¶ï¼Œåªæ˜¾ç¤ºç®€å•é”™è¯¯æ¶ˆæ¯ï¼Œéš¾ä»¥è¯Šæ–­é—®é¢˜

**ä¿®å¤**:
1. æ·»åŠ å®Œæ•´çš„tracebackè¾“å‡º
2. æ˜¾ç¤ºå¼‚å¸¸ç±»å‹å’Œè¯¦ç»†ä¿¡æ¯
3. åœ¨ç¬¬ä¸€æ¬¡é”™è¯¯æ—¶æ˜¾ç¤ºå®Œæ•´å †æ ˆè·Ÿè¸ª
4. æ”¹è¿›fallbackæç¤ºä¿¡æ¯

**ä»£ç å˜æ›´**:
```python
# ä¹‹å‰
except Exception as e:
    print(f"âš ï¸  Generation step {step} failed: {e}")

# ç°åœ¨
except Exception as e:
    import traceback
    print(f"âš ï¸  Generation step {step} failed: {type(e).__name__}: {e}")
    print(f"    Full error traceback:")
    traceback.print_exc()
    print(f"    Falling back to unpatched model...")
```

### æ•…éšœæ’é™¤æŒ‡å— / Troubleshooting Guide

#### é—®é¢˜: "Generation step [xx] failed: FlexAttentionWrapper.create_patched_forward"

**å¸¸è§åŸå› **:

1. **PyTorchç‰ˆæœ¬ä¸æ”¯æŒFlexAttention**
   - FlexAttentionéœ€è¦PyTorch 2.5+æˆ–nightlyç‰ˆæœ¬
   - æ£€æŸ¥: `python -c "import torch; print(torch.__version__)"`
   - è§£å†³: 
     ```bash
     pip install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu121
     ```

2. **æ¨¡å‹æ¶æ„ä¸å…¼å®¹**
   - æŸäº›æ¨¡å‹çš„attentionå±‚ç»“æ„å¯èƒ½ä¸patchingä¸å…¼å®¹
   - æ£€æŸ¥æ¨¡å‹æ˜¯å¦æœ‰`q_proj`, `k_proj`, `v_proj`, `o_proj`
   - è§£å†³: ä½¿ç”¨ä¼ ç»Ÿensembleæ–¹æ³•
     ```bash
     python generate.py --dataset webqa --method avg --num_ensemble 5
     ```

3. **CUDA/è®¾å¤‡é—®é¢˜**
   - FlexAttentionå¯èƒ½å¯¹æŸäº›CUDAç‰ˆæœ¬æœ‰è¦æ±‚
   - æ£€æŸ¥: `python -c "import torch; print(torch.cuda.is_available())"`
   - è§£å†³: å°è¯•CPUæ¨¡å¼æˆ–æ›´æ–°CUDAé©±åŠ¨

4. **åºåˆ—é•¿åº¦é—®é¢˜**
   - éå¸¸é•¿çš„åºåˆ—å¯èƒ½å¯¼è‡´å†…å­˜ä¸è¶³
   - è§£å†³: å‡å°‘paraphraseæ•°é‡æˆ–ä½¿ç”¨`--max_samples`é™åˆ¶

**è°ƒè¯•æ­¥éª¤**:

1. **è·å–è¯¦ç»†é”™è¯¯ä¿¡æ¯**
   ```bash
   python flex_attention_generate.py --dataset webqa --model llama3.2_3b_it \
       --num_paraphrases 5 --max_samples 10 2>&1 | tee debug.log
   ```

2. **éªŒè¯FlexAttentionå¯ç”¨æ€§**
   ```bash
   python -c "from torch.nn.attention.flex_attention import flex_attention; print('Available')"
   ```

3. **æµ‹è¯•ç®€å•æƒ…å†µ**
   ```bash
   # åªç”Ÿæˆ1ä¸ªæ ·æœ¬è¿›è¡Œæµ‹è¯•
   python flex_attention_generate.py --dataset webqa --model llama3.2_3b_it \
       --num_paraphrases 3 --max_samples 1
   ```

4. **ä½¿ç”¨fallbackæœºåˆ¶**
   - ä»£ç ä¼šè‡ªåŠ¨fallbackåˆ°æ ‡å‡†attention
   - å¦‚æœfallbackæ­£å¸¸å·¥ä½œï¼Œè¯´æ˜é—®é¢˜åœ¨FlexAttentionæœ¬èº«

**ä¸´æ—¶è§£å†³æ–¹æ¡ˆ**:
å¦‚æœFlexAttentionæŒç»­å¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿensembleæ–¹æ³•ï¼š
```bash
python generate.py --dataset webqa --model llama3.2_3b_it --method avg --num_ensemble 5
```

### å½±å“èŒƒå›´ / Impact
- ğŸŸ¢ **æ”¹è¿›** - æ›´å¥½çš„é”™è¯¯è¯Šæ–­
- ğŸŸ¢ **è°ƒè¯•** - å®Œæ•´çš„tracebackå¸®åŠ©å®šä½é—®é¢˜
- ğŸŸ¢ **ç”¨æˆ·ä½“éªŒ** - æ¸…æ™°çš„é”™è¯¯ä¿¡æ¯å’Œè§£å†³æ–¹æ¡ˆ

---

*æ­¤æ–‡æ¡£ä¼šåœ¨æ¯æ¬¡æäº¤åæ›´æ–° / This document is updated with each commit*
