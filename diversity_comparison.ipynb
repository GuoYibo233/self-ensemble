{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f8b8c9a",
   "metadata": {},
   "source": [
    "# Diversity Comparison with BERT",
    "",
    "This notebook computes the diversity of paraphrases for each prompt combination using BERT embeddings.",
    "We use a modern BERT model to calculate semantic similarity and diversity metrics.",
    "",
    "## Features",
    "",
    "- **BERT-based Semantic Diversity**: Uses transformer models to compute semantic similarity between paraphrases",
    "- **Multiple Model Support**: Automatically tries modern sentence-transformers models with fallback to standard BERT",
    "- **Comprehensive Metrics**: Computes both traditional (Jaccard) and semantic (BERT-based) diversity metrics",
    "- **Per-Prompt Analysis**: Calculates diversity for each prompt combination in your dataset",
    "",
    "## Prerequisites",
    "",
    "1. Install required packages:",
    "   ```bash",
    "   pip install -r requirements.txt",
    "   ```",
    "",
    "2. Ensure you have generated diversity data using the appropriate generation script",
    "",
    "3. Internet access is required for first-time model download from HuggingFace",
    "",
    "## How to Use",
    "",
    "1. **Update Data Path**: In the \"Configure data paths\" cell, adjust the `root` path to point to your dataset",
    "",
    "2. **Run All Cells**: Execute cells in order. The notebook will:",
    "   - Load a BERT model (tries modern models first, falls back to bert-base-uncased)",
    "   - Process each diversity file in your dataset",
    "   - Calculate BERT-based semantic diversity for paraphrases",
    "   - Compare with traditional metrics",
    "   - Display results and correlations",
    "",
    "3. **Review Results**: The final cells display:",
    "   - Per-file diversity scores",
    "   - Correlation analysis between different metrics",
    "   - Top most/least diverse prompt combinations",
    "",
    "## Understanding the Metrics",
    "",
    "- **BERT Diversity**: `1 - average_cosine_similarity` between paraphrase embeddings",
    "  - Higher values (close to 1.0) = more semantic variation",
    "  - Lower values (close to 0.0) = paraphrases are semantically similar",
    "",
    "- **Jaccard Diversity**: Traditional `OR_matches / AND_matches` ratio",
    "",
    "- **Consistency Score**: Proportion of predictions that match across paraphrases",
    "",
    "## Notes",
    "",
    "- The notebook gracefully handles missing data files",
    "- GPU acceleration is used if available",
    "- First run may be slow due to model download",
    "- Subsequent runs use cached models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from utils import partial_match_scores, is_matched_str, partial_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bert_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT model for semantic similarity\n",
    "# Try modern models first, fallback to bert-base-uncased\n",
    "model_options = [\n",
    "    'sentence-transformers/all-MiniLM-L6-v2',  # Efficient sentence-transformers model\n",
    "    'sentence-transformers/all-mpnet-base-v2',  # High quality sentence-transformers\n",
    "    'bert-base-uncased',  # Standard BERT\n",
    "]\n",
    "\n",
    "bert_model = None\n",
    "tokenizer = None\n",
    "model_name = None\n",
    "\n",
    "for model_candidate in model_options:\n",
    "    try:\n",
    "        print(f\"Trying to load: {model_candidate}\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_candidate)\n",
    "        bert_model = AutoModel.from_pretrained(model_candidate)\n",
    "        model_name = model_candidate\n",
    "        print(f\"\u2713 Successfully loaded: {model_name}\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"\u2717 Failed to load {model_candidate}: {str(e)[:100]}\")\n",
    "        continue\n",
    "\n",
    "if bert_model is None:\n",
    "    raise RuntimeError(\"Failed to load any BERT model. Please ensure you have internet access and try again.\")\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "bert_model = bert_model.to(device)\n",
    "bert_model.eval()\n",
    "\n",
    "print(f\"\\nModel loaded successfully on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bert_embedding",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings(texts, batch_size=32):\n",
    "    \"\"\"\n",
    "    Get BERT embeddings for a list of texts.\n",
    "    Uses mean pooling over token embeddings.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            \n",
    "            # Tokenize and encode\n",
    "            encoded = tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors='pt'\n",
    "            ).to(device)\n",
    "            \n",
    "            # Get model output\n",
    "            outputs = bert_model(**encoded)\n",
    "            \n",
    "            # Mean pooling\n",
    "            attention_mask = encoded['attention_mask']\n",
    "            token_embeddings = outputs.last_hidden_state\n",
    "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "            sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "            batch_embeddings = (sum_embeddings / sum_mask).cpu().numpy()\n",
    "            \n",
    "            embeddings.append(batch_embeddings)\n",
    "    \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "def compute_diversity_score(paraphrases):\n",
    "    \"\"\"\n",
    "    Compute diversity score for a list of paraphrases.\n",
    "    Higher diversity score means more semantic variation.\n",
    "    \n",
    "    Diversity is calculated as 1 - average pairwise cosine similarity.\n",
    "    \"\"\"\n",
    "    if len(paraphrases) < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    # Get embeddings\n",
    "    embeddings = get_bert_embeddings(paraphrases)\n",
    "    \n",
    "    # Compute pairwise cosine similarity\n",
    "    similarities = cosine_similarity(embeddings)\n",
    "    \n",
    "    # Get upper triangle (excluding diagonal) to avoid counting pairs twice\n",
    "    n = len(paraphrases)\n",
    "    sum_similarity = 0\n",
    "    count = 0\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            sum_similarity += similarities[i, j]\n",
    "            count += 1\n",
    "    \n",
    "    avg_similarity = sum_similarity / count if count > 0 else 0\n",
    "    \n",
    "    # Diversity is inverse of similarity\n",
    "    diversity = 1.0 - avg_similarity\n",
    "    \n",
    "    return diversity\n",
    "\n",
    "print(\"BERT embedding and diversity functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure data paths\n",
    "# You can change this to point to your specific dataset\n",
    "root_options = [\n",
    "    \"../datasets/myriadlama/llama3.2_1b_it\",\n",
    "    \"../datasets/myriadlama/llama3.2_3b_it\",\n",
    "    \"../datasets/myriadlama/llama3.1_8b_it\",\n",
    "    \"../datasets/myriadlama/qwen2.5_3b_it\",\n",
    "    \"../datasets/myriadlama/qwen2.5_7b_it\",\n",
    "    \"datasets/myriadlama/llama3.2_1b_it\",\n",
    "    \"datasets/myriadlama/llama3.2_3b_it\",\n",
    "    \"datasets/myriadlama/llama3.1_8b_it\",\n",
    "    \"datasets/myriadlama/qwen2.5_3b_it\",\n",
    "    \"datasets/myriadlama/qwen2.5_7b_it\",\n",
    "]\n",
    "\n",
    "# Find the first valid root path\n",
    "root = None\n",
    "for root_path in root_options:\n",
    "    if os.path.exists(root_path):\n",
    "        root = root_path\n",
    "        print(f\"Using data root: {root}\")\n",
    "        break\n",
    "\n",
    "if root is None:\n",
    "    print(\"Warning: No valid data root found. Available paths:\")\n",
    "    for root_path in root_options:\n",
    "        print(f\"  - {root_path} (exists: {os.path.exists(root_path)})\")\n",
    "    print(\"\\nYou may need to adjust the root paths or generate data first.\")\n",
    "    # Use a default path for demonstration\n",
    "    root = \"datasets/myriadlama/llama3.2_1b_it\"\n",
    "    print(f\"Using default path: {root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_confidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load confidence data if available\n",
    "confidence_file = os.path.join(root, \"confidence.feather\")\n",
    "\n",
    "if os.path.exists(confidence_file):\n",
    "    confi_df = pd.read_feather(confidence_file)\n",
    "    confi_df[\"sample_lemmas\"] = confi_df[\"sample_lemmas\"].apply(lambda xs: [list(x) for x in xs])\n",
    "    confi_df[\"answer_lemmas\"] = confi_df[\"answer_lemmas\"].apply(lambda xs: [list(x) for x in xs])\n",
    "    print(f\"Loaded confidence data: {len(confi_df)} rows\")\n",
    "    print(f\"Columns: {confi_df.columns.tolist()}\")\n",
    "else:\n",
    "    print(f\"Confidence file not found: {confidence_file}\")\n",
    "    confi_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diversity_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze diversity for each prompt combination\n",
    "droot = os.path.join(root, \"diversity\")\n",
    "\n",
    "if not os.path.exists(droot):\n",
    "    print(f\"Diversity directory not found: {droot}\")\n",
    "    print(\"Please generate diversity data first using the appropriate script.\")\n",
    "else:\n",
    "    print(f\"Processing diversity data from: {droot}\")\n",
    "    \n",
    "    # Storage for metrics\n",
    "    results = []\n",
    "    \n",
    "    ensemble_scores = []\n",
    "    consistency_scores = []\n",
    "    or_matches_scores = []\n",
    "    diversity_scores = []  # Original diversity (Jaccard)\n",
    "    bert_diversity_scores = []  # New BERT-based semantic diversity\n",
    "    avg_match_scores = []\n",
    "    new_ratio_scores = []\n",
    "    moe_ratio_scores = []\n",
    "    \n",
    "    file_list = [fn for fn in os.listdir(droot) if fn.endswith(\".feather\") and len(fn.split(\",\")) == 2]\n",
    "    \n",
    "    if len(file_list) == 0:\n",
    "        print(\"No diversity files found matching the pattern (files with exactly one comma).\")\n",
    "    else:\n",
    "        print(f\"Found {len(file_list)} diversity files to process\")\n",
    "    \n",
    "    for fn in tqdm(file_list, desc=\"Processing diversity files\"):\n",
    "        df = pd.read_feather(os.path.join(droot, fn))\n",
    "        df[\"answer_lemmas\"] = df[\"answer_lemmas\"].apply(lambda xs: [list(x) for x in xs])\n",
    "        \n",
    "        # Compute partial match scores\n",
    "        scores = partial_match_scores(df['predict_lemma'].tolist(), df[\"answer_lemmas\"].tolist())\n",
    "        \n",
    "        # Extract paraphrases for diversity calculation\n",
    "        all_paraphrases = []\n",
    "        for paraphrases in df[\"paraphrases\"].tolist():\n",
    "            if isinstance(paraphrases, (list, tuple)):\n",
    "                all_paraphrases.extend(paraphrases)\n",
    "        \n",
    "        # Compute BERT-based diversity for this prompt combination\n",
    "        if len(all_paraphrases) >= 2:\n",
    "            bert_diversity = compute_diversity_score(all_paraphrases)\n",
    "        else:\n",
    "            bert_diversity = 0.0\n",
    "        \n",
    "        bert_diversity_scores.append(bert_diversity)\n",
    "        \n",
    "        # Process traditional metrics if confidence data is available\n",
    "        if confi_df is not None and \"paraphrases\" in df.columns:\n",
    "            predict_by_set = [[], [], [], []]\n",
    "            \n",
    "            for paraphrases, predict_lemma in zip(df[\"paraphrases\"].tolist(), df['predict_lemma'].tolist()):\n",
    "                for idx, paraphrase in enumerate(paraphrases):\n",
    "                    matching_rows = confi_df[confi_df[\"paraphrase\"] == paraphrase]\n",
    "                    if len(matching_rows) > 0:\n",
    "                        predict = matching_rows['greedy_lemma'].tolist()[0]\n",
    "                        predict_by_set[idx].append(predict.tolist())\n",
    "                    \n",
    "                predict_by_set[-2].append(predict_lemma)\n",
    "                if len(matching_rows) > 0:\n",
    "                    predict_by_set[-1].append(matching_rows['answer_lemmas'].tolist()[0])\n",
    "            \n",
    "            # Only proceed if we have valid data\n",
    "            if len(predict_by_set[0]) > 0:\n",
    "                consistency_matches = []\n",
    "                or_matches = []\n",
    "                and_matches = []\n",
    "                avg_matches = []\n",
    "                ensemble_matches = []\n",
    "                \n",
    "                for predict1, predict2, ensemble_predict, answer_lemmas in zip(*predict_by_set):\n",
    "                    match1 = partial_match(predict1, answer_lemmas, birdirectional=False)\n",
    "                    match2 = partial_match(predict2, answer_lemmas, birdirectional=False)\n",
    "                    or_matches.append(match1 or match2)\n",
    "                    and_matches.append(match1 and match2)\n",
    "                    avg_matches.append(float(int(match1) + int(match2))/2)\n",
    "                    ensemble_matches.append(partial_match(ensemble_predict, answer_lemmas, birdirectional=False))\n",
    "                    consistency_matches.append(is_matched_str(predict1, predict2, birdirectional=True))\n",
    "                \n",
    "                moe_cnt = 0\n",
    "                new_cnt = 0\n",
    "                for and_match, or_match, ensemble_match in zip(and_matches, or_matches, ensemble_matches):\n",
    "                    if not and_match and or_match and ensemble_match:\n",
    "                        moe_cnt += 1\n",
    "                    if not and_match and not or_match and ensemble_match:\n",
    "                        new_cnt += 1\n",
    "                \n",
    "                moe_ratio = moe_cnt / len(and_matches) if len(and_matches) > 0 else 0\n",
    "                new_ratio = new_cnt / len(and_matches) if len(and_matches) > 0 else 0\n",
    "                \n",
    "                moe_ratio_scores.append(moe_ratio)\n",
    "                new_ratio_scores.append(new_ratio)\n",
    "                \n",
    "                ensemble_scores.append(scores)\n",
    "                consistency_scores.append(sum(consistency_matches) / len(consistency_matches) if len(consistency_matches) > 0 else 0)\n",
    "                or_matches_scores.append(sum(or_matches) / len(or_matches) if len(or_matches) > 0 else 0)\n",
    "                \n",
    "                # Original diversity (Jaccard)\n",
    "                jaccard_diversity = sum(or_matches) / sum(and_matches) if sum(and_matches) > 0 else 0\n",
    "                diversity_scores.append(jaccard_diversity)\n",
    "                \n",
    "                avg_match_scores.append(sum(avg_matches) / len(avg_matches) if len(avg_matches) > 0 else 0)\n",
    "                \n",
    "                # Store result for this file\n",
    "                results.append({\n",
    "                    'file': fn,\n",
    "                    'ensemble_score': scores,\n",
    "                    'consistency_score': consistency_scores[-1],\n",
    "                    'or_match_score': or_matches_scores[-1],\n",
    "                    'jaccard_diversity': jaccard_diversity,\n",
    "                    'bert_diversity': bert_diversity,\n",
    "                    'avg_match_score': avg_match_scores[-1],\n",
    "                    'moe_ratio': moe_ratio,\n",
    "                    'new_ratio': new_ratio,\n",
    "                    'num_paraphrases': len(all_paraphrases)\n",
    "                })\n",
    "        else:\n",
    "            # If no confidence data, just store basic info\n",
    "            results.append({\n",
    "                'file': fn,\n",
    "                'bert_diversity': bert_diversity,\n",
    "                'num_paraphrases': len(all_paraphrases)\n",
    "            })\n",
    "    \n",
    "    # Create results dataframe\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DIVERSITY ANALYSIS RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(results_df.to_string())\n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correlation_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "if len(ensemble_scores) > 1 and len(bert_diversity_scores) > 0:\n",
    "    print(\"\\nCORRELATION ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if len(ensemble_scores) == len(bert_diversity_scores):\n",
    "        print(f\"Pearson correlation between ensemble and BERT diversity scores: {pd.Series(ensemble_scores).corr(pd.Series(bert_diversity_scores)):.4f}\")\n",
    "    \n",
    "    if len(consistency_scores) > 0:\n",
    "        print(f\"Pearson correlation between ensemble and consistency scores: {pd.Series(ensemble_scores).corr(pd.Series(consistency_scores)):.4f}\")\n",
    "    \n",
    "    if len(or_matches_scores) > 0:\n",
    "        print(f\"Pearson correlation between ensemble and OR match scores: {pd.Series(ensemble_scores).corr(pd.Series(or_matches_scores)):.4f}\")\n",
    "    \n",
    "    if len(diversity_scores) > 0:\n",
    "        print(f\"Pearson correlation between ensemble and Jaccard diversity scores: {pd.Series(ensemble_scores).corr(pd.Series(diversity_scores)):.4f}\")\n",
    "    \n",
    "    if len(avg_match_scores) > 0:\n",
    "        print(f\"Pearson correlation between ensemble and avg match scores: {pd.Series(ensemble_scores).corr(pd.Series(avg_match_scores)):.4f}\")\n",
    "    \n",
    "    if len(moe_ratio_scores) > 0:\n",
    "        print(f\"\\nMean of MOE ratio scores: {sum(moe_ratio_scores)/len(moe_ratio_scores):.4f}\")\n",
    "    \n",
    "    if len(new_ratio_scores) > 0:\n",
    "        print(f\"Mean of new ratio scores: {sum(new_ratio_scores)/len(new_ratio_scores):.4f}\")\n",
    "    \n",
    "    if len(bert_diversity_scores) > 0:\n",
    "        print(f\"\\nMean BERT diversity score: {np.mean(bert_diversity_scores):.4f}\")\n",
    "        print(f\"Std BERT diversity score: {np.std(bert_diversity_scores):.4f}\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"\\nInsufficient data for correlation analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total files processed: {len(results)}\")\n",
    "print(f\"BERT model used: {model_name}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if len(results) > 0:\n",
    "    print(f\"\\nAverage BERT diversity across all prompt combinations: {results_df['bert_diversity'].mean():.4f}\")\n",
    "    print(f\"Min BERT diversity: {results_df['bert_diversity'].min():.4f}\")\n",
    "    print(f\"Max BERT diversity: {results_df['bert_diversity'].max():.4f}\")\n",
    "    \n",
    "    # Show top 5 most diverse prompt combinations\n",
    "    print(\"\\nTop 5 most diverse prompt combinations (by BERT diversity):\")\n",
    "    top_diverse = results_df.nlargest(5, 'bert_diversity')[['file', 'bert_diversity', 'num_paraphrases']]\n",
    "    print(top_diverse.to_string(index=False))\n",
    "    \n",
    "    # Show top 5 least diverse prompt combinations\n",
    "    print(\"\\nTop 5 least diverse prompt combinations (by BERT diversity):\")\n",
    "    least_diverse = results_df.nsmallest(5, 'bert_diversity')[['file', 'bert_diversity', 'num_paraphrases']]\n",
    "    print(least_diverse.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}