{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# FlexAttention Generation Results Analysis\n",
    "\n",
    "This notebook analyzes the results from FlexAttention-based ensemble generation and compares them with traditional ensemble methods.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, '..')\n",
    "from utils import partial_match, partial_match_scores\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set the dataset and model to analyze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATASET = \"myriadlama\"  # or \"webqa\"\n",
    "MODEL = \"qwen2.5_7b_it\"  # e.g., \"llama3.2_3b_it\", \"qwen2.5_7b_it\"\n",
    "NUM_PARAPHRASES = 5\n",
    "\n",
    "# Construct paths\n",
    "root = f\"../datasets/{DATASET}/{MODEL}\"\n",
    "print(f\"Dataset root: {root}\")\n",
    "print(f\"Exists: {os.path.exists(root)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Load FlexAttention Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FlexAttention results\n",
    "flex_file = os.path.join(root, f\"flex_attention-{NUM_PARAPHRASES}.feather\")\n",
    "\n",
    "if os.path.exists(flex_file):\n",
    "    df_flex = pd.read_feather(flex_file)\n",
    "    print(f\"✅ Loaded FlexAttention results: {len(df_flex)} samples\")\n",
    "    print(f\"\\nColumns: {df_flex.columns.tolist()}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    display(df_flex.head())\n",
    "else:\n",
    "    print(f\"❌ FlexAttention results not found: {flex_file}\")\n",
    "    print(f\"   Run: python flex_attention_generate.py --dataset {DATASET} --model {MODEL} --num_paraphrases {NUM_PARAPHRASES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Compute Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if lemmatized results are available\n",
    "if \"predict_lemma\" in df_flex.columns and \"answer_lemmas\" in df_flex.columns:\n",
    "    # Process lemmas\n",
    "    df_flex[\"answer_lemmas\"] = df_flex[\"answer_lemmas\"].apply(\n",
    "        lambda xs: [list(x) for x in xs] if isinstance(xs, list) else xs\n",
    "    )\n",
    "    \n",
    "    answers = df_flex[\"answer_lemmas\"].tolist()\n",
    "    predictions = df_flex['predict_lemma'].tolist()\n",
    "    \n",
    "    # Compute accuracy\n",
    "    flex_acc = partial_match_scores(predictions, answers)\n",
    "    print(f\"FlexAttention Accuracy: {flex_acc:.3f}\")\n",
    "    \n",
    "    # Compute per-sample matches\n",
    "    matches = [partial_match(pred, ans, False) for pred, ans in zip(predictions, answers)]\n",
    "    df_flex['correct'] = matches\n",
    "    \n",
    "    print(f\"Correct predictions: {sum(matches)}/{len(matches)} ({sum(matches)/len(matches)*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"⚠️  Lemmatized results not available\")\n",
    "    print(\"   Run: python flex_attention_generate.py --dataset {DATASET} --lemmaize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Sample Generations\n",
    "\n",
    "Look at some example generations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some examples\n",
    "print(\"Sample Generations:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i in range(min(5, len(df_flex))):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"  UUID: {df_flex.iloc[i]['uuid']}\")\n",
    "    print(f\"  Answer: {df_flex.iloc[i]['answers']}\")\n",
    "    print(f\"  Prediction: {df_flex.iloc[i]['prediction']}\")\n",
    "    if 'correct' in df_flex.columns:\n",
    "        print(f\"  Correct: {'✓' if df_flex.iloc[i]['correct'] else '✗'}\")\n",
    "    print(f\"  Generation: {df_flex.iloc[i]['generation'][:150]}...\")\n",
    "    print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Compare with Traditional Ensemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and compare with traditional ensemble methods\n",
    "methods = [\"avg\", \"max\", \"weighted_avg\", \"weighted_max\"]\n",
    "results = {}\n",
    "\n",
    "for method in methods:\n",
    "    ensemble_file = os.path.join(root, f\"ensemble_{method}-{NUM_PARAPHRASES}.feather\")\n",
    "    \n",
    "    if os.path.exists(ensemble_file):\n",
    "        df_ensemble = pd.read_feather(ensemble_file)\n",
    "        \n",
    "        if \"predict_lemma\" in df_ensemble.columns and \"answer_lemmas\" in df_ensemble.columns:\n",
    "            df_ensemble[\"answer_lemmas\"] = df_ensemble[\"answer_lemmas\"].apply(\n",
    "                lambda xs: [list(x) for x in xs] if isinstance(xs, list) else xs\n",
    "            )\n",
    "            answers = df_ensemble[\"answer_lemmas\"].tolist()\n",
    "            predictions = df_ensemble['predict_lemma'].tolist()\n",
    "            \n",
    "            acc = partial_match_scores(predictions, answers)\n",
    "            results[method] = acc\n",
    "            print(f\"{method:15s}: {acc:.3f}\")\n",
    "\n",
    "# Add FlexAttention results\n",
    "if 'flex_acc' in locals():\n",
    "    results['flex_attention'] = flex_acc\n",
    "    print(f\"{'flex_attention':15s}: {flex_acc:.3f}\")\n",
    "\n",
    "print(f\"\\nResults summary:\")\n",
    "for method, acc in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {method:20s}: {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Visualization: Method Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "if results:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    methods_list = list(results.keys())\n",
    "    accs_list = list(results.values())\n",
    "    \n",
    "    colors = ['skyblue' if m != 'flex_attention' else 'orange' for m in methods_list]\n",
    "    \n",
    "    bars = ax.bar(methods_list, accs_list, color=colors, alpha=0.8)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, acc in zip(bars, accs_list):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{acc:.3f}',\n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title(f'Ensemble Method Comparison ({NUM_PARAPHRASES} paraphrases)')\n",
    "    ax.set_ylim([0, max(accs_list) * 1.1])\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No results to plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Effect of Number of Paraphrases\n",
    "\n",
    "Compare FlexAttention performance with different numbers of paraphrases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different numbers of paraphrases\n",
    "paraphrase_results = {}\n",
    "\n",
    "for n in range(2, 11):\n",
    "    flex_file = os.path.join(root, f\"flex_attention-{n}.feather\")\n",
    "    \n",
    "    if os.path.exists(flex_file):\n",
    "        df = pd.read_feather(flex_file)\n",
    "        \n",
    "        if \"predict_lemma\" in df.columns and \"answer_lemmas\" in df.columns:\n",
    "            df[\"answer_lemmas\"] = df[\"answer_lemmas\"].apply(\n",
    "                lambda xs: [list(x) for x in xs] if isinstance(xs, list) else xs\n",
    "            )\n",
    "            answers = df[\"answer_lemmas\"].tolist()\n",
    "            predictions = df['predict_lemma'].tolist()\n",
    "            \n",
    "            acc = partial_match_scores(predictions, answers)\n",
    "            paraphrase_results[n] = acc\n",
    "\n",
    "if paraphrase_results:\n",
    "    print(\"FlexAttention accuracy vs. number of paraphrases:\")\n",
    "    for n, acc in sorted(paraphrase_results.items()):\n",
    "        print(f\"  {n} paraphrases: {acc:.3f}\")\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ns = list(paraphrase_results.keys())\n",
    "    accs = list(paraphrase_results.values())\n",
    "    \n",
    "    ax.plot(ns, accs, marker='o', linewidth=2, markersize=8)\n",
    "    ax.set_xlabel('Number of Paraphrases')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title('FlexAttention: Effect of Number of Paraphrases')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xticks(ns)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find best\n",
    "    best_n = max(paraphrase_results.items(), key=lambda x: x[1])\n",
    "    print(f\"\\nBest: {best_n[0]} paraphrases with accuracy {best_n[1]:.3f}\")\n",
    "else:\n",
    "    print(\"No results for different numbers of paraphrases\")\n",
    "    print(\"Generate them with: python flex_attention_generate.py --num_paraphrases N\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "Look at cases where FlexAttention fails:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error analysis\n",
    "if 'correct' in df_flex.columns:\n",
    "    incorrect = df_flex[df_flex['correct'] == False]\n",
    "    \n",
    "    print(f\"Incorrect predictions: {len(incorrect)} / {len(df_flex)}\")\n",
    "    print(\"\\nSample incorrect predictions:\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for i in range(min(5, len(incorrect))):\n",
    "        row = incorrect.iloc[i]\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"  UUID: {row['uuid']}\")\n",
    "        print(f\"  Expected: {row['answers']}\")\n",
    "        print(f\"  Predicted: {row['prediction']}\")\n",
    "        print(f\"  Generation: {row['generation'][:150]}...\")\n",
    "        print(\"-\"*70)\n",
    "else:\n",
    "    print(\"Correctness information not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"=\"*70)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nDataset: {DATASET}\")\n",
    "print(f\"Model: {MODEL}\")\n",
    "print(f\"Number of paraphrases: {NUM_PARAPHRASES}\")\n",
    "print(f\"Total samples: {len(df_flex)}\")\n",
    "\n",
    "if 'flex_acc' in locals():\n",
    "    print(f\"\\nFlexAttention accuracy: {flex_acc:.3f}\")\n",
    "    \n",
    "    if results:\n",
    "        traditional_accs = [acc for method, acc in results.items() if method != 'flex_attention']\n",
    "        if traditional_accs:\n",
    "            avg_traditional = sum(traditional_accs) / len(traditional_accs)\n",
    "            print(f\"Average traditional ensemble: {avg_traditional:.3f}\")\n",
    "            improvement = flex_acc - avg_traditional\n",
    "            print(f\"Improvement: {improvement:+.3f} ({improvement/avg_traditional*100:+.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
