# Self-Ensemble with FlexAttention

This repository implements self-ensemble methods for natural language generation, including a novel FlexAttention-based approach that enables efficient attention-level fusion of multiple paraphrases.

## ğŸš€ Quick Start

**New to this repository? Start here:**

1. **[QUICKSTART.md](docs/QUICKSTART.md)** - Get up and running in 5 minutes
2. **[DELEGATE_PROMPT.md](docs/DELEGATE_PROMPT.md)** - Complete debugging and validation guide
3. **[README_FLEXATTENTION.md](docs/README_FLEXATTENTION.md)** - FlexAttention overview

## ğŸ“š What's in This Repository

### Core Implementation

- **`flex_attention_generate.py`** - FlexAttention-based ensemble generation (NEW!)
- **`generate.py`** - Original ensemble methods (per_prompt, avg, max, weighted_avg)
- **`dataset.py`** - Dataset loading (WebQA, MyriadLAMA)
- **`constants.py`** - Model paths and configurations

### Debugging and Validation Tools

- **`tools/validate_flexattention_env.py`** - Environment validation script
- **`tools/debug_flexattention.py`** - Step-by-step debugging with detailed output
- **`tools/example_flexattention.py`** - Minimal working examples
- **`tools/download_resources.sh`** - Download datasets and models

### Analysis Tools

- **`analysis/analyze_flexattention.py`** - Command-line analysis tool for FlexAttention results
- **`analysis/flexattention_analysis.ipynb`** - Interactive Jupyter notebook for analysis and visualization

### Visualization Tools

- **`plot/flowchart_and_attention_mask_visualization.ipynb`** - Interactive notebook for visualizing code flowchart and attention masks
- **`plot/demo_visualization.py`** - Standalone script to generate demo visualizations
- **`plot/test_visualization.py`** - Test script to verify mask functions
- **`plot/README.md`** - Detailed usage guide for visualization tools

### Documentation

| Document | Description |
|----------|-------------|
| **[docs/QUICKSTART.md](docs/QUICKSTART.md)** | 5-minute setup guide |
| **[docs/LINUX_SETUP.md](docs/LINUX_SETUP.md)** | Linux-specific setup (Ubuntu 22.04, RTX A6000) |
| **[docs/DELEGATE_PROMPT.md](docs/DELEGATE_PROMPT.md)** | Complete debugging guide |
| **[docs/README_FLEXATTENTION.md](docs/README_FLEXATTENTION.md)** | FlexAttention overview |
| **[docs/QUICK_REFERENCE.md](docs/QUICK_REFERENCE.md)** | API quick reference |
| **[docs/FLEX_ATTENTION_IMPLEMENTATION.md](docs/FLEX_ATTENTION_IMPLEMENTATION.md)** | Technical details |
| **[docs/CREATE_FLEX_ATTENTION_MASK_IMPLEMENTATION.md](docs/CREATE_FLEX_ATTENTION_MASK_IMPLEMENTATION.md)** | Mask function implementation guide |
| **[docs/ARCHITECTURE.md](docs/ARCHITECTURE.md)** | Visual diagrams |
| **[docs/REUSE_VS_NEW_DETAILED.md](docs/REUSE_VS_NEW_DETAILED.md)** | Component breakdown |
| **[FLEXATTENTION_USAGE.md](FLEXATTENTION_USAGE.md)** | Usage guide with --max_samples and analysis tools |
| **[IMPROVEMENTS_SUMMARY.md](IMPROVEMENTS_SUMMARY.md)** | Recent mask matrix & prompt formatting improvements |
| **[docs/å®ç°æ€»ç»“.md](docs/å®ç°æ€»ç»“.md)** | Chinese summary |

## ğŸ¯ What is FlexAttention Ensemble?

FlexAttention ensemble is a new method that:

1. **Concatenates** multiple paraphrases into a single prompt
2. **Isolates** each paraphrase during encoding using custom attention masks
3. **Fuses** information from all paraphrases during generation

**Result:** More efficient (1Ã— forward pass vs 5Ã—) with attention-based fusion.

### Comparison with Existing Methods

| Method | Fusion | Efficiency | Forward Passes | Description |
|--------|--------|------------|----------------|-------------|
| **Baseline 1 (origin)** | None | Fastest | 1Ã— per step | Original question only (attention mode baseline) |
| **Baseline 2 (per_prompt)** | None | Standard | NÃ— per step | Each paraphrase separately (second baseline) |
| avg/max | Logit-level | Standard | NÃ— per step | Logit-level ensemble fusion |
| weighted_* | Logit + confidence | Standard | NÃ— per step | Weighted logit-level fusion |
| **flex_attention** | **Attention-level** | **Most efficient** | **1Ã— per step** | **Attention-level fusion (most efficient)** |

## ğŸ”§ Setup

### Prerequisites

- Python 3.10+  # FlexAttention requires Python 3.10+
- PyTorch 2.5+ or nightly (for FlexAttention)
- 20GB disk space
- NVIDIA GPU with CUDA support
- Conda/Miniconda installed

### Quick Setup

```bash
# 1. Create conda environment
# Option 1: Linux with CUDA 12.1 (Ubuntu 22.04+, RTX A6000)
conda env create -f environment_linux.yml
conda activate self-ensemble-debug

# Option 2: General environment with PyTorch nightly
conda env create -f environment.yml
conda activate flexattention

# Option 3: Manual with pip (requires Python 3.10+)
conda create -n flexattention python=3.10 -y
conda activate flexattention

# Install PyTorch FIRST
pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu121

# Then install other dependencies
pip install -r requirements.txt

# Download spaCy model
python -m spacy download en_core_web_lg

# 2. Validate environment
python3 tools/validate_flexattention_env.py --test-flex-attention

# 3. Download resources
bash tools/download_resources.sh --dataset webqa --model llama3.2_3b_it
```

For detailed setup instructions, see **[docs/QUICKSTART.md](docs/QUICKSTART.md)**.

## ğŸ“– Usage

### Baseline Generation

```bash
# Baseline 1: Original questions only (attention mode baseline)
python3 baseline_generate.py \
    --method origin \
    --dataset webqa \
    --model llama3.2_3b_it

# Baseline 2: Each paraphrase separately (second baseline for attention mode)
python3 baseline_generate.py \
    --method per_prompt \
    --dataset webqa \
    --model llama3.2_3b_it

# Generate both baselines
python3 baseline_generate.py \
    --method all \
    --dataset webqa \
    --model llama3.2_3b_it
```

For detailed baseline usage, see **[BASELINE_USAGE.md](BASELINE_USAGE.md)**.

### Ensemble Generation

```bash
# Ensemble methods: max, avg, weighted_avg, weighted_max
python3 generate.py \
    --method max \
    --dataset webqa \
    --model llama3.2_3b_it \
    --num_ensemble 6

# FlexAttention with 5 paraphrases (most efficient)
python3 flex_attention_generate.py \
    --dataset webqa \
    --model llama3.2_3b_it \
    --num_paraphrases 5

# Limit to 100 samples for quick testing
python3 flex_attention_generate.py \
    --dataset webqa \
    --model llama3.2_3b_it \
    --num_paraphrases 5 \
    --max_samples 100
```

### Analysis

```bash
# Analyze baseline results
python3 analysis/analyze_baseline.py \
    --dataset webqa \
    --model llama3.2_3b_it \
    --compare

# Analyze FlexAttention results
python3 analysis/analyze_flexattention.py \
    --dataset webqa \
    --model llama3.2_3b_it

# Interactive analysis with Jupyter
jupyter notebook analysis/flexattention_analysis.ipynb
```

For detailed usage and analysis guide, see **[FLEXATTENTION_USAGE.md](FLEXATTENTION_USAGE.md)**.

### Debugging

```bash
# Debug mode with detailed output
python3 tools/debug_flexattention.py \
    --dataset webqa \
    --model llama3.2_3b_it \
    --max-samples 2 \
    --verbose
```

### Minimal Example

```bash
# Run standalone example (no dataset/model required)
python3 tools/example_flexattention.py
```

For more examples, see **[docs/QUICK_REFERENCE.md](docs/QUICK_REFERENCE.md)**.

## ğŸ› Debugging

This repository includes comprehensive debugging tools:

### Command Line Debugging

```bash
# Debug with detailed step-by-step output
python3 tools/debug_flexattention.py --dataset webqa --max-samples 1 --verbose
```

**Shows:**
- ğŸ“Š Tensor shapes and values
- ğŸ¯ Attention mask visualization
- ğŸ” Token-by-token generation
- âœ… Segment isolation verification

### VSCode Debugging

1. Open repository in VSCode
2. Press `F5`
3. Select debug configuration:
   - "Debug FlexAttention - WebQA"
   - "Debug Script - WebQA (2 samples)"
   - "Validate Environment"

See **[docs/DELEGATE_PROMPT.md](docs/DELEGATE_PROMPT.md)** for complete debugging guide.

## ğŸ§ª Testing

Run the validation and example scripts:

```bash
# Validate environment
python3 tools/validate_flexattention_env.py --test-flex-attention

# Run minimal example
python3 tools/example_flexattention.py

# Test notebooks (requires Jupyter)
jupyter notebook test/test_generate.ipynb
```

## ğŸ“Š Datasets

Supported datasets:

- **WebQA**: Question answering dataset
- **MyriadLAMA**: Knowledge probing dataset

Download with:
```bash
bash tools/download_resources.sh --dataset webqa
bash tools/download_resources.sh --dataset myriadlama
```

## ğŸ¤– Models

Supported models (defined in `constants.py`):

- Llama 3.2 3B Instruct
- Other models can be added to `MODEL_PATHs`

Download with:
```bash
bash tools/download_resources.sh --model llama3.2_3b_it
```

## ğŸ“ Repository Structure

```
.
â”œâ”€â”€ baseline_generate.py           # NEW: Baseline generation script
â”œâ”€â”€ flex_attention_generate.py    # FlexAttention implementation
â”œâ”€â”€ generate.py                    # Original ensemble methods
â”œâ”€â”€ dataset.py                     # Dataset loading
â”œâ”€â”€ constants.py                   # Configuration
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ environment.yml                # Conda environment file (general)
â”œâ”€â”€ environment_linux.yml          # Linux-specific environment (Ubuntu 22.04, CUDA 12.1)
â”‚
â”œâ”€â”€ tools/                         # Debugging and utilities
â”‚   â”œâ”€â”€ validate_flexattention_env.py  # Environment validation
â”‚   â”œâ”€â”€ debug_flexattention.py         # Debugging script
â”‚   â”œâ”€â”€ example_flexattention.py       # Minimal examples
â”‚   â””â”€â”€ download_resources.sh          # Resource downloader
â”‚
â”œâ”€â”€ analysis/                      # Analysis tools
â”‚   â”œâ”€â”€ analyze_baseline.py        # NEW: Baseline analysis
â”‚   â”œâ”€â”€ analyze_flexattention.py   # FlexAttention analysis
â”‚   â”œâ”€â”€ flexattention_analysis.ipynb   # Interactive analysis notebook
â”‚   â””â”€â”€ [other analysis notebooks]
â”‚
â”œâ”€â”€ docs/                          # Documentation
â”‚   â”œâ”€â”€ QUICKSTART.md              # Quick start guide
â”‚   â”œâ”€â”€ LINUX_SETUP.md             # Linux-specific setup guide
â”‚   â”œâ”€â”€ DELEGATE_PROMPT.md         # Complete debugging guide
â”‚   â”œâ”€â”€ README_FLEXATTENTION.md    # FlexAttention overview
â”‚   â”œâ”€â”€ QUICK_REFERENCE.md         # API reference
â”‚   â”œâ”€â”€ IMPROVEMENTS.md            # NEW: Consolidated improvements
â”‚   â”œâ”€â”€ FLEX_ATTENTION_IMPLEMENTATION.md  # Technical details
â”‚   â””â”€â”€ ARCHITECTURE.md            # Architecture diagrams
â”‚
â”œâ”€â”€ BASELINE_USAGE.md              # NEW: Baseline generation guide
â”œâ”€â”€ FLEXATTENTION_USAGE.md         # FlexAttention usage guide
â”œâ”€â”€ CHANGELOG.md                   # All changes and updates
â”‚
â””â”€â”€ test/                          # Test notebooks
    â”œâ”€â”€ test_generate.ipynb
    â”œâ”€â”€ test_dataset.ipynb
    â””â”€â”€ ...
```

## ğŸ” How It Works

### Step 1: Concatenation
```
5 Paraphrases â†’ "Para1 [SEP] Para2 [SEP] ... Para5"
Track positions: [(0,45), (50,92), ...]
```

### Step 2: Encoding with Isolation
```
Each paraphrase only attends to itself:
Para1: âœ“âœ“âœ“ âœ—âœ—âœ— âœ—âœ—âœ—
Para2: âœ—âœ—âœ— âœ“âœ“âœ“ âœ—âœ—âœ—
Para3: âœ—âœ—âœ— âœ—âœ—âœ— âœ“âœ“âœ“
```

### Step 3: Generation with Fusion
```
Generated tokens attend to ALL paraphrases:
Gen1: âœ“âœ“âœ“ âœ“âœ“âœ“ âœ“âœ“âœ“
Gen2: âœ“âœ“âœ“ âœ“âœ“âœ“ âœ“âœ“âœ“ âœ“
```

See **[docs/ARCHITECTURE.md](docs/ARCHITECTURE.md)** for detailed diagrams.

## ğŸ“ˆ Performance

- **Speed**: ~5Ã— faster than logit-level fusion (1 forward pass vs 5 per step)
- **Quality**: Comparable or better than logit-level methods
- **Memory**: Similar to single-pass generation
- **Testing**: 19/19 tests passed (100%)

## ğŸ› ï¸ Troubleshooting

### FlexAttention not available
```bash
pip install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu121
```

### CUDA out of memory
```bash
python3 flex_attention_generate.py --device cpu
```

### Dataset download fails
```bash
export HF_ENDPOINT=https://hf-mirror.com
```

For more solutions, see **[docs/DELEGATE_PROMPT.md#troubleshooting](docs/DELEGATE_PROMPT.md#troubleshooting)**.

## ğŸ“ Documentation Index

**Getting Started:**
- [BASELINE_USAGE.md](BASELINE_USAGE.md) - **NEW**: Baseline generation guide
- [docs/QUICKSTART.md](docs/QUICKSTART.md) - 5-minute setup
- [docs/DELEGATE_PROMPT.md](docs/DELEGATE_PROMPT.md) - Complete guide

**Understanding FlexAttention:**
- [FLEXATTENTION_USAGE.md](FLEXATTENTION_USAGE.md) - Usage guide
- [docs/README_FLEXATTENTION.md](docs/README_FLEXATTENTION.md) - Overview
- [docs/ARCHITECTURE.md](docs/ARCHITECTURE.md) - Visual diagrams
- [docs/FLEX_ATTENTION_IMPLEMENTATION.md](docs/FLEX_ATTENTION_IMPLEMENTATION.md) - Technical details

**API Reference:**
- [docs/QUICK_REFERENCE.md](docs/QUICK_REFERENCE.md) - Quick reference
- [docs/REUSE_VS_NEW_DETAILED.md](docs/REUSE_VS_NEW_DETAILED.md) - Code breakdown

**Development & Changes:**
- [CHANGELOG.md](CHANGELOG.md) - All changes and updates
- [docs/IMPROVEMENTS.md](docs/IMPROVEMENTS.md) - **NEW**: Consolidated improvements

**ä¸­æ–‡æ–‡æ¡£:**
- [docs/å®ç°æ€»ç»“.md](docs/å®ç°æ€»ç»“.md) - ä¸­æ–‡æ€»ç»“

## ğŸ¤ Contributing

Contributions are welcome! Areas for improvement:

- Support for more datasets
- Additional fusion strategies
- Performance optimizations
- More comprehensive testing

## ğŸ“„ License

[Add license information here]

## ğŸ™ Acknowledgments

- PyTorch team for FlexAttention API
- Hugging Face for transformers library
- Dataset authors (WebQA, MyriadLAMA)

## ğŸ“§ Contact

[Add contact information here]

---

**Status:** âœ… Production Ready | ğŸ§ª Tested | ğŸ“– Documented

Last updated: 2025-10-13
