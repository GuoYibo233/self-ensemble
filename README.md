# Self-Ensemble with FlexAttention

This repository implements self-ensemble methods for natural language generation, including a novel FlexAttention-based approach that enables efficient attention-level fusion of multiple paraphrases.

## ğŸš€ Quick Start

**New to this repository? Start here:**

1. **[QUICKSTART.md](docs/QUICKSTART.md)** - Get up and running in 5 minutes
2. **[DELEGATE_PROMPT.md](docs/DELEGATE_PROMPT.md)** - Complete debugging and validation guide
3. **[README_FLEXATTENTION.md](docs/README_FLEXATTENTION.md)** - FlexAttention overview

## ğŸ“š What's in This Repository

### Core Implementation

- **`flex_attention_generate.py`** - FlexAttention-based ensemble generation (NEW!)
- **`generate.py`** - Original ensemble methods (per_prompt, avg, max, weighted_avg)
- **`dataset.py`** - Dataset loading (WebQA, MyriadLAMA)
- **`constants.py`** - Model paths and configurations

### Debugging and Validation Tools

- **`tools/validate_flexattention_env.py`** - Environment validation script
- **`tools/debug_flexattention.py`** - Step-by-step debugging with detailed output
- **`tools/example_flexattention.py`** - Minimal working examples
- **`tools/download_resources.sh`** - Download datasets and models

### Analysis Tools

- **`analysis/analyze_flexattention.py`** - Command-line analysis tool for FlexAttention results
- **`analysis/flexattention_analysis.ipynb`** - Interactive Jupyter notebook for analysis and visualization

### Documentation

| Document | Description |
|----------|-------------|
| **[docs/QUICKSTART.md](docs/QUICKSTART.md)** | 5-minute setup guide |
| **[docs/LINUX_SETUP.md](docs/LINUX_SETUP.md)** | Linux-specific setup (Ubuntu 22.04, RTX A6000) |
| **[docs/DELEGATE_PROMPT.md](docs/DELEGATE_PROMPT.md)** | Complete debugging guide |
| **[docs/README_FLEXATTENTION.md](docs/README_FLEXATTENTION.md)** | FlexAttention overview |
| **[docs/QUICK_REFERENCE.md](docs/QUICK_REFERENCE.md)** | API quick reference |
| **[docs/FLEX_ATTENTION_IMPLEMENTATION.md](docs/FLEX_ATTENTION_IMPLEMENTATION.md)** | Technical details |
| **[docs/ARCHITECTURE.md](docs/ARCHITECTURE.md)** | Visual diagrams |
| **[docs/REUSE_VS_NEW_DETAILED.md](docs/REUSE_VS_NEW_DETAILED.md)** | Component breakdown |
| **[FLEXATTENTION_USAGE.md](FLEXATTENTION_USAGE.md)** | Usage guide with --max_samples and analysis tools |
| **[IMPROVEMENTS_SUMMARY.md](IMPROVEMENTS_SUMMARY.md)** | Recent mask matrix & prompt formatting improvements |
| **[docs/å®ç°æ€»ç»“.md](docs/å®ç°æ€»ç»“.md)** | Chinese summary |

## ğŸ¯ What is FlexAttention Ensemble?

FlexAttention ensemble is a new method that:

1. **Concatenates** multiple paraphrases into a single prompt
2. **Isolates** each paraphrase during encoding using custom attention masks
3. **Fuses** information from all paraphrases during generation

**Result:** More efficient (1Ã— forward pass vs 5Ã—) with attention-based fusion.

### Comparison with Existing Methods

| Method | Fusion | Efficiency | Forward Passes |
|--------|--------|------------|----------------|
| per_prompt | None | Baseline | 5Ã— per step |
| avg/max | Logit-level | 5Ã— cost | 5Ã— per step |
| **flex_attention** | **Attention-level** | **Most efficient** | **1Ã— per step** |

## ğŸ”§ Setup

### Prerequisites

- Python 3.10+  # FlexAttention requires Python 3.10+
- PyTorch 2.5+ or nightly (for FlexAttention)
- 20GB disk space
- NVIDIA GPU with CUDA support
- Conda/Miniconda installed

### Quick Setup

```bash
# 1. Create conda environment
# Option 1: Linux with CUDA 12.1 (Ubuntu 22.04+, RTX A6000)
conda env create -f environment_linux.yml
conda activate self-ensemble-debug

# Option 2: General environment with PyTorch nightly
conda env create -f environment.yml
conda activate flexattention

# Option 3: Manual with pip (requires Python 3.10+)
conda create -n flexattention python=3.10 -y
conda activate flexattention

# Install PyTorch FIRST
pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu121

# Then install other dependencies
pip install -r requirements.txt

# Download spaCy model
python -m spacy download en_core_web_lg

# 2. Validate environment
python3 tools/validate_flexattention_env.py --test-flex-attention

# 3. Download resources
bash tools/download_resources.sh --dataset webqa --model llama3.2_3b_it
```

For detailed setup instructions, see **[docs/QUICKSTART.md](docs/QUICKSTART.md)**.

## ğŸ“– Usage

### Basic Generation

```bash
# FlexAttention with 5 paraphrases
python3 flex_attention_generate.py \
    --dataset webqa \
    --model llama3.2_3b_it \
    --num_paraphrases 5

# Limit to 100 samples for quick testing
python3 flex_attention_generate.py \
    --dataset webqa \
    --model llama3.2_3b_it \
    --num_paraphrases 5 \
    --max_samples 100
```

### Analysis

```bash
# Analyze FlexAttention results
python3 analysis/analyze_flexattention.py \
    --dataset webqa \
    --model llama3.2_3b_it

# Interactive analysis with Jupyter
jupyter notebook analysis/flexattention_analysis.ipynb
```

For detailed usage and analysis guide, see **[FLEXATTENTION_USAGE.md](FLEXATTENTION_USAGE.md)**.

### Debugging

```bash
# Debug mode with detailed output
python3 tools/debug_flexattention.py \
    --dataset webqa \
    --model llama3.2_3b_it \
    --max-samples 2 \
    --verbose
```

### Minimal Example

```bash
# Run standalone example (no dataset/model required)
python3 tools/example_flexattention.py
```

For more examples, see **[docs/QUICK_REFERENCE.md](docs/QUICK_REFERENCE.md)**.

## ğŸ› Debugging

This repository includes comprehensive debugging tools:

### Command Line Debugging

```bash
# Debug with detailed step-by-step output
python3 tools/debug_flexattention.py --dataset webqa --max-samples 1 --verbose
```

**Shows:**
- ğŸ“Š Tensor shapes and values
- ğŸ¯ Attention mask visualization
- ğŸ” Token-by-token generation
- âœ… Segment isolation verification

### VSCode Debugging

1. Open repository in VSCode
2. Press `F5`
3. Select debug configuration:
   - "Debug FlexAttention - WebQA"
   - "Debug Script - WebQA (2 samples)"
   - "Validate Environment"

See **[docs/DELEGATE_PROMPT.md](docs/DELEGATE_PROMPT.md)** for complete debugging guide.

## ğŸ§ª Testing

Run the validation and example scripts:

```bash
# Validate environment
python3 tools/validate_flexattention_env.py --test-flex-attention

# Run minimal example
python3 tools/example_flexattention.py

# Test notebooks (requires Jupyter)
jupyter notebook test/test_generate.ipynb
```

## ğŸ“Š Datasets

Supported datasets:

- **WebQA**: Question answering dataset
- **MyriadLAMA**: Knowledge probing dataset

Download with:
```bash
bash tools/download_resources.sh --dataset webqa
bash tools/download_resources.sh --dataset myriadlama
```

## ğŸ¤– Models

Supported models (defined in `constants.py`):

- Llama 3.2 3B Instruct
- Other models can be added to `MODEL_PATHs`

Download with:
```bash
bash tools/download_resources.sh --model llama3.2_3b_it
```

## ğŸ“ Repository Structure

```
.
â”œâ”€â”€ flex_attention_generate.py    # FlexAttention implementation
â”œâ”€â”€ generate.py                    # Original ensemble methods
â”œâ”€â”€ dataset.py                     # Dataset loading
â”œâ”€â”€ constants.py                   # Configuration
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ environment.yml                # Conda environment file (general)
â”œâ”€â”€ environment_linux.yml          # Linux-specific environment (Ubuntu 22.04, CUDA 12.1)
â”‚
â”œâ”€â”€ tools/                         # Debugging and utilities
â”‚   â”œâ”€â”€ validate_flexattention_env.py  # Environment validation
â”‚   â”œâ”€â”€ debug_flexattention.py         # Debugging script
â”‚   â”œâ”€â”€ example_flexattention.py       # Minimal examples
â”‚   â””â”€â”€ download_resources.sh          # Resource downloader
â”‚
â”œâ”€â”€ analysis/                      # Analysis tools
â”‚   â”œâ”€â”€ analyze_flexattention.py   # Command-line analysis
â”‚   â”œâ”€â”€ flexattention_analysis.ipynb   # Interactive analysis notebook
â”‚   â””â”€â”€ [other analysis notebooks]
â”‚
â”œâ”€â”€ docs/                          # Documentation
â”‚   â”œâ”€â”€ QUICKSTART.md              # Quick start guide
â”‚   â”œâ”€â”€ LINUX_SETUP.md             # Linux-specific setup guide
â”‚   â”œâ”€â”€ DELEGATE_PROMPT.md         # Complete debugging guide
â”‚   â”œâ”€â”€ README_FLEXATTENTION.md    # FlexAttention overview
â”‚   â”œâ”€â”€ QUICK_REFERENCE.md         # API reference
â”‚   â”œâ”€â”€ FLEX_ATTENTION_IMPLEMENTATION.md  # Technical details
â”‚   â””â”€â”€ ARCHITECTURE.md            # Architecture diagrams
â”‚
â”œâ”€â”€ FLEXATTENTION_USAGE.md         # Usage guide for new features
â”œâ”€â”€ IMPROVEMENTS_SUMMARY.md        # Recent improvements summary
â”‚
â””â”€â”€ test/                          # Test notebooks
    â”œâ”€â”€ test_generate.ipynb
    â”œâ”€â”€ test_dataset.ipynb
    â””â”€â”€ ...
```

## ğŸ” How It Works

### Step 1: Concatenation
```
5 Paraphrases â†’ "Para1 [SEP] Para2 [SEP] ... Para5"
Track positions: [(0,45), (50,92), ...]
```

### Step 2: Encoding with Isolation
```
Each paraphrase only attends to itself:
Para1: âœ“âœ“âœ“ âœ—âœ—âœ— âœ—âœ—âœ—
Para2: âœ—âœ—âœ— âœ“âœ“âœ“ âœ—âœ—âœ—
Para3: âœ—âœ—âœ— âœ—âœ—âœ— âœ“âœ“âœ“
```

### Step 3: Generation with Fusion
```
Generated tokens attend to ALL paraphrases:
Gen1: âœ“âœ“âœ“ âœ“âœ“âœ“ âœ“âœ“âœ“
Gen2: âœ“âœ“âœ“ âœ“âœ“âœ“ âœ“âœ“âœ“ âœ“
```

See **[docs/ARCHITECTURE.md](docs/ARCHITECTURE.md)** for detailed diagrams.

## ğŸ“ˆ Performance

- **Speed**: ~5Ã— faster than logit-level fusion (1 forward pass vs 5 per step)
- **Quality**: Comparable or better than logit-level methods
- **Memory**: Similar to single-pass generation
- **Testing**: 19/19 tests passed (100%)

## ğŸ› ï¸ Troubleshooting

### FlexAttention not available
```bash
pip install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu121
```

### CUDA out of memory
```bash
python3 flex_attention_generate.py --device cpu
```

### Dataset download fails
```bash
export HF_ENDPOINT=https://hf-mirror.com
```

For more solutions, see **[docs/DELEGATE_PROMPT.md#troubleshooting](docs/DELEGATE_PROMPT.md#troubleshooting)**.

## ğŸ“ Documentation Index

**Getting Started:**
- [docs/QUICKSTART.md](docs/QUICKSTART.md) - 5-minute setup
- [docs/DELEGATE_PROMPT.md](docs/DELEGATE_PROMPT.md) - Complete guide

**Understanding FlexAttention:**
- [docs/README_FLEXATTENTION.md](docs/README_FLEXATTENTION.md) - Overview
- [docs/ARCHITECTURE.md](docs/ARCHITECTURE.md) - Visual diagrams
- [docs/FLEX_ATTENTION_IMPLEMENTATION.md](docs/FLEX_ATTENTION_IMPLEMENTATION.md) - Technical details

**API Reference:**
- [docs/QUICK_REFERENCE.md](docs/QUICK_REFERENCE.md) - Quick reference
- [docs/REUSE_VS_NEW_DETAILED.md](docs/REUSE_VS_NEW_DETAILED.md) - Code breakdown

**Debugging & Development:**
- [CHANGELOG.md](CHANGELOG.md) - Change history and updates
- [CHANGELOG_FLEXATTENTION_DEBUG.md](CHANGELOG_FLEXATTENTION_DEBUG.md) - Detailed debug log
- [FLEXATTENTION_FIX_SUMMARY.md](FLEXATTENTION_FIX_SUMMARY.md) - Recent fix summary

**ä¸­æ–‡æ–‡æ¡£:**
- [docs/å®ç°æ€»ç»“.md](docs/å®ç°æ€»ç»“.md) - ä¸­æ–‡æ€»ç»“

## ğŸ¤ Contributing

Contributions are welcome! Areas for improvement:

- Support for more datasets
- Additional fusion strategies
- Performance optimizations
- More comprehensive testing

## ğŸ“„ License

[Add license information here]

## ğŸ™ Acknowledgments

- PyTorch team for FlexAttention API
- Hugging Face for transformers library
- Dataset authors (WebQA, MyriadLAMA)

## ğŸ“§ Contact

[Add contact information here]

---

**Status:** âœ… Production Ready | ğŸ§ª Tested | ğŸ“– Documented

Last updated: 2025-10-13
